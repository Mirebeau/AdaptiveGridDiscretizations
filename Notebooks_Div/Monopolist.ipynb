{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaptive PDE discretizations on Cartesian grids\n",
    "## Volume : Divergence form PDEs\n",
    "## Chapter : The monopolist problem\n",
    "\n",
    "We present numerical methods meant to address optimization problems posed on the set of *convex functions*.\n",
    "These methods are applied to the monopolist problem, described below and arising from theoretical economics, which reads: \n",
    "$$\n",
    "    \\max_u \\int_X (<x,\\nabla u(x)> - u(x) - C(\\nabla u(x))) \\rho(x) dx,\n",
    "$$\n",
    "where $\\rho$ is a non-negative density over a domain $X$, and $C$ is a convex cost function. This optimization problem is subject to the following constraints:\n",
    "$$\n",
    "    u \\text{ convex, non-negative, and } \\nabla u(x) \\in Y \\text{ a.e. } x \\in X,\n",
    "$$\n",
    "where $Y$ is a given convex domain. \n",
    "\n",
    "We briefly describe the economic interpretation of this problem in a first section, and then present some numerical experiments in dimension one and two.\n",
    "\n",
    "<!--- \n",
    "on a convex set $X\\subset R^d$., whose gradient is possibly contrained within another convex set $Y\\subset R^d$. \n",
    "--->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**Summary**](Summary.ipynb) of volume Divergence form PDEs, this series of notebooks.\n",
    "\n",
    "[**Main summary**](../Summary.ipynb) of the Adaptive Grid Discretizations \n",
    "\tbook of notebooks, including the other volumes.\n",
    "\n",
    "# Table of contents\n",
    "  * [1. Introduction](#1.-Introduction)\n",
    "    * [1.1 A convex barrier for convex functions](#1.1-A-convex-barrier-for-convex-functions)\n",
    "    * [1.2 The monopolist problem](#1.2-The-monopolist-problem)\n",
    "  * [2. The one dimensional monopolist problem](#2.-The-one-dimensional-monopolist-problem)\n",
    "    * [2.1 Quadratic production costs](#2.1-Quadratic-production-costs)\n",
    "    * [2.2 Null costs within a bounded product range](#2.2-Null-costs-within-a-bounded-product-range)\n",
    "  * [3. Two dimensional, quadratic, monopolist](#3.-Two-dimensional,-quadratic,-monopolist)\n",
    "    * [3.1 Finite element discretization of the energy](#3.1-Finite-element-discretization-of-the-energy)\n",
    "    * [3.2 Subgradient measures of a piecewise linear function](#3.2-Subgradient-measures-of-a-piecewise-linear-function)\n",
    "    * [3.3 Optimization, uniformly convex domain](#3.3-Optimization,-uniformly-convex-domain)\n",
    "    * [3.4 Non-uniformly convex domain](#3.4-Non-uniformly-convex-domain)\n",
    "  * [4. Two dimensional, linear, monopolist](#4.-Two-dimensional,-linear,-monopolist)\n",
    "    * [4.1 Finite element approach](#4.1-Finite-element-approach)\n",
    "    * [4.2 Intersected subgradient approach (*TODO*)](#4.2-Intersected-subgradient-approach-(*TODO*))\n",
    "\n",
    "\n",
    "\n",
    "**Acknowledgement.** Some of the experiments presented in these notebooks are part of \n",
    "ongoing research with Ludovic Métivier and Da Chen.\n",
    "\n",
    "Copyright Jean-Marie Mirebeau, Centre Borelli, ENS Paris-Saclay, CNRS, University Paris-Saclay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Importing the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.path.insert(0,\"..\") # Allow import of agd from parent directory (useless if conda package installed)\n",
    "# from Miscellaneous import TocTools; print(TocTools.displayTOC('Monopolist','Div'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "ExportCode"
    ]
   },
   "outputs": [],
   "source": [
    "from agd import LinearParallel as lp\n",
    "from agd import AutomaticDifferentiation as ad\n",
    "norm_infinity = ad.Optimization.norm_infinity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "ExportCode"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial import ConvexHull, convex_hull_plot_2d\n",
    "import scipy\n",
    "import scipy.optimize as sciopt\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "ExportCode"
    ]
   },
   "outputs": [],
   "source": [
    "from agd.ExportedCode.Notebooks_Algo.Meissner import LinearConstraint_AD,QuadraticObjective_AD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 A convex barrier for convex functions\n",
    "\n",
    "\n",
    "We present numerical methods meant to address optimization problems posed on the set of convex functions, defined over a convex set $X\\subset R^d$, and whose gradient is almost everywhere constrained within another convex set $Y\\subset R^d$. \n",
    "$$\n",
    "    \\mathrm{Conv}(X;Y) := \\{u : X \\to R; u \\text{ convex}, \\nabla u(x) \\in Y\\ a.e.\\ x \\in X\\}\n",
    "$$\n",
    "We provide a discrete counterpart of this set, defined in terms of a family of constraints, which are non-linear in dimension $d\\geq 2$. As discussed below, the corresponding barrier function is convex, and admits an interpretation of it as an entropy.\n",
    "\n",
    "\n",
    "**Discrete convexity constraints.**\n",
    "Let $X\\subset R^d$ be a set, possibly finite (in constrast with the previous paragraph). Let $u : X \\to R$, and for each $x \\in X$ let \n",
    "$$\n",
    "    \\partial_x u := \\{v \\in R^d; \\forall y \\in X, u(y)-u(x) \\geq <v,y-x>\\}\n",
    "$$\n",
    "denote the subgradient of $u$ at $x$. In this context we define\n",
    "$$\n",
    "    \\mathrm{Conv}(X;Y) := \\{u : X \\to R; \\forall x \\in X,\\ Y \\cap \\partial_x u \\neq \\emptyset\\}.\n",
    "$$\n",
    "The following remarks are in order:\n",
    "- If $X$ is a convex set, then the above two definitions of $\\mathrm{Conv}(X;Y)$ are compatible. \n",
    "- If $\\tilde X := \\mathrm{Hull}(X)$ is the convex hull of $X$, then $\\mathrm{Conv}(X;Y) \\subset \\mathrm{Conv}(\\tilde X;Y)$, where a function $u:X \\to R$ is implicitly extended to $\\tilde X$ by its lower convex envelope.\n",
    "- If $X$ is a finite set, then only the second definition makes sense. In addition, the interior of $\\mathrm{Conv}(X;Y)$ is characterized by the strict inequality constraints:\n",
    "$$\n",
    "    |Y \\cap \\partial_x u| > 0, \\forall x \\in X\n",
    "$$\n",
    "\n",
    "In the following, we assume that the set $X$ is finite, and denote by $\\partial X := X \\cap \\partial \\mathrm{Hull}(X)$ the subset of points lying on the boundary of its convex hull.\n",
    "If $Y = R^d$, and more generally if $Y$ is unbounded, then the cells associated to points $x \\in X$ can be unbounded, or empty, hence the associated constraints require a special numerical treatment.\n",
    "\n",
    "**The barrier function.**\n",
    "Constrained optimization methods based on interior points, such as `trust-constr` in `scipy.optimize.minimize`, often rely on *barrier functions* which are defined as a negative weighted sum of the logarithm of the constraints.\n",
    "In our case:\n",
    "$$\n",
    "    S(u) := -\\sum_{x \\in X} \\rho(x) \\ln |Y \\cap \\partial_x u|,\n",
    "$$\n",
    "where $\\rho : X \\to [0,\\infty[$ is a given collection of weights.\n",
    "We limit here our discussions to the case where $Y$ is bounded, since otherwise the points $x \\in \\partial X$ require a special treatment, see the relevant sections of the notebook.\n",
    "\n",
    "For efficient optimization, one expects the barrier function to be well behaved, in particular it should be (i) convex $\\nabla^2 S \\succ 0$, (ii) the hessian should dominate the gradient $|<\\nabla S,v>| \\leq C |<v,\\nabla^2 S v>|^\\frac 1 2$ (self consistency condition), and (iii) the hessian should dominate the third order derivatives, $|\\nabla^3 S(v,v,v)| \\leq 2|<v,\\nabla^2 S v>|^\\frac 3 2$. \n",
    "In our case, in dimension $d\\geq 2$, one can check that (i) and (ii) hold, by virtue of Minkowski's inequality. In contrast whereas (iii) fails, and in fact $S$ is not but three times differentiable, that does not seem to be a major issue.\n",
    "\n",
    "**Entropy interpretation.**\n",
    "The entropy of the discrete measure defined as a Dirac mass of weight $\\rho(x)$ at each position $x\\in X$, and w.r.t the counting measure on these points, is defined as \n",
    "$$\n",
    "    H(\\rho) := \\sum_{x \\in X} \\rho(x) \\ln \\rho(x).\n",
    "$$\n",
    "\n",
    "Denote by $\\mu$ the density on $R^d$ which assigns a weight $\\rho(x)$ to each cell $C_x := Y \\cap \\partial_x u$ in a uniform manner, for all $x \\in X$.\n",
    "Then the entropy of $\\mu$, w.r.t the Lebesgue measure on its support, is \n",
    "$$\n",
    "    H(\\mu) = - \\int_{R^d} \\mu \\ln \\mu \n",
    "    = - \\sum_{x \\in X} \\int_{C_x} \\frac{\\rho(x)}{|C_x|} \\ln \\frac {\\rho(x)}{|C_x|}\n",
    "    = - \\sum_{x \\in X} \\rho(x) \\ln \\frac {\\rho(x)}{|C_x|}\n",
    "    = \\sum_{x \\in X} \\rho(x) \\ln |C_x| - \\sum_{x \\in X} \\rho(x) \\ln \\rho(x).\n",
    "$$\n",
    "Therefore our barrier function is obtained as the difference between the entropy of the source measure $\\rho$, and of its image measure $\\mu$ by the subgradient map \n",
    "$$\n",
    "    S(u) = H(\\rho) - H(\\mu).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 The monopolist problem\n",
    "\n",
    "We briefly review the monopolist problem, arising in theoretical economics, and which is numerically addressed in this notebook.\n",
    "\n",
    "Suppose a monopolist is able to produce a variety of products, whose characteristics are encoded as a point in a set $Y \\subset R^d$. The monopolist is able to fix arbitrary prices\n",
    "$$\n",
    "    \\pi : Y \\to R,\n",
    "$$\n",
    "except for the null product, which must be avalaible for free: $\\pi(0) = 0$. There is also a fixed production cost \n",
    "$$\n",
    "    C : Y \\to ]-\\infty,\\infty].\n",
    "$$\n",
    "The characteristics of the potential customers are encoded as a point in a set $X\\subset R^d$. The utility to customer $x \\in X$ of a product $y \\in Y$ is modeled as the scalar product\n",
    "$$\n",
    "    {<}x,y>.\n",
    "$$\n",
    "This assumption of bi-linearity of the utility is quite strong from the modeling point of view, but bypassing it is outside of the scope of this notebook. \n",
    "\n",
    "Each customer scans the available products, and buys a single one (possibly the null product) which provides the best utility minus cost. This leads to the customer utility function, defined for any customer characteristics $x\\in X$:\n",
    "$$\n",
    "    u(x) := \\max_{y \\in Y} <x,y> - \\pi(y).\n",
    "$$\n",
    "By construction, this utility function is:\n",
    "- *non-negative*, due to the availability of the null product $\\pi(0)=0$.\n",
    "- *convex* since it is the upper envelope of the family of affine functions $u_y : x\\mapsto <x,y> - \\pi(y)$, for all $y \\in Y$.\n",
    "- has a *gradient constrained within $Y$*, by the envelope theorem and since $\\nabla u_y = y$ identically.\n",
    "\n",
    "The density of customers with each given characteristic is denoted $\\rho : X \\to R$. The overall profit of the monopolist, obtained as the selling price minus the production cost, are thus\n",
    "$$\n",
    "    \\int_X (\\pi(y(x)) - C(y(x))) \\rho(x) dx,\n",
    "$$\n",
    "where $y(x)$ is the product chosen by customer $x$.\n",
    "\n",
    "Observing that $u(x) = <x,y(x)>-\\pi(y(x))$, by definition, and that $y(x) = \\nabla u(x)$, by the envelope theorem, we reformulate the total profit as \n",
    "$$\n",
    "\\int_X (<x,\\nabla u(x)> - u(x) - C(\\nabla u(x))) \\rho(x) dx.\n",
    "$$\n",
    "The purpose of the monopolist is to maximize this profit, knowing $C$ and $\\rho$, by adjusting the prices $\\pi$.\n",
    "Since the utility function $u : X \\to R$ is the Legendre-Fenchel conjugate of the prices, we can use it as the unknown, and recover the prices afterwards using another Legendre-Fenchel transform.\n",
    "\n",
    "**The quadratic costs model**, is characterized by $Y:=R^d$, $C(y) := \\frac 1 2 \\|y\\|^2$.\n",
    "\n",
    "Interpretation (d=2) : Suppose the product $y = (y_0,y_1)$ is a car, where $y_0$ accounts for the car performance, and $y_1$ accounts for its comfort. For simplicity these two characteristics are not a-priori bounded, as reflected by the choice $Y = R^d$, but the production cost $C(y_0,y_1) = \\frac 1 2 (y_0^2+y_1^2)$ grows quadratically with them.\n",
    "\n",
    "The monopolist profit (sales minus production costs), for the optimal product catalog, is obtained as \n",
    "$$\n",
    "    \\max_u \\int_X (<x,\\nabla u(x)> -u(x)- \\frac 1 2 \\|\\nabla u(x)\\|^2) \\rho(x) dx.\n",
    "$$\n",
    "among *non-negative convex* functions $u : X \\to R$.\n",
    "\n",
    "**The bounded product range, null costs, model**, is characterized by $Y = [0,1]^d$, $C=0$.\n",
    "\n",
    "Interpretation ($d=2$) : suppose the product $y = (y_0,y_1)$ is an online streaming service, where $y_0$ accounts for subscription the movies, whereas $y_1$ accounts for the subscription to sports. \n",
    "The four corners of the set $Y = [0,1]^2$ represent immediate deterministic offers, for instance $(1,0)$ represents a subscription to movies but not sports. Other points of $Y$ represent time delayed offers, or lotteries. The marginal cost of an additional customer subscription is negligible, hence the choice $C=0$.\n",
    "\n",
    "The monopolist profit, for the optimal product catalog, is obtained as\n",
    "$$\n",
    "    \\max_u \\int_X (<x,\\nabla u(x)> - u(x)) \\rho(x) dx.\n",
    "$$\n",
    "among *non-negative convex* functions $u : X \\to R$ such that $\\nabla u(X) \\subset [0,1]^d$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The one dimensional monopolist problem\n",
    "\n",
    "The one dimensional monopolist problem has been well studied and can be solved analytically, at least when the density of customer traits is constant. Nevertheless, we adopt a numerical approach as a preparation for the multi-dimensional case, and in order to illustrate some qualitative behavior. More complex and subtle behavior is obtained in higher dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discretization** The set $X = \\{x_0 < \\cdots < x_{I-1}\\}$ is a finite ordered sets of reals, discretizing the interval $[x_0,x_{I-1}]$.\n",
    "The unknown function $u$ is known by its values $u_0,\\cdots,u_{I-1}$ at these points. Our discretization relies on some midpoint averages and differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg(z):\n",
    "    \"\"\"Midpoint averages\"\"\" \n",
    "    return (z[1:]+z[:-1])/2\n",
    "\n",
    "def diff(z): \n",
    "    \"\"\"Midpoint slopes\"\"\"\n",
    "    return z[1:]-z[:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose the following domain and density."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nX = 51 # Number of discretization points\n",
    "X = np.linspace(0,1,nX) # Domain is [0,1]\n",
    "ρ = np.ones(nX-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A null initial guess is used for the optimization problems. First and second order automatic differentiation is used to differentiate the objective function and constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "U = np.zeros_like(X) # Initial guess\n",
    "U_ad = ad.Sparse.identity(constant=U)\n",
    "U_ad2 = ad.Sparse2.identity(constant=U)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Quadratic production costs \n",
    "\n",
    "\n",
    "**Qualitative behavior : voluntarily neglecting some potential customers.** In the quadratic cost model, the monopolist has the possibility to sell products to all clients including the poorest ones, and always make a profit out of it. However, the monopolist often willingly reduces the product catalog, and thus does not address the lower end market, in order to avoid self competition. \n",
    "\n",
    "**Optimization problem** The (opposite of the) monopolist profit is obtained as\n",
    "$$\n",
    "    \\min_u \\int_X (u(x)-x u'(x) + \\frac 1 2 u'(x)^2) \\rho(x) dx\n",
    "$$\n",
    "subject to $u$ convex, non-negative. \n",
    "\n",
    "We implement this objective function in the next cell, using finite differences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quadratic_objective(U,X,ρ):\n",
    "    \"\"\"\n",
    "    u and X are defined at nodes,\n",
    "    ρ is defined between nodes.\n",
    "    \"\"\"\n",
    "    mX = avg(X); mU = avg(U) # Midpoint values\n",
    "    dX = diff(X); dU = diff(U) # Successive differences\n",
    "    g = dU / dX # Gradient at the midpoint\n",
    "    \n",
    "    integrand = mU - mX*g + g**2/2\n",
    "    return np.sum(integrand * ρ * dX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that, for a piecewise linear one dimensional convex function, the subgradient measure at a point is obtained as the difference of the gradients (slopes) immediately before and afterwards. We impose as a constraint that these subgradients measures be positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interior_subgradients(U,X):\n",
    "    \"\"\"Returns the subgradients measures of U at the interior points of X\"\"\"\n",
    "    g = diff(U) / diff(X) # Gradient at the midpoint\n",
    "    return diff(g) # Subgradient measure is the difference of successive gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `trust-constr` interior point optimization method is used for convenience, and for generality in view of the subsequent applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sol = sciopt.minimize(x0=U,method='trust-constr',\n",
    "    **QuadraticObjective_AD(quadratic_objective(U_ad2,X,ρ)),\n",
    "    constraints = [LinearConstraint_AD(interior_subgradients(U_ad,X))],\n",
    "    bounds = sciopt.Bounds(0.,np.inf)\n",
    "                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recally that $u(x)$ is the utility to customer $x$, and that $u'(x)$ is the product bought. The trait $x$ represents how much customer $x$ values the offered goods, hence it is closely related as the wealth of customer $x$. We can distinguish two behaviors:\n",
    "- Poor customers buy the null product: $u(x)=0$ for sufficiently small $x$.\n",
    "- Rich customers buy a product specifically designed for their characteristic: $u'$ is strictly increasing for sufficiently large $x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Quadratic monopolist: utility function\")\n",
    "plt.plot(X,sol.x);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Commented : on using the agd library (too trivial to be part of it)**\n",
    "\n",
    "<!---\n",
    "Again, we can use the `ConvexEntropy.ConvexBarrier` method, this time without the set $Y$ which is not needed.\n",
    "\n",
    "lcons_cvx_ad = ConvexEntropy.ConvexBarrier(u_ad,X,Xbd)[1];\n",
    "\n",
    "sol_cvx = sciopt.minimize(x0=u,method='trust-constr',\n",
    "    **QuadraticObjective_AD(objective(u_ad2,ρ,X)),\n",
    "    constraints = [LinearConstraint_AD(lcons_cvx_ad)],\n",
    "    bounds = sciopt.Bounds(0.,np.inf)\n",
    "                     )\n",
    "assert np.allclose(sol.x,sol_cvx.x)\n",
    "--->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Null costs within a bounded product range\n",
    "\n",
    "\n",
    "**Qualitative behavior : extremal products only.** In principle, the monopolist may an infinite variety of products $Y = [0,1]$. However, the optimal product line only features the extremal products $0$ and $1$. In the context of a subscription service, this means that the optimal strategy does not feature any lottery or time-delayed discounted offer.\n",
    "\n",
    "\n",
    "\n",
    "**Optimization problem.** The monopolist profit is the opposite of \n",
    "$$\n",
    "    \\min_u \\int_X (u(x)-x u'(x)) \\rho(x) dx\n",
    "$$\n",
    "subject to $u$ convex, non-negative, and $u'(x)\\in Y := [0,1]$ for all $x \\in X$. \n",
    "\n",
    "We implement this objective function in the next cell, using finite differences.\n",
    "\n",
    "<!---\n",
    "$u(0) = 0$, $u'(0)\\geq 0$, $u'(1)\\leq 1$.\n",
    "--->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_objective(U,X,ρ):\n",
    "    \"\"\"\n",
    "    u and X are defined at nodes,\n",
    "    ρ is defined between nodes.\n",
    "    \"\"\"\n",
    "    mX = avg(X); mU = avg(U) # Midpoint values\n",
    "    dX = diff(X); dU = diff(U) # Successive differences\n",
    "    g = dU / dX # Gradient at the midpoint\n",
    "    \n",
    "    integrand = mU-mX*g\n",
    "    return np.sum(integrand * ρ * dX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already implemented in the `interior_subgradient` map the constraint that the subgradients of $u$ at interior points of $X$ have positive measure. \n",
    "In order to enforce the additional constraint $u'(x)\\in Y$ for all $x \\in X$, we also consider the subgradients at the extremal points, which are unbounded, and intersect them with the set $Y:=[0,1]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boundary_subgradients(U,X,Y):\n",
    "    \"\"\"Returns the subgradient measures of u at the \n",
    "    extremal points of X, intersected with Y\"\"\"\n",
    "    g = diff(U) / diff(X)\n",
    "    return ad.array([ g[0]-Y[0], Y[-1]-g[-1] ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `trust-constr` interior point optimization method is again used for convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sol = sciopt.minimize(x0=U,method='trust-constr',\n",
    "    **QuadraticObjective_AD(linear_objective(U_ad2,X,ρ)),\n",
    "    constraints = [LinearConstraint_AD(interior_subgradients(U_ad,X)),\n",
    "                   LinearConstraint_AD(boundary_subgradients(U_ad,X,Y))],\n",
    "    bounds = sciopt.Bounds(0.,np.inf)\n",
    "                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected for this specific model, the utility function is piecewise affine, with only two distinct slopes, corresponding to the two following products:\n",
    "- $y=0$, the null product, available for free, chosen by `poor` customers.\n",
    "- $y=1$, the full product, chosen by `rich` customers.\n",
    "\n",
    "In dimension $1$, the optimal product line for the monopolist does not feature any intermediate product $0<y<1$, such as a time delayed offer or a lottery ticket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Linear monopolist: utility function\")\n",
    "plt.plot(X,sol.x);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As announced, we can use a linear solver to achieve the same result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_ad = linear_objective(U_ad,X,ρ).to_dense()\n",
    "lcons_ad = np.concatenate([boundary_subgradients(U_ad,X,Y),interior_subgradients(U_ad,X)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sol_lin = sciopt.linprog(obj_ad.gradient(), options={'sparse':True},\n",
    "               A_ub = (-lcons_ad).tangent_operator(), b_ub = lcons_ad.value, \n",
    "               bounds=np.transpose([np.zeros(nX),np.full(nX,np.inf)]) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The solution obtained with the linear the linear solver is identical... up to numerical solver error, which is not so small. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.allclose(sol.x,sol_lin.x,atol=2e-3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Solution difference, trust-constr and linprog\")\n",
    "plt.plot(X,sol.x-sol_lin.x);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Commented : Using the AGD library (seems to trivial for a proper implementation)**\n",
    "\n",
    "<!---\n",
    "Finally, let us mention that the routine `ConvexEntropy.ConvexBarrier` can take into account the target space $Y=[0,1]$ directly. Hence there is no need to impose the slope bound independently.\n",
    "\n",
    "Xbd=np.zeros_like(X); Xbd[0]=Xbd[-1]=1 # Boundary of X\n",
    "Y = [0.,1.] # Region within which the gradient is constrained\n",
    "lcons_cvxY_ad = ConvexEntropy.ConvexBarrier(u_ad,X,Xbd,Y)[1];\n",
    "\n",
    "\n",
    "sol_cvx = sciopt.minimize(x0=u,method='trust-constr',\n",
    "    **QuadraticObjective_AD(objective(u_ad2,ρ,X)),\n",
    "    constraints = [LinearConstraint_AD(lcons_cvxY_ad)],\n",
    "    bounds = sciopt.Bounds(0.,np.inf)\n",
    "                     )\n",
    "                     \n",
    "assert np.allclose(sol.x,sol_cvx.x,atol=2e-3) \n",
    "--->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Two dimensional, quadratic, monopolist \n",
    "\n",
    "We address the two dimensional monopolist problem, with a quadratic cost function and an otherwise unconstrained product line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Finite element discretization of the energy\n",
    "\n",
    "In contrast with most other notebooks in this repository, we use finite elements (continuous piecewise affine) to discretize the monopolist objective function.\n",
    "\n",
    "A $d$-dimensional triangulation is presented as a pair \n",
    "$$\n",
    "    (X,T)\n",
    "$$\n",
    "where $X \\in (R^d)^N$ is a set of $N$ points, and $T\\in (\\{0,\\cdots,N-1\\}^{d+1})^M$ defines $M$ simplices by providing the indices of their $d+1$ vertices.\n",
    "\n",
    "For simplicity, we use the *Delaunay triangulation* of a regular sampling of three basic shapes: the square, equilateral triangle, and unit disk. For reasons discussed later, it will be important to know the *non-strictly convex faces* of these domains. \n",
    "\n",
    "**Hidden boundary routine**\n",
    "\n",
    "<!---\n",
    "def Boundary(X,l=[1,1]):\n",
    "    \"\"\"Returns the boundary of X (indices), trigonometrically ordered, \n",
    "    starting with the extremal point in the direction of l\"\"\"\n",
    "    delaunay=scipy.spatial.Delaunay(X.T)\n",
    "    DX = np.unique(delaunay.convex_hull.reshape(-1)) # Boundary points\n",
    "    mX = np.mean(X[:,DX],axis=1) # Some interior point\n",
    "    DX = DX[np.argsort( np.angle((X[0,DX]-mX[0]) + 1j*(X[1,DX]-mX[1])) )]\n",
    "    DX = np.roll(DX,-np.argmax(X[0,DX]*l[0]+X[1,DX]*l[1]))\n",
    "    return DX\n",
    "--->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": [
     "ExportCode"
    ]
   },
   "outputs": [],
   "source": [
    "def square_X(n):\n",
    "    \"\"\"A regular sampling of the unit square, approx n^2 points.\"\"\"\n",
    "    aX = np.linspace(0,1,n+1)\n",
    "    X = np.array(np.meshgrid(aX,aX,indexing='ij')).reshape(2,-1)\n",
    "    tol = 0.5/n\n",
    "    return X, (X[0]>1-tol,X[1]>1-tol,X[0]<tol,X[1]<tol)\n",
    "\n",
    "def triangle_X(n):\n",
    "    \"\"\"A regular sampling of the equailateral triangle of vertices \n",
    "    (1,0), (-1/2,sqrt 3/2),(-1/2,-sqrt 3/2), approx n^2/2 points\"\"\"\n",
    "    X = np.array([ (i/n,j/n) for i in range(n+1) for j in range(n-i+1)]).T\n",
    "    tol = 0.5/n\n",
    "    def aff(X): return np.array([1-(X[0]+X[1])*3/2, (X[1]-X[0])*np.sqrt(3)/2])\n",
    "    return aff(X), (X[0]+X[1]>1-tol,X[0]<tol,X[1]<tol)\n",
    "\n",
    "def disk_X(n):\n",
    "    \"\"\"A regular sampling of the unit disc, approx π n^2 points\"\"\"\n",
    "    τ = 2*np.pi\n",
    "    X = np.array([ (i*np.cos(θ),i*np.sin(θ)) for i in range(n+1) \n",
    "        for θ in np.linspace(0,τ,np.maximum(1,int(τ*i)),endpoint=False)]).T/n\n",
    "    return X,tuple() # No facet : the disk is uniformly convex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": [
     "ExportCode"
    ]
   },
   "outputs": [],
   "source": [
    "def Delaunay(X): return scipy.spatial.Delaunay(X.T).simplices.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": [
     "ExportCode"
    ]
   },
   "outputs": [],
   "source": [
    "def show_X(X,DX):\n",
    "    plt.triplot(*X,Delaunay(X).T)\n",
    "    for F in DX: plt.scatter(*X[:,F])\n",
    "    plt.axis('equal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,4))\n",
    "plt.subplot(131); show_X(*square_X(10))\n",
    "plt.subplot(132);show_X(*triangle_X(10))\n",
    "plt.subplot(133); show_X(*disk_X(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following a standard finite element approach, we compute the gradient and mean on each simplex of a given piecewise linear function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": [
     "ExportCode"
    ]
   },
   "outputs": [],
   "source": [
    "def gradientFE(u,X,T):\n",
    "    \"\"\"Gradient on each simplex of the piecewise affine function u\"\"\"\n",
    "    A = [ X[:,T[1]]-X[:,T[0]], X[:,T[2]]-X[:,T[0]] ]\n",
    "    return lp.solve_AV(A,u[T[1:]]-u[T[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,_ = disk_X(10)\n",
    "assert np.allclose(gradientFE(X[0]+2*X[1],X,Delaunay(X)),[[1],[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": [
     "ExportCode"
    ]
   },
   "outputs": [],
   "source": [
    "def meanFE(u,T):\n",
    "    \"\"\"Mean on each simplex of piecewise affine function u\"\"\"\n",
    "    return np.mean(u[...,T],axis=-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,_ = triangle_X(10)\n",
    "plt.scatter(*meanFE(X,Delaunay(X)))\n",
    "plt.triplot(*X,Delaunay(X).T); plt.axis('equal');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": [
     "ExportCode"
    ]
   },
   "outputs": [],
   "source": [
    "def cellmeasFE(X,T):\n",
    "    \"\"\"Area (or volume) of each simplex in the triangulation\"\"\"\n",
    "    return lp.det(X[:,T[1:]]-X[:,None,T[0]])/np.math.factorial(len(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "nX=10\n",
    "X,_ = square_X(nX); \n",
    "assert np.allclose(cellmeasFE(X, Delaunay(X)), 0.5/nX**2)\n",
    "X,_ = triangle_X(10); \n",
    "assert np.allclose(cellmeasFE(X, Delaunay(X)),3*np.sqrt(3)/4/nX**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we discretize the objective functional \n",
    "$$\n",
    "    \\int_\\Omega (u(z) - <z,\\nabla u(z)> + \\frac 1 2 \\| \\nabla u(z)\\|^2) \\rho(z) dz,\n",
    "$$\n",
    "possibly omitting the quadratic term. Note that this functional is quadratic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": [
     "ExportCode"
    ]
   },
   "outputs": [],
   "source": [
    "def monopolist_objective(u,X,T,ρ=1.,quadratic=True):\n",
    "    g = gradientFE(u,X,T)\n",
    "    integrand = meanFE(u,T) - lp.dot_VV(meanFE(X,T),g) # u(z) - <grad u(z),z>\n",
    "    if quadratic: integrand += lp.dot_VV(g,g)/2 # + |grad u(z)|^2 /2\n",
    "    return np.sum(integrand*cellmeasFE(X,T)*ρ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Subgradient measures of a piecewise linear function\n",
    "\n",
    "The following code computes the subgradients measures of a function $u:X \\to R$ defined over a finite set $X$. The pseudo-code of this routine is as follows:\n",
    "* Compute the convex envelope of the set \n",
    "$$\n",
    "    \\{(x,u(x)); x \\in X\\}\n",
    "$$\n",
    "which defines a triangulated surface $S$ in $R^3$.\n",
    "* Remove all the triangles of $S$ whose normal vector does not point downwards.\n",
    "What is left is the graph of a function, denoted as \n",
    "$$\n",
    "    \\{(x,\\tilde u(x)); x \\in \\mathrm{Hull}(X)\\},\n",
    "$$\n",
    "and $\\tilde u$ is *piecewise linear and convex* over a triangulation $\\cal T$ of the set $X$.\n",
    "Note that $\\tilde u(x) = u(x)$ for all $x\\in X$, if $u$ admits any convex extension.\n",
    "* Compute the gradient of $\\nabla \\tilde u$ on each triangle of $T\\in \\cal T$, denoted as $g_T$.\n",
    "* Denote by $\\mathrm{int}(X) := X \\setminus \\partial \\mathrm{Hull}(X)$ the finite set of interior points of $X$.\n",
    "\n",
    "Recall that our objective is to compute, for each $x \\in \\mathrm{int}(X)$, the area of \n",
    "$$\n",
    "    \\partial_x u = \\mathrm{Hull}\\{g_T; T \\in {\\cal T}, x \\in T\\}.\n",
    "$$\n",
    "Denote by $T_1,...,T_I \\in \\cal T$ the triangles containing $x$ and oriented counter-clockwise around $x$. Then the polygon $|\\partial_x u|$ can be decomposed as a union of triangles, and its area computed, as follows\n",
    "$$\n",
    "    |\\partial_x u| = \\sum_{1 \\leq i \\leq I} |\\mathrm{Hull}\\{g_{T_i}, g_{T_{i+1}}, g_x\\}|,\n",
    "$$\n",
    "where $g_x$ is any point of $\\partial_x u$. The pseudo-code for the rest of the function is thus:\n",
    "\n",
    "* For each $x \\in \\mathrm{int}(X)$, find an arbitrary but fixed triangle $T(x)\\in \\cal T$ containing $x$, and define $g_x = g_{T_x}$. (The AD information within $g_x$ can be removed here, since this gradient only serves as an arbitrary element of $\\partial_x u$.)\n",
    "* For each triangle $T \\in \\cal T$, and each vertex $x$ of $T$, find the triangle $T' \\in \\cal T$ counter-clockwise around $x$. Compute the area contribution of $|\\mathrm{Hull}\\{g_{T}, g_{T'}, g_x\\}|$ to the subgradient area $|\\partial_x u|$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FacetGradients(u,X,tol_bd=1e-6):\n",
    "    \"\"\"\n",
    "    Computes the convex hull of the graph of u. \n",
    "    Returns  \n",
    "        - S : the facets, triplets of vertices, oriented counter-clockwise\n",
    "        - N : the facet neighbors\n",
    "        - Sint : which facets are interior\n",
    "        - Sg : the gradient on each facet\n",
    "    \"\"\"\n",
    "    # The graph of the function u, with all AD information removed\n",
    "    u = u.reshape(-1); X = X.reshape(2,-1)\n",
    "    graph = np.append(ad.remove_ad(X),ad.remove_ad(u)[None],axis=0) \n",
    "    hull = ConvexHull(graph.T) # Key geometric step\n",
    "\n",
    "    if len(hull.vertices)!=u.size: # Then hull.vertices == np.arange(u.size)\n",
    "        raise ValueError(\"Non convex function : empty subgradients at \",set(range(u.size))-set(hull.vertices))\n",
    "\n",
    "    # Variables naming convention : \n",
    "    # - S... data attached to simplices, \n",
    "    # - X... data attached to vertices\n",
    "    # - N... data attached to neighbor vertices\n",
    "\n",
    "    # Orient the simplices counter-clockwise\n",
    "    S = hull.simplices.T     \n",
    "    N = hull.neighbors.T\n",
    "    cw = np.sign(lp.det(X[:,S[1:]]-X[:,S[0,None]]))==-1\n",
    "    S[1,cw],S[2,cw] = S[2,cw],S[1,cw] \n",
    "    N[1,cw],N[2,cw] = N[2,cw],N[1,cw] \n",
    "\n",
    "    # Find simplices which do not correspond to the lower convex hull\n",
    "    Sbd = hull.equations[:,2] >= -tol_bd # Boundary simplices safe to omit\n",
    "    Sint = ~Sbd # Interior simplices\n",
    "    \n",
    "    # Compute the gradient associated to each facet, excluding boundary facets\n",
    "    Sdu = u[S[1:]]-u[S[0]]              # Value differences, for each simplex\n",
    "    SdX = X[:,S[1:]]-X[:,S[0,None]]     # Vertex differences, for each simplex\n",
    "\n",
    "    Sdu[:,Sbd]   = np.nan \n",
    "    SdX[:,:,Sbd] = np.nan # Erase boundary simplices, to avoid zero divide\n",
    "\n",
    "    Sg = lp.solve_AV(lp.transpose(SdX),Sdu) # Gradient of u on each simplex\n",
    "\n",
    "    return S,N,Sint,Sg\n",
    "\n",
    "def SubgradientMeasures(S,N,Sint,Sg,Sbd=np.nan):\n",
    "    \"\"\"Compute the subgradient measures of a convex functions, \n",
    "    at the interior points of a domain. Sbd : value to put in boundary cells.\"\"\"\n",
    "    \n",
    "    # Choose a reference simplex for each vertex, and thus a reference point in each cell\n",
    "    XS = np.full(np.max(S)+1,-1,S.dtype)\n",
    "    XS[S[:,Sint]] = np.arange(S.shape[1],dtype=S.dtype)[None,Sint] # Ref simplex per vertex\n",
    "    Xg = ad.remove_ad(Sg)[:,XS]    # Reference gradient for each vertex\n",
    "\n",
    "    Ng = Sg[:,N] # Gradients for the neighbor cells\n",
    "    Srg = Xg[:,S] # Reference gradient associated to each vertex of the simplex \n",
    "\n",
    "    # Compute the contribution of the simplex and its neighbor to the subgradient measure\n",
    "    Sm = lp.det([Sg[:,None]-Srg,np.roll(Ng,-1,axis=1)-Srg])/2\n",
    "    S = S.reshape(-1); Sm = ad.simplify_ad(Sm.reshape(-1))\n",
    "    \n",
    "    # Put desired value in boundary subgadient cells\n",
    "    Xbd = (np.bincount(S,np.isnan(Sm))>0)*np.bincount(S)\n",
    "    bd = Xbd[S]\n",
    "    Sm[bd>0]=Sbd/bd[bd>0]\n",
    "    \n",
    "    return S,Sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a validity test, we check the subgradient measures of quadratic function sampled over a Cartesian grid, in dimension $d=2$.\n",
    "Define the square grid \n",
    "$$\n",
    "X = \\{-n,\\cdots, 0,1,\\cdots,n\\}^d,\n",
    "$$ \n",
    "and the quadratic function \n",
    "$$\n",
    "u(x) := \\frac 1 2 <x,Ax> + <b,x> +c,\n",
    "%x^\\top \\cdot A \\cdot x + b^\\top \\cdot x\n",
    "$$\n",
    "where $A\\in S_d^{++}$, $b \\in R^d$, and $c \\in R$.\n",
    "Then \n",
    "$\\nabla u(x) = A x+ b$ is affine, and thus maps $X$ to another lattice.\n",
    "The subgradient cells $\\partial_x u$, for $x \\in X$ sufficiently far from the boundary, all have the same area \n",
    "$$\n",
    "    |C_x| = \\det A\n",
    "$$\n",
    "which is the determinant of the image lattice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quadratic_grid_test(quad,n):\n",
    "    \"\"\"Compare the subgradient measures with the quadratic form determinant.\n",
    "    (Expected to match in the interior.)\"\"\"\n",
    "    aX = np.linspace(-n,n,2*n+1)\n",
    "    X = np.array(np.meshgrid(aX,aX,indexing='ij'))\n",
    "    \n",
    "    S,Sm = SubgradientMeasures(*FacetGradients(quad(X),X))\n",
    "    meas = np.bincount(S,Sm).reshape(X.shape[1:])\n",
    "    \n",
    "    x_ad = ad.Dense2.identity(constant=[0.,0.])\n",
    "    A = quad(x_ad).coef2\n",
    "    \n",
    "    return meas,np.linalg.det(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choosing $A=\\mathrm{Id}$, all subgradient associated to non-boundary points cells are square, and have unit area, as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[nan, nan, nan, nan, nan, nan, nan],\n",
       "        [nan,  1.,  1.,  1.,  1.,  1., nan],\n",
       "        [nan,  1.,  1.,  1.,  1.,  1., nan],\n",
       "        [nan,  1.,  1.,  1.,  1.,  1., nan],\n",
       "        [nan,  1.,  1.,  1.,  1.,  1., nan],\n",
       "        [nan,  1.,  1.,  1.,  1.,  1., nan],\n",
       "        [nan, nan, nan, nan, nan, nan, nan]]),\n",
       " 1.0)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def quad0(X): return 0.5*(X[0]**2+X[1]**2) + X[1]-2\n",
    "quadratic_grid_test(quad0,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choosing some anisotropic matrix $A$, we see that only cells sufficiently far from the boundary have unit area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[  nan,   nan,   nan,   nan,   nan,   nan,   nan],\n",
       "        [  nan, 0.875, 0.875, 0.875, 0.875, 0.875,   nan],\n",
       "        [  nan, 0.75 , 0.75 , 0.75 , 0.75 , 0.75 ,   nan],\n",
       "        [  nan, 0.75 , 0.75 , 0.75 , 0.75 , 0.75 ,   nan],\n",
       "        [  nan, 0.75 , 0.75 , 0.75 , 0.75 , 0.75 ,   nan],\n",
       "        [  nan, 0.875, 0.875, 0.875, 0.875, 0.875,   nan],\n",
       "        [  nan,   nan,   nan,   nan,   nan,   nan,   nan]]),\n",
       " 0.75)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def quad1(X): return 0.5*(X[0]**2-3*X[0]*X[1]+3*X[1]**2) \n",
    "quadratic_grid_test(quad1,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Optimization, uniformly convex domain\n",
    "\n",
    "For reasons unclear to us, the scipy optimizer `trust-constr` does not appear to like this problem.\n",
    "Indeed, it is likely quite stiff, but on the other hand it is convex. For this reason, we implement by ourselves the logarithmic barriers and penalized objective function.\n",
    "\n",
    "**Case of a uniformly convex domain.**\n",
    "When the discretized domain is uniformly convex, as in the case of a disk, the subgradient measures of the boundary points are guaranteed to be $+\\infty$, hence positive. Therefore, we only need to impose the positivity of the measure of the interior subgradient cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Entropy(S,Sm,ρ=None):\n",
    "    \"\"\"Evaluates - sum_x ρ(x) log(|D_x u|)\"\"\"\n",
    "    _,Xcount = np.unique(S, return_counts=True)\n",
    "\n",
    "    Si = np.argsort(S + Xcount.size*Xcount[S])\n",
    "    S,Sm = S[Si],Sm[Si]\n",
    "\n",
    "    # For efficient sparse AD : distinguish points by how many simplices contain them\n",
    "    begin=0 # Start index, for points contained in a given number of vertices\n",
    "    Ent = 0. # Entropy, to be computed\n",
    "    for count,ncount in zip(*np.unique(Xcount,return_counts=True)):\n",
    "        end = begin+count*ncount\n",
    "        Xc = S[begin:end:count]\n",
    "        Smc = Sm[begin:end].reshape((ncount,count)).sum(axis=1) # Find subgradient areas\n",
    "        Entc = - np.log(Smc) # Entropy, to be integrated\n",
    "        if ρ is not None: Entc = Entc*ρ[Xc]\n",
    "#        if Xbd is not False: Entc[Xbd[Xc]]=0 # Erase entropy associated with boundary cells\n",
    "        ad.simplify_ad(Entc)\n",
    "        begin = end\n",
    "        Ent = Ent+Entc.sum()\n",
    "\n",
    "    return Ent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_uconv(u,X,T,λ0,λ1,**kwargs):\n",
    "    ρ = cellmeasFE(X,T)\n",
    "    T_ = T.reshape(-1)\n",
    "    ρX = np.bincount(T_,ρ[T_])/3\n",
    "    try: return (monopolist_objective(u,X,T,**kwargs) \n",
    "            + λ0*Entropy(*SubgradientMeasures(*FacetGradients(u,X),1),ρ)\n",
    "            - λ1*np.sum(np.log(u)*ρX) )\n",
    "    except ValueError: return np.inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = 0.2*disk_X(5)[0] + 0.6\n",
    "u = 0.5*(X[0]**2+X[1]**2)\n",
    "T=Delaunay(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialization, objective 0.004528000185872112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../agd/AutomaticDifferentiation/Base.py:43: RuntimeWarning: invalid value encountered in log\n",
      "  def log(x):\ty=1./x; return (np.log(x),y,-y**2)\n",
      "/Users/mirebeau/opt/miniconda3/envs/agd-hfm_dev/lib/python3.7/site-packages/ipykernel_launcher.py:7: RuntimeWarning: invalid value encountered in log\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, Newton step 0.03125, objective -0.0164576520214907.\n",
      "Iteration 2, Newton step 0.03125, objective -0.018998595987283637.\n",
      "Iteration 3, Newton step 0.03125, objective -0.019961277431456925.\n",
      "Iteration 4, Newton step 0.0625, objective -0.02075121201382407.\n",
      "Iteration 5, Newton step 0.125, objective -0.021242993589233487.\n",
      "Iteration 6, Newton step 0.25, objective -0.022046671706127632.\n",
      "Iteration 8, Newton step 0.5, objective -0.023796820909843464.\n",
      "Iteration 10, Newton step 1.0, objective -0.02448252160332036.\n",
      "Iteration 12, Newton step 1.0, objective -0.02448406404911364.\n",
      "Iteration 14, Newton step 1.0, objective -0.024484064800353905.\n",
      "Convergence criterion satisfied, terminating.\n",
      "Iteration 15, Newton step 1.0, objective -0.024484064800353946.\n",
      "CPU times: user 20.8 s, sys: 459 ms, total: 21.3 s\n",
      "Wall time: 17.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sol = ad.Optimization.newton_minimize(objective_uconv,u,(X,T,0.01,0.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimal product line is found as the gradient of the solution. Self-competition from the monopolist leads to the artificial creation of rarity, with the final situation being that:\n",
    "- Poor people buy the null product.\n",
    "- Average people buy from a one-dimensional line of products.\n",
    "- Rich people have a product Taylored to their preferences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_u(u,X,scatter=False):\n",
    "    \"\"\"Display the subgradient cells associated with interior discretization points.\"\"\"\n",
    "    _,N,_,Sg = FacetGradients(u,X)\n",
    "    edges = np.array([Sg[:,None]+0*N,Sg[:,N]])\n",
    "    edges = np.moveaxis(edges,0,1).reshape(2,2,-1)\n",
    "    plt.plot(*edges,color='black')\n",
    "    if scatter: plt.scatter(*Sg) # Show the subgradients too\n",
    "    plt.axis('equal');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[5,5])\n",
    "plt.title(\"Optimal monopolist product line,\\n consumers distributed on a disk\")\n",
    "show_u(sol,X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Failed optimization attempt with `trust-constr`**\n",
    "\n",
    "<!---\n",
    "\n",
    "def NonlinearConstraint(f,fargs):\n",
    "    \"\"\"\n",
    "    Represents the constraint : np.bincount(indices,weights) >= 0,\n",
    "    where (indices, weights) = f(x,*fargs)\n",
    "    (Indices may be repeated, and the associated values must be summed.)\n",
    "    \"\"\"\n",
    "    def fun(x):\n",
    "        ind,wei = f(x,*fargs)\n",
    "        return np.bincount(ind,wei)\n",
    "    def grad(x): \n",
    "        ind,wei = f(ad.Sparse.identity(constant=x),*fargs)\n",
    "        triplets = (wei.coef.reshape(-1),(ind.repeat(wei.size_ad),wei.index.reshape(-1)))\n",
    "        return scipy.sparse.coo_matrix(triplets).tocsr()\n",
    "    def hess(x,v): # v is a set of weights, provided by the optimizer\n",
    "        ind,wei = f(ad.Sparse2.identity(constant=x),*fargs)\n",
    "        return np.sum(v[ind]*wei).hessian_operator()\n",
    "    return sciopt.NonlinearConstraint(fun,0.,np.inf,jac=grad,hess=hess,keep_feasible=True)\n",
    "\n",
    "# -----------------\n",
    "\n",
    "X = 0.2*disk_X(5) + 0.5 # Unit disk, centered at (2,2)\n",
    "T = Delaunay(X)\n",
    "u_guess = 0.5*(X[0]**2+X[1]**2) # Strictly convex guess\n",
    "\n",
    "u_ad2 = ad.Sparse2.identity(constant=np.zeros_like(u_guess))\n",
    "obj = QuadraticObjective_AD(monopolist_objective(u_ad2,X,T) )\n",
    "\n",
    "def convexity_constraint(u,X):\n",
    "    try: #return SubgradientMeasures(*FacetGradients(u,X),1)\n",
    "        S,Sm = SubgradientMeasures(*FacetGradients(u,X),1)\n",
    "        return S,100*Sm\n",
    "    except ValueError: return np.arange(len(u)),np.full(len(u),-1.)\n",
    "    \n",
    "nlcons = NonlinearConstraint(convexity_constraint,(X,))\n",
    "\n",
    "# --------------------\n",
    "\n",
    "#%%time\n",
    "sol = sciopt.minimize(x0=u_guess,method='trust-constr',\n",
    "    **obj,\n",
    "    constraints = [nlcons], #\n",
    "    bounds = sciopt.Bounds(0.,np.inf),options={'verbose':1,'maxiter':600})\n",
    "\n",
    "print([obj['fun'] (z) for z in (u_guess,sol.x)]) # decreases ??\n",
    "\n",
    "plt.figure(figsize=[5,5]) # Expected shape ??\n",
    "S,N,Sint,Sg = FacetGradients(sol.x,X)\n",
    "plt.scatter(*Sg)\n",
    "plt.axis('equal');\n",
    "\n",
    "--->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Non-uniformly convex domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interior_subgradients_line(u,X):\n",
    "    \"\"\"Similar to interior_subgradients, except that u is defined \n",
    "    over a family of *aligned* but unordered points X.\"\"\"\n",
    "    v = X[:,0]-X[:,1]; v /= np.linalg.norm(v) # Find the line direction\n",
    "    X = lp.dot_VV(X,v[:,None]) # Compute the points abcissa\n",
    "    ai = np.argsort(X) # Reorder\n",
    "    X = X[ai]; u=u[ai]\n",
    "    Sm = interior_subgradients(u,X) # Subgradient measures, 1D code\n",
    "    ρ = avg(diff(X)) # Weight function\n",
    "    return - np.sum(ρ*np.log(Sm))\n",
    "    \n",
    "    \n",
    "def objective(u,X,DX,T,λ0,λ1,λ2,**kwargs):\n",
    "    return objective_uconv(u,X,T,λ0,λ1,**kwargs) \\\n",
    "        + λ2*sum(interior_subgradients_line(u[F],X[:,F]) for F in DX)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,DX = square_X(10)\n",
    "T=Delaunay(X)\n",
    "X1 = 1+X\n",
    "X2 = 1.5+lp.dot_AV(lp.rotation(np.pi/4)[:,:,None],X-0.5)\n",
    "def quad(X): return 0.5*(X[0]**2+X[1]**2) # Dummy convex funciton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialization, objective 0.021335005660464323\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../agd/AutomaticDifferentiation/Base.py:43: RuntimeWarning: invalid value encountered in log\n",
      "  def log(x):\ty=1./x; return (np.log(x),y,-y**2)\n",
      "/Users/mirebeau/opt/miniconda3/envs/agd-hfm_dev/lib/python3.7/site-packages/ipykernel_launcher.py:7: RuntimeWarning: invalid value encountered in log\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, Newton step 0.00390625, objective -0.8065060747378304.\n",
      "Iteration 2, Newton step 0.001953125, objective -0.8967836497221306.\n",
      "Iteration 3, Newton step 0.001953125, objective -0.9515783707734139.\n",
      "Iteration 4, Newton step 0.001953125, objective -0.9698238754680881.\n",
      "Iteration 5, Newton step 0.00390625, objective -0.9755941979067989.\n",
      "Iteration 6, Newton step 0.0078125, objective -0.9832993756525432.\n",
      "Iteration 8, Newton step 0.03125, objective -1.023596276497147.\n",
      "Iteration 10, Newton step 0.125, objective -1.1747507004297872.\n",
      "Iteration 12, Newton step 0.03125, objective -1.2081214129381739.\n",
      "Iteration 14, Newton step 0.125, objective -1.2704464441736527.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mirebeau/opt/miniconda3/envs/agd-hfm_dev/lib/python3.7/site-packages/scipy/sparse/linalg/dsolve/linsolve.py:206: MatrixRankWarning: Matrix is exactly singular\n",
      "  warn(\"Matrix is exactly singular\", MatrixRankWarning)\n",
      "/Users/mirebeau/opt/miniconda3/envs/agd-hfm_dev/lib/python3.7/site-packages/ipykernel_launcher.py:10: RuntimeWarning: invalid value encountered in log\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 16, Newton step 0.125, objective -1.3571514374498088.\n",
      "Iteration 20, Newton step 1.0, objective -1.4640518988957456.\n",
      "Iteration 24, Newton step 1.0, objective -1.4683237983407287.\n",
      "Convergence criterion satisfied, terminating.\n",
      "Iteration 28, Newton step 1.0, objective -1.4683240240627677.\n",
      "CPU times: user 42.6 s, sys: 606 ms, total: 43.2 s\n",
      "Wall time: 36 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sol1 = ad.Optimization.newton_minimize(objective,quad(X1),(X1,DX,T,0.01,0.01,0.001),step_min = 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialization, objective 0.02132684403943591\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mirebeau/opt/miniconda3/envs/agd-hfm_dev/lib/python3.7/site-packages/ipykernel_launcher.py:7: RuntimeWarning: invalid value encountered in log\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, Newton step 0.00390625, objective -0.8208427543339621.\n",
      "Iteration 2, Newton step 0.0078125, objective -1.260151897064709.\n",
      "Iteration 3, Newton step 0.015625, objective -1.2848047656192387.\n",
      "Iteration 4, Newton step 0.03125, objective -1.2933757408499496.\n",
      "Iteration 5, Newton step 0.0625, objective -1.308806870841307.\n",
      "Iteration 6, Newton step 0.125, objective -1.3345738066485004.\n",
      "Iteration 8, Newton step 0.25, objective -1.398310662861263.\n",
      "Iteration 10, Newton step 1.0, objective -1.4665347792203143.\n",
      "Iteration 12, Newton step 1.0, objective -1.4773455120328294.\n",
      "Iteration 14, Newton step 1.0, objective -1.4773455180286688.\n",
      "Convergence criterion satisfied, terminating.\n",
      "Iteration 16, Newton step 1.0, objective -1.477345518028669.\n",
      "CPU times: user 24 s, sys: 434 ms, total: 24.4 s\n",
      "Wall time: 20.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sol2 = ad.Optimization.newton_minimize(objective,quad(X2),(X2,DX,T,0.01,0.01,0.001),step_min = 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[11,5])\n",
    "\n",
    "plt.subplot(121); \n",
    "plt.title(\"Optimal monopolist product line,\\n consumers distributed on a square\");\n",
    "show_u(sol1,X1)\n",
    "\n",
    "plt.subplot(122); \n",
    "plt.title(\"Optimal monopolist product line,\\n consumers distributed on a rotated square\");\n",
    "show_u(sol2,X2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,DX = triangle_X(10)\n",
    "T=Delaunay(X)\n",
    "X1 = 1+ 0.7*lp.dot_AV(lp.rotation(np.pi/4)[:,:,None],X)\n",
    "X2 = 1+ 0.7*lp.dot_AV(lp.rotation(np.pi/4-np.pi/3)[:,:,None],X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialization, objective 0.018415709707352526\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../agd/AutomaticDifferentiation/Base.py:43: RuntimeWarning: invalid value encountered in log\n",
      "  def log(x):\ty=1./x; return (np.log(x),y,-y**2)\n",
      "/Users/mirebeau/opt/miniconda3/envs/agd-hfm_dev/lib/python3.7/site-packages/ipykernel_launcher.py:7: RuntimeWarning: invalid value encountered in log\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, Newton step 0.0078125, objective -0.18528731552021108.\n",
      "Iteration 2, Newton step 0.015625, objective -0.31795318899562264.\n",
      "Iteration 3, Newton step 0.015625, objective -0.3290278691751811.\n",
      "Iteration 4, Newton step 0.03125, objective -0.3372893868151492.\n",
      "Iteration 5, Newton step 0.0625, objective -0.3431631782283434.\n",
      "Iteration 6, Newton step 0.125, objective -0.3522332625963088.\n",
      "Iteration 8, Newton step 0.5, objective -0.3808599626586719.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mirebeau/opt/miniconda3/envs/agd-hfm_dev/lib/python3.7/site-packages/scipy/sparse/linalg/dsolve/linsolve.py:206: MatrixRankWarning: Matrix is exactly singular\n",
      "  warn(\"Matrix is exactly singular\", MatrixRankWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10, Newton step 1.0, objective -0.39569294335070115.\n",
      "Iteration 12, Newton step 1.0, objective -0.39796922825961956.\n",
      "Iteration 14, Newton step 1.0, objective -0.39796922999163575.\n",
      "Convergence criterion satisfied, terminating.\n",
      "Iteration 16, Newton step 0.5, objective -0.3979692299916357.\n",
      "CPU times: user 16.8 s, sys: 421 ms, total: 17.3 s\n",
      "Wall time: 13.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sol1 = ad.Optimization.newton_minimize(objective,quad(X1),(X1,DX,T,0.01,0.01,0.001),step_min = 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialization, objective 0.018518821260304817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mirebeau/opt/miniconda3/envs/agd-hfm_dev/lib/python3.7/site-packages/ipykernel_launcher.py:7: RuntimeWarning: invalid value encountered in log\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, Newton step 0.00390625, objective -0.06312327903629611.\n",
      "Iteration 2, Newton step 0.00390625, objective -0.1093586177990498.\n",
      "Iteration 3, Newton step 0.00390625, objective -0.13376842542503625.\n",
      "Iteration 4, Newton step 0.00390625, objective -0.1410787453124801.\n",
      "Iteration 5, Newton step 0.0078125, objective -0.14694456436687744.\n",
      "Iteration 6, Newton step 0.015625, objective -0.1550315085671805.\n",
      "Iteration 8, Newton step 0.0625, objective -0.19926852485113888.\n",
      "Iteration 10, Newton step 0.0625, objective -0.24879998790661575.\n",
      "Iteration 12, Newton step 0.25, objective -0.3102062881813852.\n",
      "Iteration 14, Newton step 0.5, objective -0.35483877843590983.\n",
      "Iteration 16, Newton step 1.0, objective -0.37256470932408914.\n",
      "Iteration 20, Newton step 1.0, objective -0.37452920708491194.\n",
      "Convergence criterion satisfied, terminating.\n",
      "Iteration 23, Newton step 1.0, objective -0.3745292070866183.\n",
      "CPU times: user 23.5 s, sys: 702 ms, total: 24.2 s\n",
      "Wall time: 18.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sol2 = ad.Optimization.newton_minimize(objective,quad(X2),(X2,DX,T,0.01,0.01,0.001),step_min = 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[11,5])\n",
    "plt.subplot(121)\n",
    "plt.title(\"Optimal monopolist product line,\\n consumers distributed on a triangle\");\n",
    "show_u(sol1,X1)\n",
    "\n",
    "plt.subplot(122) \n",
    "plt.title(\"Optimal monopolist product line,\\n consumers distributed on a rotated triangle\");\n",
    "show_u(sol2,X2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Performance analysis.**\n",
    "The computation time is dominated by the AD methods, which are indeed a bit slow, yet the room for improvement limited due to the linear solves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.99 ms, sys: 1.59 ms, total: 5.58 ms\n",
      "Wall time: 4.42 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "u = quad(X2)\n",
    "obj = objective(u,X2,DX,T,0.01,0.01,0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 493 ms, sys: 16 ms, total: 509 ms\n",
      "Wall time: 311 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sol2_ad = ad.Sparse2.identity(constant=u)\n",
    "obj_ad = objective(sol2_ad,X2,DX,T,0.01,0.01,0.001);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 113 ms, sys: 1.97 ms, total: 114 ms\n",
      "Wall time: 113 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "step = obj_ad.solve_stationnary();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Two dimensional, linear, monopolist\n",
    "\n",
    "In this section, we constrain the product line of the monopolist within a given convex polygonal region $Y$.\n",
    "This can be implemented in several ways:\n",
    "- (Finite element approach) Estimate the gradients using the finite element method, and constrain them within $Y$.\n",
    "- (Subgradient approach) Compute the subgradients associated with the boundary cells, and require that their intersection with $Y$ has positive area.\n",
    "\n",
    "Both approaches are consistent. The first one is easier to implement, whereas the second one is more in the spirit of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Finite element approach\n",
    "\n",
    "This approach amounts to linear constraints since:\n",
    "- the convex polygonal region $Y$ can be regarded as a finite intersection of half-spaces.\n",
    "- the finite element gradients have linear components.\n",
    "\n",
    "**Should we constrain the gradients associated to interior triangles ?**\n",
    "There is no definite answer to this question. On the one hand, since the solution $u$ is convex, its gradient can only be extremal on the domain boundary, which suggests discarding the gradients associated to interior triangles.\n",
    "On the other hand, the discretized unknown $u$, yields a non-convex function when interpolated on the finite element triangulation (although there exists a triangulation onto which the interpolation is convex)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def det(u,v): return u[0]*v[1]-u[1]*v[0]\n",
    "def gradient_constraints(u,X,T,Y):\n",
    "    \"\"\"Y lists the vertices, counter-clockwise, of the given convex region. \n",
    "    (Repeat the endpoint for a bounded region.*)\"\"\"\n",
    "    g = gradientFE(u,X,T)\n",
    "    dY = np.roll(Y,-1,axis=1)-Y\n",
    "    gY = g[:,None]-Y[:,:,None]\n",
    "    return det(dY[:,:,None],gY)\n",
    "\n",
    "def boundary_triangulation(T,DX):\n",
    "    \"\"\"Keep only triangles containing a boundary vertex.\"\"\"\n",
    "    if np.ndim(DX)>1: DX = np.logical_or.reduce(DX,axis=0) # Gather all boundary faces\n",
    "    return T[:,np.logical_or.reduce(DX[T],axis=0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "nX=11\n",
    "X,DX = square_X(nX-1)\n",
    "T = Delaunay(X)\n",
    "Tbd = boundary_triangulation(T,DX)\n",
    "Y = np.array([(1,0),(1,1),(0,1),(0,0)]).T\n",
    "def guess(X): return 0.1+0.25*((X[0]**2+X[1]**2)+X[0]+X[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.triplot(*X,Tbd.T)\n",
    "plt.scatter(*Y)\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_YFE(u,X,DX,T,Tbd,Y,λ0,λ1,λ2,λ3):\n",
    "    \"\"\"Objective function where the gradient\n",
    "    is constrained within a set Y\"\"\"\n",
    "    ρbd = cellmeasFE(X,Tbd)\n",
    "    return objective(u,X,DX,T,λ0,λ1,λ2,quadratic=False) \\\n",
    "        -λ3*np.sum(ρbd*np.log(gradient_constraints(u,X,Tbd,Y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialization, objective -0.021317362821699434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../agd/AutomaticDifferentiation/Base.py:43: RuntimeWarning: invalid value encountered in log\n",
      "  def log(x):\ty=1./x; return (np.log(x),y,-y**2)\n",
      "/Users/mirebeau/opt/miniconda3/envs/agd-hfm_dev/lib/python3.7/site-packages/scipy/sparse/linalg/dsolve/linsolve.py:206: MatrixRankWarning: Matrix is exactly singular\n",
      "  warn(\"Matrix is exactly singular\", MatrixRankWarning)\n",
      "/Users/mirebeau/opt/miniconda3/envs/agd-hfm_dev/lib/python3.7/site-packages/ipykernel_launcher.py:6: RuntimeWarning: invalid value encountered in log\n",
      "  \n",
      "/Users/mirebeau/opt/miniconda3/envs/agd-hfm_dev/lib/python3.7/site-packages/ipykernel_launcher.py:7: RuntimeWarning: invalid value encountered in log\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, Newton step 0.00390625, objective -0.08348597981683419.\n",
      "Iteration 2, Newton step 0.00390625, objective -0.11963657250280649.\n",
      "Iteration 3, Newton step 0.00390625, objective -0.13786512510388116.\n",
      "Iteration 4, Newton step 0.0078125, objective -0.1620544379140475.\n",
      "Iteration 5, Newton step 0.015625, objective -0.2105725274235191.\n",
      "Iteration 6, Newton step 0.015625, objective -0.25428365539508146.\n",
      "Iteration 8, Newton step 0.0625, objective -0.38506952987162574.\n",
      "Iteration 10, Newton step 0.125, objective -0.43253859750717805.\n",
      "Iteration 12, Newton step 0.25, objective -0.45100900217990153.\n",
      "Iteration 14, Newton step 1.0, objective -0.45913368134409543.\n",
      "Iteration 16, Newton step 1.0, objective -0.4595992503551368.\n",
      "Iteration 20, Newton step 1.0, objective -0.459617838296294.\n",
      "Convergence criterion satisfied, terminating.\n",
      "Iteration 21, Newton step 0.5, objective -0.4596178382962939.\n",
      "CPU times: user 38.6 s, sys: 571 ms, total: 39.2 s\n",
      "Wall time: 27.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sol = ad.Optimization.newton_minimize(objective_YFE,guess(X),(X,DX,T,Tbd,Y,0.01,0.01,0.001,0.001),step_min = 0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This specific problem is known to have an explicit solution, which is piecewise linear, and whose gradients are the extreme points $(0,0),(1,0),(0,1),(1,1)$ of the set $Y$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exact_sol(x,y):\n",
    "    \"\"\"Exact solution for the linear monopolist model, \n",
    "    with uniform density over a square.\"\"\"\n",
    "    a=2/3; b=(4-np.sqrt(2))/3\n",
    "    return np.maximum.reduce((0*x,x-a,y-a,x+y-b))\n",
    "\n",
    "aX = np.linspace(0,1)\n",
    "X0 = np.array(np.meshgrid(aX,aX,indexing='ij'))\n",
    "\n",
    "plt.title(\"Exact solution to the linear model, on a square\")\n",
    "plt.contourf(*X0,exact_sol(*X0))\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the coarse grid size, and the inaccuracies introduced by the numerical method, we only obtain some approximation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_u(sol,X,scatter=True) # Gradients are expected to concentrate in the corners."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Somehow approximates the exact solution, which is piecewise linear\n",
    "plt.contourf(*X.reshape(2,nX,nX),sol.reshape(nX,nX),levels=20)\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Intersected subgradient approach (*TODO*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}