{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d89fe1f6",
   "metadata": {},
   "source": [
    "# Adaptive PDE discretizations on Cartesian grids\n",
    "## Volume : Divergence form PDEs\n",
    "## Part : Primal-Dual optimization\n",
    "## Chapter : Vector unbalanced optimal transport based on the Beckman formula\n",
    "\n",
    "$\n",
    "\\newcommand\\bR{\\mathbb{R}}\n",
    "\\def\\vp{\\varphi}\n",
    "\\def\\ve{\\epsilon}\n",
    "\\def\\<{\\langle} \\def\\>{\\rangle}\n",
    "\\def\\diff{\\mathrm{d}}\n",
    "\\def\\cF{\\mathcal{F}}\n",
    "\\DeclareMathOperator*\\argmin{argmin}\n",
    "\\DeclareMathOperator\\prox{prox}\n",
    "\\DeclareMathOperator\\pinv{pinv}\n",
    "\\DeclareMathOperator\\span{span}\n",
    "\\DeclareMathOperator\\step{step}\n",
    "\\DeclareMathOperator\\diver{div}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6a7df4",
   "metadata": {},
   "source": [
    "We implement a relaxed form of matrix optimal transport, based on the Bellman formulation of the $1$-Wasserstein distance, with a squared penalty term for the constraint.\n",
    "\n",
    "### Beckman minimal flow formula\n",
    "\n",
    "The 1-Wasserstein optimal transport distance, between two densities $\\mu$ and $\\nu$ expressed using the *Beckmann minimal flow formula* as\n",
    "$$\n",
    "    W^1(\\mu,\\nu) = \\inf_\\sigma \\{\\int |\\sigma|; \\diver \\sigma = \\mu-\\nu\\}.\n",
    "$$\n",
    "**Boundary condition.** We assume that $\\sigma \\cdot n = 0$, i.e. no mass flows inside or outside the domain. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54844aba",
   "metadata": {},
   "source": [
    "### Relaxation of the transport constraint\n",
    "\n",
    "A relaxation of the *Beckmann minimal flow formula* can be defined for any (large) $\\lambda>0$ and any exponent $p\\in [1,\\infty[$ as \n",
    "$$\n",
    "    W^1_{\\lambda,p}(\\mu,\\nu) := \\inf_\\sigma \\int |\\sigma| + \\frac \\lambda p \\int |\\diver \\sigma +\\nu-\\mu |^p.\n",
    "$$\n",
    "Such a relaxation is necessary if the masses of $\\mu$ and $\\nu$ are distinct.\n",
    "\n",
    "The choice $p=1$ is easily the most natural when $\\mu$ and $\\nu$ are densities. \n",
    "However, in the intended application, $\\mu$ and $\\nu$ are seismograms, which means that the squares $\\mu^2$ and $\\nu^2$ can be regarded as energy densities. This, together with disappointing experiments in the $p=1$ case, leads us to focus on $p=2$ in this notebook.\n",
    "\n",
    "In the following, we denote $\\xi := \\nu - \\mu$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3d35ad",
   "metadata": {},
   "source": [
    "### Vector extension\n",
    "The Beckmann formula admits a straightforward generalizations to vector valued measures $\\mu$ and $\\nu$, and so do their unbalanced extensions (which are typically preferable in this context since the mass balance equation is usually not satisfied).\n",
    "\n",
    "In order to avoid transporting a measure onto itself, it is natural to assume that $\\mu$ and $\\nu$ are valued within a cone of vectors, such as the non-negative vectors $\\bR_+^d \\subset \\bR^d$, or the non-negative symmetric matrices $S_d^+ \\subset S_d$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6571899d",
   "metadata": {},
   "source": [
    "### The Chambolle-Pock primal-dual optimization algorithm\n",
    "\n",
    "This algorithm addresses saddle point problems of the form\n",
    "$$\n",
    "    \\inf_{x\\in X} \\sup_{y\\in Y} L(x,y) := \\<K x,y\\> + f(x) - g^*(y),\n",
    "$$\n",
    "where $X,Y$ are Hilbert spaces, where $K$ is a linear operator, and $f$ and $g$ are convex functions, subject to mild assumptions.\n",
    "\n",
    "The saddle point problem defined by the *Lagrangian* $L(x,y)$ is equivalent to the 'primal' minimization of $E$, and 'dual' maximization of $E^*$, where\n",
    "$$\n",
    "    \\inf_{x\\in X} E(x) := f(x)+g(K x)\n",
    "$$\n",
    "and \n",
    "$$\n",
    "    \\sup_{y\\in Y} E^*(y) := - (g^*(y)+f^*(-K^\\top y)).\n",
    "$$\n",
    "One always has $E(x) \\geq E^*(y)$, with equality when a $(x,y)$ is a saddle point.\n",
    "\n",
    "The algorithm, which simultaneously minimizes $E$ and maximizes $E^*$, reads in its basic version\n",
    "$$\n",
    "    x^{n+1} = \\prox_{\\tau_1 f} (x^n - \\tau_1 K^\\top y^n),\\quad\n",
    "    \\overline x^{n+1} = 2 x^{n+1} - x^n,\\quad\n",
    "    y^{n+1} = \\prox_{\\tau_2 g^*}(y^n + \\tau_2 K \\overline x^{n+1}).\n",
    "$$\n",
    "Running this algorithm requires an implementation of \n",
    "- the linear operators $K$ and $K^\\top$, and time steps $\\tau_1>0$, $\\tau_2>0$, such that $\\tau_1\\tau_2 < \\|K\\|^2$.\n",
    "- the proximal operators $\\prox_{\\tau f}$ and $\\prox_{\\tau g}$\n",
    "- optionally, the functionals $f$, $f^*$, $g$ and $g^*$, which allows using the primal dual gap as a stopping criterion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c077f80",
   "metadata": {},
   "source": [
    "### Primal-dual formulation, explicit\n",
    "\n",
    "We consider the *Lagrangian*\n",
    "$$\n",
    "    L(\\vp,\\sigma) := \\int \\<\\nabla \\vp,\\sigma\\> + (\\frac 1 {2\\lambda} \\vp^2 - \\xi \\vp) - |\\sigma|.\n",
    "$$\n",
    "In other words, with the $x=\\vp \\in L^2$, $y=\\sigma\\in (L^2)^d$, and \n",
    "$$\n",
    "    K = \\nabla,\\quad f(\\vp):=\\int_\\Omega \\frac 1 {2\\lambda} \\vp^2-\\vp \\xi,\\quad g^*(\\sigma) = \\int_\\Omega |\\sigma|.\n",
    "$$\n",
    "Note that the numerical scheme implementing $K$ is bounded in terms of the grid scale, even though the linear operator $K$ itself is unbounded in the continuous domain. The implementation of this approach does not require solving any optimization problem, hence it is referred to as 'explicit'.\n",
    "\n",
    "\n",
    "<!---\n",
    "\\begin{align*}\n",
    "    L(\\vp,\\sigma) \n",
    "    &:= \\int \\<\\nabla \\vp,\\sigma\\> + (\\frac 1 {2\\lambda} \\vp^2 - \\xi \\vp) - |\\sigma|,\\\\\n",
    "    &:= - \\int |\\sigma| + \\<\\diver\\sigma+\\xi,\\vp\\> - \\frac 1 {2\\lambda} \\vp^2,\n",
    "\\end{align*}\n",
    "where the equality follows from the adjointness of gradient and negative divergence.\n",
    "--->\n",
    "\n",
    "The corresponding *primal energy*,  is obtained as \n",
    "$$\n",
    "    E(\\vp) := \\sup_\\sigma L(\\sigma,\\vp) = \\int \\frac 1 {2\\lambda} \\vp^2 - \\vp \\xi + \\chi_{|\\nabla \\vp|\\leq 1},\n",
    "$$\n",
    "where the characteristic function $\\chi_{|\\nabla \\vp|\\leq 1}$ vanishes if $|\\nabla \\vp|\\leq 1$, and equals $\\infty$ otherwise.\n",
    "\n",
    "Noting that, by the adjointness of gradient and negative divergence,\n",
    "$$\n",
    "L(\\vp,\\sigma) = - \\int |\\sigma| + \\<\\diver\\sigma+\\xi,\\vp\\> - \\frac 1 {2\\lambda} \\vp^2,\n",
    "$$\n",
    "we obtain the *dual energy* as \n",
    "$$\n",
    "    E^*(\\sigma) := \\inf_\\vp L(\\sigma,\\vp) = -\\int |\\sigma| + \\frac \\lambda 2  |\\diver \\sigma +\\xi |^2,\n",
    "$$\n",
    "thus $E^*$ is (the opposite of) the energy defining the problem of interest.\n",
    "\n",
    "<!---\n",
    "The problem that we would like to address is thus, \n",
    "$$\n",
    "    \\inf_\\vp E(\\sigma) = \\inf_\\vp \\sup_\\sigma L(\\sigma,\\vp) = \\sup_\\sigma E^*(\\vp),\n",
    "$$\n",
    "by the usual duality argument, which we do not attempt to justify here.\n",
    "\n",
    "We rely on the Chambolle-Pock primal dual optimization algorithm. Note that $\\frac 1 {2\\lambda} \\vp^2$ is a strongly convex term, which allows the usage of an accelerated variant. Alternative methods could be used as well, such as the forward backward algorithm.\n",
    "\n",
    "**Brouillon**\n",
    "For the problem of interest,\n",
    "$$\n",
    "    K = \\nabla,\\quad f(\\vp):=\\int_\\Omega \\frac 1 {2\\lambda} \\vp^2-\\vp \\xi,\\quad g^*(\\sigma) = \\int_\\Omega |\\sigma|.\n",
    "$$\n",
    "We present below some implementations of $K$, $K^\\top$, $\\prox_f$, $\\prox_{g^*}$, as needed to run the optimization algorithm.\n",
    "\n",
    "$$\n",
    "    \\inf_\\vp \\sup_\\sigma \\<K \\sigma,\\vp\\> + f(\\sigma) - g^*(\\vp),\n",
    "$$\n",
    "where $K$ is a linear operator, and $f$ and $g$ are convex functions.\n",
    "--->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b82d88",
   "metadata": {},
   "source": [
    "### Primal dual formulation, implicit\n",
    "\n",
    "We can define $K=K^\\top=\\mathrm{Id}$, and let $x=\\sigma$, $y=\\eta$, and \n",
    "$$\n",
    "    f(\\sigma):= \\int_\\Omega |\\sigma|,\\quad g(\\sigma) := \\int \\frac \\lambda 2  |\\diver \\sigma +\\xi |^2.\n",
    "$$\n",
    "The computation of $g^*$ and $\\prox_g$ is discussed below. The approach allows using larger proximal time steps, but requires the inversion of a linear system (which may be accelerated using a fast Fourier transform), hence it is referred to as 'implicit'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb5489b",
   "metadata": {},
   "source": [
    "### Numerical implementation, and discussion of efficiency\n",
    "\n",
    "We assume that $\\mu$ and $\\nu$ are supported over a rectangular domain, and use staggered finite differences, similarly to the mincut problem. Reflected boundary conditions on $\\sigma$ ensure that no mass flows outside the domain. \n",
    "\n",
    "The numerical implementations presented below are not state of the art, and are likely not suitable for realistic applications. They should only be regarded as experimentation and tutorial on the topic of primal dual optimization. More precisely, on large test cases:\n",
    "- The implicit discretization suffers from the choice of linear solver. An approach based on the FFT would be applicable and much more efficient.\n",
    "- The explicit discretization (cpu or gpu) suffers from the excessive number of time steps, up to hundreds of thousands, which increases as the grid scale decreases and makes numerical cost excessive. The gpu implementation also suffers from the limited accuracy of `float32` scalars. \n",
    "\n",
    "<!---\n",
    "### $H^{-1}$ variant\n",
    "The following quantity\n",
    "$$\n",
    "    H^{-1}_\\lambda(\\mu,\\nu)^2 := \\inf_\\sigma \\int |\\sigma|^2 + \\lambda \\int |\\nabla \\cdot \\sigma -( \\mu-\\nu) |^2,\n",
    "$$\n",
    "is equivalent to the Sobolev $H^{-1}$ norm of $\\mu-\\nu$. Its computation only requires solving a single linear system, discretizing a Poisson equation. \n",
    "--->\n",
    "### References\n",
    "\n",
    "- F. Santambrogio, (2015). *Optimal transport for applied mathematicians*. Birkäuser, NY, 55(58-63), 94.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199fb35a",
   "metadata": {},
   "source": [
    "[**Summary**](Summary.ipynb) of volume Divergence form PDEs, this series of notebooks.\n",
    "\n",
    "[**Main summary**](../Summary.ipynb) of the Adaptive Grid Discretizations \n",
    "\tbook of notebooks, including the other volumes.\n",
    "\n",
    "# Table of contents\n",
    "  * [1. Implementation of the operators](#1.-Implementation-of-the-operators)\n",
    "    * [1.1 Proximal operator of the quadratic term in the explicit formulation](#1.1-Proximal-operator-of-the-quadratic-term-in-the-explicit-formulation)\n",
    "    * [1.2 Proximal operator of the Euclidean norm](#1.2-Proximal-operator-of-the-Euclidean-norm)\n",
    "    * [1.3 Gradient](#1.3-Gradient)\n",
    "    * [1.4 Implicit formulation](#1.4-Implicit-formulation)\n",
    "  * [2. One dimensional transport](#2.-One-dimensional-transport)\n",
    "    * [2.1 Running the primal-dual optimizers](#2.1-Running-the-primal-dual-optimizers)\n",
    "    * [2.2 Comparing with exact solutions](#2.2-Comparing-with-exact-solutions)\n",
    "    * [2.3 Vector data](#2.3-Vector-data)\n",
    "  * [3. Two dimensions](#3.-Two-dimensions)\n",
    "    * [3.1 From a Dirac mass to another](#3.1-From-a-Dirac-mass-to-another)\n",
    "    * [3.2 Oscillating signals and lift](#3.2-Oscillating-signals-and-lift)\n",
    "    * [3.3 A loss function between seismograms](#3.3-A-loss-function-between-seismograms)\n",
    "\n",
    "\n",
    "\n",
    "**Acknowledgement.** Some of the experiments presented in these notebooks are part of \n",
    "ongoing research with Ludovic Métivier and Da Chen.\n",
    "\n",
    "Copyright Jean-Marie Mirebeau, Centre Borelli, ENS Paris-Saclay, CNRS, University Paris-Saclay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44fde98e",
   "metadata": {},
   "source": [
    "## 0. Importing the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "dca1349e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.path.insert(0,\"..\") # Allow import of agd from parent directory (useless conda package is installed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "885230ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agd import AutomaticDifferentiation as ad\n",
    "from agd import LinearParallel as lp\n",
    "from agd.ODE.proximal import chambolle_pock\n",
    "norm = ad.Optimization.norm\n",
    "from agd import Plotting\n",
    "from agd import Metrics\n",
    "from agd import FiniteDifferences as fd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "afa6d960",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agd.Eikonal.HFM_CUDA.MinCut import stag_grids\n",
    "from agd.ExportedCode.Notebooks_Div.Prox_MinCut import drop, grad_ndiv_operators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80238f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np; xp=np\n",
    "from matplotlib import pyplot as plt\n",
    "import scipy.sparse.linalg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40a18fb",
   "metadata": {},
   "source": [
    "### 0.1 Additional configuration\n",
    "\n",
    "Uncomment the following line to use the gpu implementation (requires a cuda enabled GPU).\n",
    "\n",
    "<!---\n",
    "In addition to the CPU implementation described below, a (reasonnably) optimized GPU implementation of the explicit formulation is provided.\n",
    "\n",
    "The GPU implementation offers a 300x speedup, for sufficiently large instances, and is also (reasonnably) simple. It may be used as a starting point for GPU implementations of the Chambolle-Pock optimization algorithm in other settings.\n",
    "--->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2fb02ee",
   "metadata": {
    "tags": [
     "GPU_config"
    ]
   },
   "outputs": [],
   "source": [
    "#import cupy as xp; from agd.Eikonal.HFM_CUDA import BeckmanOT "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73635e5e",
   "metadata": {},
   "source": [
    "The CPU and GPU implementations are meant to be consistent up to machine precision. The following method checks this property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c43eb599",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_cpu_gpu(data_cpu,maxiter=20,data_gpu=None,**kwargs):\n",
    "    if xp is np: return # No GPU available\n",
    "    res_cpu,data = solve1_ot(*data_cpu,maxiter=maxiter,E_rtol=0,**kwargs) # Run on the cpu\n",
    "\n",
    "    # Run on the gpu\n",
    "    (λ,ξ,dx) = data_cpu if data_gpu is None else data_gpu \n",
    "    res_gpu = BeckmanOT.solve_ot(λ,xp.asarray(ξ,dtype=np.float32),dx,maxiter=maxiter,E_rtol=0,stop_period=1,**kwargs)\n",
    "\n",
    "    # Compare the results\n",
    "    ops,tmp = res_cpu['ops'],res_cpu['tmp']\n",
    "    E_primal,E_dual = ops['E_primal'],ops['E_dual']\n",
    "    def norminf(a): return np.max(np.abs(a))\n",
    "    print(\"ϕ max error : \",norminf(res_gpu[\"ϕ\"].get()-res_cpu['x']))\n",
    "    print(\"σ max error : \",norminf(res_gpu[\"σ\"].get()-res_cpu['y']))\n",
    "    print(f\"primal value. GPU={res_gpu['primal_values'][-1]}, CPU={E_primal(tmp['x_'])}\")\n",
    "    print(f\"dual value. GPU={res_gpu['dual_values'][-1]}, CPU={E_dual(tmp['yold'])}\")\n",
    "\n",
    "    tols={'rtol':1e-5,'atol':1e-5}; \n",
    "    assert np.allclose(res_gpu['dual_values'][-1],E_dual(tmp['yold']),**tols)\n",
    "    assert np.allclose(res_gpu['primal_values'][-1],E_primal(tmp[\"x_\"]),**tols)\n",
    "    assert np.allclose(res_gpu[\"ϕ\"].get(),res_cpu['x'],**tols)\n",
    "    assert np.allclose(res_gpu[\"σ\"].get(),res_cpu['y'],**tols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95fb45a4",
   "metadata": {},
   "source": [
    "## 1. Implementation of the operators\n",
    "\n",
    "Two of the proximal operators that we implement are related to quadratic functions. For generality, consider a generic  inhomogeneous quadratic function $g$ defined as \n",
    "$$\n",
    "    g(z) = \\tfrac 1 2 z^\\top A z + b^\\top z + c,\n",
    "$$\n",
    "where $A$ is positive semi-definite, $b$ is a vector, and $c$ is a constant.\n",
    "Then one easily checks that\n",
    "$$\n",
    "        \\prox_{\\tau g}(z) = (1+\\tau A)^{-1} (z-\\tau b), \\quad g^*(z) = \\frac 1 2 (z-b)(\\epsilon+A)^{-1}(z-b) - c\n",
    "$$\n",
    "where we introduced a relaxation parameter $\\ve\\geq 0$. Note that the exact Legendre-Fenchel dual is obtained when $\\ve=0$, but is finitely valued only if $A$ is symmetric positive definite. If this condition is not satisfied, then we use $\\ve >0$ numerically. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547c6ccd",
   "metadata": {},
   "source": [
    "### 1.1 Proximal operator of the quadratic term in the explicit formulation\n",
    "\n",
    "Observe that \n",
    "$$\n",
    "    f(\\vp):=\\int_\\Omega \\frac 1 {2\\lambda} \\vp^2-\\vp \\xi,\n",
    "$$\n",
    "is a separable function of the pointwise values of $\\vp$, defined pointwise by a quadratic expression.\n",
    "Therefore we can compute pointwise the related proximal operator and Legendre-Fenchel dual, using the previous formulas.\n",
    "\n",
    "<!---\n",
    "We recognize a quadratic function, Consider the quadratic function \n",
    "$$\n",
    "    f_0(\\vp) := \\frac 1 {2\\lambda} \\vp^2 - \\xi \\vp.\n",
    "$$\n",
    "The proximal operator of an isotropic quadratic function admits an explicit expression, which is easily computed: the quantity\n",
    "$$\n",
    "    \\eta \\mapsto \\frac 1 2 (\\eta-\\vp)^2 + \\tau \\Big(\\frac 1 {2\\lambda} \\eta^2 - \\xi \\eta\\Big)\n",
    "$$\n",
    "is minimized when $\\eta - \\vp + \\tau\\eta/\\lambda - \\tau\\xi$, equivalently\n",
    "$$\n",
    "\\prox_{\\tau f}(\\phi) = \\eta = (1+\\tau/\\lambda)^{-1}(\\vp+\\tau \\xi).\n",
    "$$\n",
    "Likewise, we compute the Legendre-Fenchel dual.\n",
    "\n",
    "onsider the pointwise expression of $f$, which reads \n",
    "A similar approach is applied to the \n",
    "--->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0abc481",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_f(λ,ξ):\n",
    "    def f(ϕ): return (ϕ**2/(2.*λ)-ξ*ϕ).sum()\n",
    "    def fs(ϕ): return ((λ/2.)*(ϕ+ξ)**2).sum()\n",
    "    def prox_f(ϕ,τ): return (ϕ+τ*ξ)/(1+τ/λ)\n",
    "    return f,fs,prox_f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f82674",
   "metadata": {},
   "source": [
    "### 1.2 Proximal operator of the Euclidean norm\n",
    "\n",
    "The function $g^*(\\sigma)$ is defined as the integration over $x\\in \\Omega$ of the norm of $\\sigma(x)$.\n",
    "We choose to implement using a *metric* object for extensibility, allowing for instance the generalization\n",
    "$$\n",
    "    g^*(\\sigma) := \\int \\cF_x(\\sigma(x)) \\diff x\n",
    "$$\n",
    "which involves a (Isotropic, Riemannian, Finsler, etc) metric over the domain $\\Omega$. See the [notebook on the mincut problem](Prox_MinCut.ipynb) on the choice of metric in a related setting.\n",
    "\n",
    "The function $g^*$ is separable, hence the dual function and the proximal operator can be computed pointwise.\n",
    "- The Legendre-Fenchel dual to a norm $g_0^*(x) = \\cF_0(x)$ is the characteristic function $g_0 = \\chi_{B^*}$ of the unit ball $B$ of the dual metric $\\cF^*_0(x)$ (note that norm duality is not the same as Legendre-Fenchel duality). In order to get finite result, we implement a relaxation of this characteristic function. This relaxation only affects the stopping criterion of the optimization algorithm.\n",
    "- The dual proximal operator $\\prox_{\\tau g_0}$ is the othogonal projection onto $B^*$, and the proximal operator $\\prox_{\\tau g_0^*}$ can be computed using the Moreau formula.\n",
    "\n",
    "<!---\n",
    "$$\n",
    "    abs(\\sigma) := |\\sigma|, \\abs^*(\\sigma):=\\chi_{[-1,1]}(\\sigma), \\prox_\n",
    "$$\n",
    "--->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "58b18304",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_gs(multichannel_depth=0,relax_norm_constraint=0.01):\n",
    "    \"\"\"\n",
    "    Inputs : \n",
    "    - multichannel_depth : the number of additional dimensions for multichannel inputs\n",
    "    - relax_norm_constraint : a small relaxation parameter used in the characteristic function approximation\n",
    "    \"\"\"\n",
    "    def rsh(σ): return σ.reshape((-1,*σ.shape[1+multichannel_depth:])) \n",
    "    metric = Metrics.Isotropic(1.)\n",
    "    prox = metric.make_prox() # Proximal operator of the norm\n",
    "    proj = metric.make_proj_dual() # Projection onto the dual unit ball\n",
    "    \n",
    "    def gs(σ):  return metric.norm(rsh(σ)).sum()\n",
    "    def g(σ): # Some (arbitrary) relaxation of the characteristic function of the ball\n",
    "        violation = np.maximum(0,metric.norm(rsh(σ))-1) # How much the norm constraint is violated\n",
    "        return np.sum(violation**2)/relax_norm_constraint**2 \n",
    "    def prox_gs(σ,τ): return prox(rsh(σ),τ).reshape(σ.shape)\n",
    "    return gs,g,prox_gs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f6612a",
   "metadata": {},
   "source": [
    "### 1.3 Gradient\n",
    "\n",
    "We use upwind finite differences, on a staggered grid.\n",
    "In dimension $1$, the chosen scheme is second order consistent. In dimension two, it is only first order consistent.\n",
    "Note that the `grad2` scheme corresponds to the `gradc` scheme in the [notebook on the mincut problem](Prox_MinCut.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "beee163b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad1(ϕ,dx): return (drop(ϕ,1,axis=-1)-drop(ϕ,-1,axis=-1))[None]/dx\n",
    "def grad2(ϕ,dx): return ad.array([drop(drop(ϕ, 1,axis=-2+i)-drop(ϕ,-1,-2+i),-1,-1-i)/dxi\n",
    "                                  for i,dxi in enumerate(dx)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f352d7",
   "metadata": {},
   "source": [
    "The Chambolle-Pock primal-dual optimization algorithm requires the proximal steps sizes to obey a CFL condition, involving the squared norm of the gradient operator. The latter is classically estimated as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5359b9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm2_grad(dx): \n",
    "    \"\"\"\n",
    "    Squared norm of the basic discretized gradient operator \n",
    "    (assuming one grid scale per dimension).\n",
    "    \"\"\"\n",
    "    return 4*sum(dxi**-2 for dxi in dx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4df0c1",
   "metadata": {},
   "source": [
    "The function `grad_ndiv_operators` then conveniently computes the negative divergence by assembling the (sparse) matrix of the gradient, via automatic differentiation, and transposing it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a38356",
   "metadata": {},
   "source": [
    "### 1.4 Implicit formulation\n",
    "\n",
    "The second primal-dual formulation of the Beckman optimal transport problem does not involve a coupling operator ($K=\\mathrm{Id}$), but requires to solve a some linear systems in each iteration. It is based on the following functionals:\n",
    "\n",
    "$$\n",
    "    f(\\sigma):= \\int_\\Omega |\\sigma|,\\quad g(\\sigma) := \\int \\frac \\lambda 2  |\\diver \\sigma +\\xi |^2.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3d1b1dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The coupling operator of the second implementation is the identity\n",
    "def make2_K():\n",
    "    \"\"\"Make the identity operator\"\"\"\n",
    "    def K(x): return x\n",
    "    K.T=K # Identity is its own transpose\n",
    "    return K\n",
    "\n",
    "# The operator f of the second implementation is g^* from the first.\n",
    "def make2_f(*args,**kwargs): return make_gs(*args,**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceeee184",
   "metadata": {},
   "source": [
    "Regarding $g$, we recognize a quadratic expression, as considered in the introduction of this section. We first the proximal operator and dual function in the generic case using automatic differentiation to compute the coefficients.\n",
    "\n",
    "**Note on performance.** For the application of interest, a more efficient way to solve the linear systems considered here would be to use a fast Fourier transform. However, we do not follow this approach here for simplicity. (Multigrid methods, or suitable sparse Cholesky factorizations, would also apply.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0fe147c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_quad(f,shape,τ,relax_dual=1e-3):\n",
    "    \"\"\"\n",
    "    Sparse matrix based implementations of a convex quadratic function f,\n",
    "    the Legendre-Fenchel conjugate, and the proximal operator.\n",
    "    Inputs\n",
    "    - f : an implementation of a (possibly inhomogeneous) quadratic f\n",
    "    - shape : input shape for f\n",
    "    - τ : time step for the proximal operator (frozen)\n",
    "    - relax_dual : a small relaxation parameter for the computation of the dual\n",
    "    \"\"\"\n",
    "    z_ad = ad.Sparse2.identity(shape)\n",
    "    f_ad = f(z_ad) # Second order Taylor expansion of f at zero\n",
    "    #ad.simplify_ad(f_ad) # counterproductive here\n",
    "    A = f_ad.hessian_operator().T # Transpose for csc format\n",
    "    b = f_ad.to_first().to_dense().coef # First order term\n",
    "    c = f_ad.value # Constant term\n",
    "    eye = scipy.sparse.eye(z_ad.size)\n",
    "    iAτ = scipy.sparse.linalg.factorized(τ*A+eye) \n",
    "    iAϵ = scipy.sparse.linalg.factorized(A+relax_dual*eye)\n",
    "    def f_(z): # Reimplementation using sparse matrices for efficiency\n",
    "        z=z.reshape(-1) \n",
    "        return np.sum((0.5*(A*z)+b)*z)+c\n",
    "    def fs(z): # Relaxed legendre-Fenchel dual\n",
    "        t=z.reshape(-1)-b\n",
    "        return np.sum(0.5*iAϵ(t)*t)-c\n",
    "    def prox_f(z,τ2): # Proximal operator of f\n",
    "        assert τ==τ2\n",
    "        return iAτ(z.reshape(-1)-τ*b).reshape(z.shape)\n",
    "    return f_,fs,prox_f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd514ee",
   "metadata": {},
   "source": [
    "Now we can implement $g(\\sigma) := \\int \\frac \\lambda 2  |\\diver \\sigma +\\xi |^2$, and deduce the related operators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e38135b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def div1(σ,dx):\n",
    "    \"\"\"Divergence of σ (no flux on boundary σ.n = 0, dual to upwind scheme).\"\"\"\n",
    "    return np.concatenate((σ[...,:1],σ[...,1:]-σ[...,:-1],-σ[...,-1:]),axis=-1)[0]/dx\n",
    "\n",
    "def appzero(arr,axis):\n",
    "    \"\"\"Appends a zero along an axis\"\"\"\n",
    "    if axis<0: axis += arr.ndim\n",
    "    return np.pad(arr, pad_width = ((0,0),)*axis+((0,1),)+((0,0),)*(arr.ndim-axis-1))\n",
    "    \n",
    "def div2(σ,dx):\n",
    "    \"\"\"Divergence of σ (no flux on boundary σ.n = 0, dual to upwind scheme).\"\"\"\n",
    "    σ0,σ1 = σ\n",
    "    return (appzero(np.concatenate([σ0[...,:1,:],σ0[...,1:,:]-σ0[...,:-1,:],-σ0[...,-1:,:]],axis=-2)/dx[0],-1) +\n",
    "            appzero(np.concatenate([σ1[...,:,:1],σ1[...,:,1:]-σ1[...,:,:-1],-σ1[...,:,-1:]],axis=-1)/dx[1],-2) )\n",
    "\n",
    "def g2(σ,λ,ξ,dx): \n",
    "    \"\"\"Function implementing the divergence constraint\"\"\"\n",
    "    div = {1:div1,2:div2}[len(dx)]\n",
    "    return np.sum( (λ/2)*(div(σ,dx)+ξ)**2)\n",
    "\n",
    "\n",
    "def shape_σ(ξ,dx):\n",
    "    \"\"\"Returns the shape of the vector field σ, in terms of the shape of the measure ξ and dimension\"\"\"\n",
    "    ndim = len(dx)\n",
    "    return (ndim,)+tuple(s-(i+ndim>=ξ.ndim) for i,s in enumerate(ξ.shape))\n",
    "\n",
    "def make2_gs(λ,ξ,dx,τ,**kwargs):\n",
    "    g,gs,prox_g = make_quad(lambda σ:g2(σ,λ,ξ,dx),shape_σ(ξ,dx),1./τ,**kwargs)\n",
    "    def prox_gs(σ,τ2):\n",
    "        assert τ==τ2\n",
    "        return σ - τ*prox_g(σ/τ,1./τ) # Moreau formula for the dual prox\n",
    "    return gs,g,prox_gs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb2f1e9",
   "metadata": {},
   "source": [
    "Before running the experiments, let us validate some of these implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "55e2361d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "#shape = (7,); corners = [[0],[1]]\n",
    "shape = (4,3); corners = [[0,0],[1,1]]\n",
    "λ=2\n",
    "Xs,Xv,dx = stag_grids(shape,corners)\n",
    "ξ = np.random.rand(*shape)\n",
    "σ0 = np.random.rand(*shape_σ(ξ,dx))\n",
    "ϕ0 = np.random.rand(*shape)\n",
    "\n",
    "# Check that σx-σy = τ (grad g)(σy) if σy = prox_g(σx,τ)\n",
    "τ=3\n",
    "g_scheme=lambda σ:g2(σ,λ,ξ,dx)\n",
    "g,gs,prox_g = make_quad(g_scheme,shape_σ(ξ,dx),τ)\n",
    "σx = np.random.rand(*σ0.shape)\n",
    "σy = prox_g(σx,τ)\n",
    "g_ad = g_scheme(ad.Dense.identity(constant=σy))\n",
    "assert np.allclose(σx-σy, τ*g_ad.coef.reshape(σx.shape))\n",
    "\n",
    "# Check adjointness of gradient and divergence\n",
    "grad_ = {1:grad1,2:grad2}[len(dx)]\n",
    "div_ = {1:div1,2:div2}[len(dx)]\n",
    "assert np.allclose((grad_(ϕ0,dx)*σ0).sum(),-(ϕ0*div_(σ0,dx)).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a19026e",
   "metadata": {},
   "source": [
    "## 2. One dimensional transport\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15cc6af7",
   "metadata": {},
   "source": [
    "### 2.1 Running the primal-dual optimizers\n",
    "\n",
    "Let us generate a first instance.\n",
    "\n",
    "<!---\n",
    "μ=np.zeros_like(Xs)\n",
    "μ[n//6]=1/dx # Dirac mass at 1/6\n",
    "\n",
    "ν = np.zeros_like(Xs)\n",
    "ν[(5*n)//6]=0.8/dx # Weighted Dirac mass at 5/6\n",
    "--->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "251a017c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n=101\n",
    "Xs,Xv,dx = stag_grids(n,[0,1]) # Grid for scalars, staggered grid for vectors, grid scale\n",
    "Xs,Xv = Xs[0],Xv[0] # Only one coordinate in dimension one\n",
    "\n",
    "# Two Gaussian bumps, with different masses\n",
    "std_dev = 0.1; μ = np.exp(-(Xs-0.3)**2/(2*std_dev**2)); ν = 0.8*np.exp(-(Xs-0.6)**2/(2*std_dev**2))\n",
    "# Alternatively, two Dirac measures with different masses, at 1/6 and 5/6\n",
    "#μ=np.zeros_like(Xs); ν = np.zeros_like(Xs) μ[n//6]=1/dx; ν[(5*n)//6]=0.8/dx \n",
    "\n",
    "ξ = ν-μ # Difference of measures\n",
    "λ = 1. # Relaxation parameter for the constraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "81dfbc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(Xs,μ,label=\"μ\")\n",
    "plt.plot(Xs,ν,label=\"ν\")\n",
    "plt.plot(Xs,ξ,label=\"ξ=ν-μ\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8d0b1e",
   "metadata": {},
   "source": [
    "Now generate the various operators, using convenience functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5c526072",
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve1_ot(λ,ξ,dx,τ_f=None,relax_norm_constraint=0.01,\n",
    "             accelerated=False,maxiter=5000,K=None,ρ_overrelax=1.8,ϕ0=None,σ0=None,**kwargs):\n",
    "    \"\"\"\n",
    "    Solve the Beckman optimal transport problem via explicit primal-dual optimization.\n",
    "    \"\"\"\n",
    "    vdim = len(dx) # One grid scale per dimension. \n",
    "    if ϕ0 is None: ϕ0 = np.zeros(ξ.shape)\n",
    "    \n",
    "    if τ_f is None: τ_f = 5/np.sqrt(norm2_grad(dx))\n",
    "    τ_gs = 1./(norm2_grad(dx)*τ_f)\n",
    "\n",
    "    impl_f = make_f(λ,ξ)\n",
    "    cvx_f = 1/λ if accelerated else 0 # Coercivity constant of f, used in accelerated variant\n",
    "    impl_gs = make_gs(multichannel_depth=ξ.ndim-vdim,relax_norm_constraint=relax_norm_constraint)\n",
    "    if K is None: K,_ = grad_ndiv_operators({1:grad1,2:grad2}[vdim],dx,ξ.shape)\n",
    "    \n",
    "    res = chambolle_pock(impl_f,impl_gs,τ_f,τ_gs,K,ϕ0,y=σ0,\n",
    "                         cvx_f=cvx_f,maxiter=maxiter,ρ_overrelax=ρ_overrelax,**kwargs)\n",
    "    return res,{'vdim':vdim,'τ_f':τ_f,'τ_gs':τ_gs,'cvx_f':cvx_f}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ce4d3d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve2_ot(λ,ξ,dx,τ_f=None,relax_norm_constraint=0.01,relax_dual=1e-3,\n",
    "             maxiter=5000,K=None,ρ_overrelax=1.8,**kwargs):\n",
    "    \"\"\"\n",
    "    Solve the Beckman optimal transport problem via implicit primal-dual optimization.\n",
    "    \"\"\"\n",
    "    vdim = len(dx) # One grid scale per dimension. \n",
    "    σ0 = np.zeros(shape_σ(ξ,dx))\n",
    "\n",
    "    if τ_f is None: τ_f=0.01*λ\n",
    "    τ_gs = 1./τ_f # Note that |K|^2 = 1 since K=Id\n",
    "\n",
    "    K = make2_K()\n",
    "    impl_f = make2_f(multichannel_depth=ξ.ndim-vdim,relax_norm_constraint=relax_norm_constraint)\n",
    "    impl_gs = make2_gs(λ,ξ,dx,τ_gs,relax_dual=relax_dual)\n",
    "\n",
    "    res = chambolle_pock(impl_f,impl_gs,τ_f,τ_gs,K,σ0,\n",
    "                         maxiter=maxiter,ρ_overrelax=ρ_overrelax,**kwargs)   \n",
    "    return res, {'vdim':vdim,'τ_f':τ_f,'τ_gs':τ_gs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "09762c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve3_ot(λ,ξ,dx,**kwargs): \n",
    "    \"\"\"\n",
    "    Solve the Beckman optimal transport problem via explicit primal-dual optimization on the GPU\n",
    "    \"\"\"\n",
    "    res = BeckmanOT.solve_ot(λ,ξ,dx,**kwargs)\n",
    "    return {key:ad.cupy_generic.cupy_get(value) for key,value in res.items()},None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df75496",
   "metadata": {},
   "source": [
    "The two CPU numerical solvers above are distinct approaches to the *same* optimization problem. Thus they produce identical results, up to a change of sign in the objective function, and up to the numerical accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "79c3c839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primal-dual solver completed 381 steps in 0.02280712127685547 seconds\n",
      "Primal-dual solver completed 71 steps in 0.0025250911712646484 seconds\n",
      "Number of iterations : 381 (explicit), 71 (implicit)\n",
      "Explicit scheme : 5.143935876000981 <= W1 distance <= 5.145449398747404\n",
      "Implicit scheme : 5.145514983134441 <= W1 distance <= 5.145449648021886\n",
      "Largest difference in solution σ between the two approaches : 6.589223307393289e-06\n"
     ]
    }
   ],
   "source": [
    "res1,_ = solve1_ot(λ,ξ,dx)\n",
    "res2,_ = solve2_ot(λ,ξ,dx)\n",
    "\n",
    "print(f\"Number of iterations : {res1['niter']} (explicit), {res2['niter']} (implicit)\")\n",
    "print(f\"Explicit scheme : {-res1['primal_values'][-1]} <= W1 distance <= {-res1['dual_values'][-1]}\")\n",
    "print(f\"Implicit scheme : {res2['dual_values'][-1]} <= W1 distance <= {res2['primal_values'][-1]}\")\n",
    "\n",
    "val1 = res1['primal_values'][-1]; val2 = res2['primal_values'][-1]\n",
    "assert np.abs(val1+val2)<=2e-3*abs(val1)\n",
    "\n",
    "# σ is the dual variable for the first approach, and the primal variable for the second approach\n",
    "err_σ = np.max(np.abs(res1['y']-res2['x']))\n",
    "print(f\"Largest difference in solution σ between the two approaches : {err_σ}\")\n",
    "assert err_σ < 1e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a904f49b",
   "metadata": {},
   "source": [
    "We'll discuss in some explicit solutions in the next section. In the meanwhile, here is a plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9e5ad0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Optimal primal and dual solutions\")\n",
    "plt.plot(Xs,res1['x'],label=\"optimal ϕ\")\n",
    "plt.plot(Xv,res1['y'][0],label=\"optimal σ\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e25cd1e",
   "metadata": {},
   "source": [
    "The energy of the solutions is very non-monotonic along the iterations of the optimization algorithm. It initially grows to large values, before decreasing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "83ca07fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[10,4])\n",
    "for i,(name,res) in enumerate(((\"explicit\",res1),(\"implicit\",res2))):\n",
    "    plt.subplot(1,2,1+i)\n",
    "    plt.title(f\"Energy of iterates, {name} scheme\")\n",
    "    plt.plot(res['primal_values'],label=\"primal\")\n",
    "    plt.plot(res['dual_values'],label=\"dual\")\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607ea93d",
   "metadata": {},
   "source": [
    "The eikonal GPU solver is based on the explicit formulation, and produces numerical results identical to the CPU implementation up to machine precision, in the first iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5dbe91e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_cpu_gpu((λ,ξ,dx))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77d4305",
   "metadata": {},
   "source": [
    " Note that the GPU uses float32 scalars, which incur more numerical error that the float64 scalars used by the CPU, hence the final result may be somewhat less accurate.\n",
    " \n",
    " <!---\n",
    "res3,_ = solve3_ot(λ,ξ,dx,maxiter=1,ϕ0=res1['x'],σ0=res1['y'])\n",
    "print(f\"Explicit scheme (gpu) : {-res3['primal_values'][-1]} <= W1 distance <= {-res3['dual_values'][-1]}\")\n",
    "\n",
    "compare_cpu_gpu((λ,ξ,dx),ϕ0=res1['x'],σ0=res1['y'])\n",
    "--->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5a496ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if xp is not np: \n",
    "    res3,_ = solve3_ot(λ,ξ,dx)\n",
    "    print(f\"Explicit scheme (gpu) : {-res3['primal_values'][-1]} <= W1 distance <= {-res3['dual_values'][-1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928759ba",
   "metadata": {},
   "source": [
    "**Scaling invariance**\n",
    "\n",
    "The considered problems are invariant upon scaling $\\lambda$ and the domain dimensions (equivalently, the grid scale $dx$) by the same factor. \n",
    "We illustrate this invariance numerically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "497dec41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning : duality gap not reduced to target within iteration budget\n",
      "Primal-dual solver completed 100 steps in 0.012045145034790039 seconds\n"
     ]
    }
   ],
   "source": [
    "res1,_ = solve1_ot(λ,ξ,dx,maxiter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "375562dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning : duality gap not reduced to target within iteration budget\n",
      "Primal-dual solver completed 100 steps in 0.003999233245849609 seconds\n"
     ]
    }
   ],
   "source": [
    "c0 = 10 # Simultaneous scaling of penalization, scale\n",
    "res1_rescaled,_ = solve1_ot(c0*λ,ξ,c0*dx,relax_norm_constraint=0.01/np.sqrt(c0),maxiter=100)\n",
    "assert np.allclose(res1_rescaled['x'],c0*res1['x']) # Same primal solution\n",
    "assert np.allclose(res1_rescaled['y'],c0*res1['y']) # Same dual solution\n",
    "assert res1_rescaled['niter'] == res1['niter']\n",
    "assert np.allclose(res1_rescaled['primal_values'],c0*res1['primal_values']) # Same primal values\n",
    "assert np.allclose(res1_rescaled['dual_values'],c0*res1['dual_values']) # Same dual values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e4415a40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning : duality gap not reduced to target within iteration budget\n",
      "Primal-dual solver completed 40 steps in 0.0014579296112060547 seconds\n",
      "Warning : duality gap not reduced to target within iteration budget\n",
      "Primal-dual solver completed 40 steps in 0.0012538433074951172 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([ 1.72449856e+00, -5.30140858e+06, -2.88557201e+05, -7.96894949e+03]),\n",
       " array([ 1.23529458e+00, -5.30140882e+06, -2.88557387e+05, -7.96912908e+03]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxiter=40 \n",
    "res2,_ = solve2_ot(λ,ξ,dx,maxiter=maxiter)\n",
    "res2_rescaled,_ = solve2_ot(c0*λ,ξ,c0*dx,maxiter=maxiter,relax_norm_constraint=0.01/np.sqrt(c0),relax_dual=0.001*c0)\n",
    "assert np.allclose(c0*res2['x'],res2_rescaled['x']) # Same solution\n",
    "assert np.allclose(res2_rescaled['primal_values'],c0*res2['primal_values']) # Same primal values\n",
    "res2_rescaled['dual_values'],c0*res2['dual_values'] # Dual values differ a little bit. TODO : find why "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff16f0b",
   "metadata": {},
   "source": [
    "**Large problem dimensions.**\n",
    "\n",
    "When the problem dimension increases, as discussed in the introduction:\n",
    "- the explicit scheme is subject to the CFL condition $\\tau_f\\tau_{g^*} \\|K\\|^2 \\leq 1$, so that the proximal time step is reduced, and the number of iterations increases. \n",
    "- the implicit scheme uses an identical time step, and terminates in a similar number of iterations.\n",
    "\n",
    "At some point, the second method becomes more efficient, especially since if the linear inversions are done efficiently (fft, etc). Note that the 'acceleration' of the first method (which uses adaptive primal time steps depending on the coercivity constant of the objective function and the iteration number) does not seem to help in practice. \n",
    "\n",
    "<!---\n",
    "It is common for primal dual methods to take hundreds or thousands of iterations. \n",
    "\n",
    "The ergodic convergence rate of the primal dual gap is $O(1/N)$ (ergodic means that it is established for the average $(x^0+x^1+\\cdots+x^{N-1})/N$ but averaging does not seem necessary in practice). Since $f$ is strictly convex, one can also use an accelerated variant which admits an $O(1/N^2)$ convergence rate. However, in practice, the advantage is not always obvious (it may even be counterproductive, as illustrated below). \n",
    "\n",
    "For convenience, let us make a function which runs the algorithm based on the optimal transport data.\n",
    "\n",
    "**TODO : reformulate based on observations (initial bug ??)**\n",
    "\n",
    "The convergence is a bit slow, with more than 2000 iterations to reach the target duality gap reduction, by a factor $10^4$ w.r.t. the initial guess (which is identically zero).\n",
    "There are two reasons for this:\n",
    "- The test case is a bit stiff, involving two faraway Dirac masses.\n",
    "- We do not take advantage of the strict convexity of the dual objective functional.\n",
    "\n",
    "Another advantage of the accelerated variant is that it is less ensitive to the choice of time step $\\tau_f$. However, it must be acknowledged that the decrease in energy is very non-monotonic.\n",
    "\n",
    "\n",
    "plt.loglog(range(0,res['niter'],10),res['gaps'],label=\"standard\")\n",
    "plt.loglog(range(0,res_acc['niter'],10),res_acc['gaps'],label=\"accelerated\")\n",
    "rng = np.linspace(1,res_acc['niter'])\n",
    "plt.loglog(rng,1000000/(1+rng)**2,label='second order')\n",
    "plt.legend();\n",
    "--->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "db216da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "n=1001 # Same setting, larger dimensions\n",
    "Xs,Xv,dx = stag_grids(n,[0,1]) # Grid for scalars, staggered grid for vectors, grid scale\n",
    "Xs,Xv = Xs[0],Xv[0] # Only one coordinate in dimension one\n",
    "\n",
    "std_dev = 0.1\n",
    "μ = np.exp(-(Xs-0.3)**2/(2*std_dev**2))\n",
    "ν = np.exp(-(Xs-0.6)**2/(2*std_dev**2))\n",
    "ξ = ν-μ # Difference of measures\n",
    "\n",
    "λ = 1. # Relaxation parameter for the constraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6fc75706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primal-dual solver completed 3541 steps in 0.11805200576782227 seconds\n",
      "CPU times: user 118 ms, sys: 1.72 ms, total: 120 ms\n",
      "Wall time: 119 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "res1,_ = solve1_ot(λ,ξ,dx,maxiter=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c88031c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primal-dual solver completed 101 steps in 0.0045490264892578125 seconds\n",
      "CPU times: user 6.77 ms, sys: 1.28 ms, total: 8.04 ms\n",
      "Wall time: 7.09 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "res2,_ = solve2_ot(λ,ξ,dx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "45372492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iterations : 3541 (explicit), 101 (implicit)\n",
      "Explicit scheme : 56.61484666584439 <= W1 distance <= 56.65330041888937\n",
      "Implicit scheme : 56.608355059740546 <= W1 distance <= 56.65378364937431\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of iterations : {res1['niter']} (explicit), {res2['niter']} (implicit)\")\n",
    "print(f\"Explicit scheme : {-res1['primal_values'][-1]} <= W1 distance <= {-res1['dual_values'][-1]}\")\n",
    "print(f\"Implicit scheme : {res2['dual_values'][-1]} <= W1 distance <= {res2['primal_values'][-1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d016c248",
   "metadata": {},
   "source": [
    "The CPU and GPU implementations produce identical results in the first steps, up to machine precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1e81bbf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_cpu_gpu((λ,ξ,dx))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82146e79",
   "metadata": {},
   "source": [
    "However, due to the limited float32 precision, the GPU eikonal solver accumulates substantial errors when the grid scale is small. For this reason, the primal dual gap is not sufficiently reduced, and instead the optimization stops when the numerical solution stabilizes.\n",
    "\n",
    "*Note on performance.* The GPU implementation is not (much) faster than the CPU implementation in this instance. This is expected for one dimensional problems, and a different situation holds in two dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6df0a3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "if xp is not np:\n",
    "    res3,_ = solve3_ot(λ,ξ,dx)\n",
    "    print(f\"Explicit scheme (gpu) : {-res3['primal_values'][-1]} <= W1 distance <= {-res3['dual_values'][-1]}\")\n",
    "    print(f\"Active stopping criterion : {res3['stopping_criterion']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdffadc0",
   "metadata": {},
   "source": [
    "### 2.2 Comparing with exact solutions\n",
    "\n",
    "We compute the relaxed optimal transport distance between Dirac masses, for which an explicit solution exits. Note that this solution is not completely trivial, due to the nonstandard relaxation, with $p=2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4993680f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def opt_ϕ(a,b,x,λ):\n",
    "    \"\"\"\n",
    "    Assumes that μ = a δ_0 and ν = b δ_1 are Dirac measures, that ϵ=0,\n",
    "    and returns the optimal ϕ. (Closed form solution.)\n",
    "    \"\"\"\n",
    "    def hat(r,x0): return np.maximum(0,r-np.abs(x-x0))\n",
    "    \n",
    "    α=np.sqrt(λ*a); β=np.sqrt(λ*b)\n",
    "    if α+β<=1: return hat(β,1) - hat(α,0) \n",
    "\n",
    "    t=0.5*λ*(a-b)\n",
    "    α=0.5+t; β=0.5-t\n",
    "    if α>=0 and β>=0: return hat(β,1) - hat(α,0)\n",
    "    \n",
    "    r = np.sqrt(λ*np.abs(a-b))\n",
    "    return -hat(r,0) if a>b else hat(r,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "53af07da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def comp_ϕ(a,b,x,λ,**kwargs):\n",
    "    \"\"\"Returns the numerical and the exact potential ϕ\"\"\"\n",
    "    dx = x[1]-x[0]\n",
    "    μ = (a/dx)*(np.abs(x)<dx/2)\n",
    "    ν = (b/dx)*(np.abs(x-1)<dx/2)\n",
    "    assert np.sum(μ>0)==1 and np.sum(ν>0)==1\n",
    "    \n",
    "    res,_ = solve1_ot(λ,ν-μ,dx[None])\n",
    "    plt.plot(x,res['x'],label=\"numerical\")\n",
    "    plt.plot(x,opt_ϕ(a,b,x,λ),label=\"exact\")\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7e80382e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xs,Xv,dx = stag_grids(100,[-2,2])\n",
    "Xs = Xs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "19d1bbbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primal-dual solver completed 481 steps in 0.011433124542236328 seconds\n"
     ]
    }
   ],
   "source": [
    "plt.title(\"Exact and numerical solutions (Moderate relaxation).\")\n",
    "comp_φ(1,0.8,Xs,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "dcdcbb8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primal-dual solver completed 111 steps in 0.003306150436401367 seconds\n"
     ]
    }
   ],
   "source": [
    "plt.title(\"Exact and numerical solutions (Small relaxation).\")\n",
    "comp_φ(1,0.8,Xs,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "eef59c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primal-dual solver completed 531 steps in 0.013766050338745117 seconds\n"
     ]
    }
   ],
   "source": [
    "plt.title(\"Exact and numerical solutions (Large relaxation).\")\n",
    "comp_φ(1,0.8,Xs,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5c9ae4",
   "metadata": {},
   "source": [
    "### 2.3 Vector data\n",
    "\n",
    "The Beckman formulation of optimal transport, with a relaxed transport constraint, easily extends to vector data.\n",
    "\n",
    "*On the lift mapping.*\n",
    "One difficulty with oscillating data is that it tends to be transported on itself. In order to avoid this phenomenon, we add the norm $x_n = \\sqrt{x_0^2+\\cdots+x_{n-1}^2}$ of the signal as a supplementary channel. This can be regarded as a lift into the Lorentz cone\n",
    "$$\n",
    "    \\{(x_0,\\cdots,x_n)\\in \\bR^{n+1}|x_n\\geq 0, x_n^2\\geq x_0^2+\\cdots+x_{n-1}^2\\}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ed163505",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lift(μ0,μ1): return np.stack([μ0,μ1,np.sqrt(μ0**2+μ1**2)],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4204d3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the grid\n",
    "Xs,Xv,dx = stag_grids(300,[-20,20])\n",
    "Xs=Xs[0]; Xv=Xv[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "65bacd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the measures\n",
    "def gaussian(x,ρ): return np.exp(-x**2/(2*ρ**2)) / np.sqrt(2*np.pi*ρ)\n",
    "def damped_oscillating(x,ρ): \n",
    "    g = gaussian(x,ρ)\n",
    "    return lift(g*np.cos(x),g*np.sin(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "768075a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ρ = 2\n",
    "x0 = 10\n",
    "μ = damped_oscillating(Xs,ρ)\n",
    "ν = damped_oscillating(Xs-x0,ρ)\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(1,3,1)\n",
    "plt.title(\"First signal component\")\n",
    "plt.plot(Xs,μ[0],Xs,ν[0])\n",
    "plt.subplot(1,3,2)\n",
    "plt.title(\"Second signal component\")\n",
    "plt.plot(Xs,μ[1],Xs,ν[1])\n",
    "plt.subplot(1,3,3)\n",
    "plt.title(\"Additional signal component\")\n",
    "plt.plot(Xs,μ[2],Xs,ν[2]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0c708c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def damped_oscillating_test(x0,ρ,x,channel=None,λ=None,mass_ratio=1.):\n",
    "    μ = damped_oscillating(x,ρ)\n",
    "    ν = mass_ratio*damped_oscillating(x-x0,ρ)\n",
    "    ξ = μ-ν\n",
    "    if channel is not None: ξ = ξ[channel]\n",
    "    if λ is None: λ = 2*ρ/np.max(μ) # Somehow a length over an amplitude\n",
    "    return λ,ξ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f961aab",
   "metadata": {},
   "source": [
    "Let us compare the cpu and the gpu implementations on vector valued data.\n",
    "\n",
    "<!---\n",
    "#W1dist,(res,data,inputs) = damped_oscillating_test(x0,ρ,Xs,dx,channel=None)\n",
    "#K = res['ops']['K']\n",
    "#%%time\n",
    "#for i in range(100):W1dist,(res,data,inputs) = damped_oscillating_test(x0,ρ,Xs,dx,channel=None,grad=K)\n",
    "--->\n",
    "\n",
    "<!---\n",
    "%%time\n",
    "x0=5; ρ = np.sqrt(3);\n",
    "\n",
    "\n",
    "# Comparing with former implementation\n",
    "def norm2(x,axis=None): \n",
    "    \"\"\"Squared Euclidean norm, along given axis (optional)\"\"\"\n",
    "    return np.sum(x**2,axis=axis)\n",
    "\n",
    "def div(σ,dx):\n",
    "    div_σ = np.concatenate((σ,np.zeros_like(σ[:1])),axis=0)\n",
    "    div_σ[1:] -=σ\n",
    "    div_σ /= dx\n",
    "    return div_σ\n",
    "\n",
    "def W1relax(σ,γ,dx,λ,ϵ=1e-3):\n",
    "    \"\"\"Discretizes \\int |σ| + (λ/2) |div σ + γ|^2\"\"\"\n",
    "    return np.sqrt(ϵ**2+σ**2).sum() + (λ/2) * norm2(div(σ,dx)+γ)\n",
    "\n",
    "\n",
    "def div_v(σ,dx):\n",
    "    div_σ = np.concatenate((σ,np.zeros_like(σ[:,:1])),axis=1)\n",
    "    div_σ[:,1:] -=σ\n",
    "    div_σ /= dx\n",
    "    return div_σ\n",
    "\n",
    "def W1relax_v(σ,γ,dx,λ,ϵ=1e-3):\n",
    "    \"\"\"Discretizes \\int |σ| + (λ/2) |div σ + γ|^2\"\"\"\n",
    "    vdim,n = γ.shape; σ = σ.reshape((vdim,n-1))\n",
    "    return np.sqrt(ϵ**2+norm2(σ,axis=0)).sum() + (λ/2) * norm2(div_v(σ,dx)+γ)\n",
    "\n",
    "\n",
    "W1dist,(res,data,inputs) = damped_oscillating_test(x0,ρ,Xs,dx,channel=0)\n",
    "W1old = W1relax(res['y'][0],inputs['ξ'],dx,inputs['λ'],0)\n",
    "print(f\"Scalar case : {W1dist=}, {W1old=}\")\n",
    "\n",
    "W1dist,(res,data,inputs) = damped_oscillating_test(x0,ρ,Xs,dx,channel=None)\n",
    "W1old = W1relax_v(res['y'],inputs['ξ'],dx,inputs['λ'],0)\n",
    "print(f\"Vector case : {W1dist=}, {W1old=}\")\n",
    "--->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "dfecc32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x0=5; ρ = np.sqrt(3);\n",
    "λ,ξ = damped_oscillating_test(x0,ρ,Xs)\n",
    "compare_cpu_gpu((λ,ξ,dx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cd16a0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x0s = np.sort(np.concatenate([np.linspace(-15,15,11),np.linspace(-4,4,10)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7ec4cade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 399 ms, sys: 2.96 ms, total: 402 ms\n",
      "Wall time: 401 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ρ=np.sqrt(3); mass_ratio=0.8\n",
    "results = [[-solve1_ot(*damped_oscillating_test(x0,ρ,Xs,channel,mass_ratio=mass_ratio),dx,verbosity=0)[0]['dual_values'][-1]\n",
    "            for x0 in x0s] for channel in [0,1,2,None]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da96337f",
   "metadata": {},
   "source": [
    "We empirically observe, for these parameters, that:\n",
    "- Using an oscillating channel produces a sharp minimum, when the signals are in phase, but also some spurious local optima.\n",
    "- Using the additional norm channel avoids the local minima problem, but the minimum is not all well localized.\n",
    "- Using all channels combines the strengths of the other two approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "90220e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x0s,np.transpose(results));\n",
    "plt.title(\"Relaxed W1 distance depending on shift, using a single channel or all of them\")\n",
    "plt.legend([0,1,2,\"All\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac621d36",
   "metadata": {},
   "source": [
    "## 3. Two dimensions\n",
    "\n",
    "We illustrate some two dimensional test cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7e64a0",
   "metadata": {},
   "source": [
    "### 3.1 From a Dirac mass to another\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "827c614a",
   "metadata": {},
   "outputs": [],
   "source": [
    "n=100\n",
    "Xs,Xv,dx = stag_grids((n,n+1),[[-1,-1],[1,1]])\n",
    "μ = np.zeros(Xs[0].shape); ν = μ.copy() \n",
    "μ[n//6,n//3]=1/np.prod(dx); ν[3*n//4,5*n//6]=0.8/np.prod(dx)\n",
    "\n",
    "ξ = ν-μ; λ = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b21063b",
   "metadata": {},
   "source": [
    "In this quite singular problem instance, both the implicit and the explicit formulations need a very large number of iterations to reach convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "cce39539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning : duality gap not reduced to target within iteration budget\n",
      "Primal-dual solver completed 30000 steps in 5.057033061981201 seconds\n",
      "CPU times: user 4.94 s, sys: 104 ms, total: 5.05 s\n",
      "Wall time: 5.06 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "res1,_ = solve1_ot(λ,ξ,dx,maxiter=30000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e8a1cf67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning : duality gap not reduced to target within iteration budget\n",
      "Primal-dual solver completed 5000 steps in 11.522531032562256 seconds\n",
      "CPU times: user 11.6 s, sys: 296 ms, total: 11.9 s\n",
      "Wall time: 11.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "res2,_ = solve2_ot(λ,ξ,dx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4564d60a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Implicit : 2973.9283624321574 <= W1 dist <= 3067.16517585262\n",
      "Explicit : 2901.201052824469 <= W1 dist <= 3067.4317100245635\n"
     ]
    }
   ],
   "source": [
    "print(f\"Implicit : {res2['dual_values'][-1]} <= W1 dist <= {res2['primal_values'][-1]}\")\n",
    "print(f\"Explicit : {-res1['primal_values'][-1]} <= W1 dist <= {-res1['dual_values'][-1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb134d76",
   "metadata": {},
   "source": [
    "The optimal flow is mostly, but not exactly (even in theory), concentrated on a line between the two Dirac masses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d5924746",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical results from the two methods are similar\n",
    "# σ = res1['y'] \n",
    "σ = res2['x']\n",
    "\n",
    "plt.title(\"Optimal flow between two Diracs\")\n",
    "Plotting.quiver(*Xv,*σ,subsampling=(3,3))\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e27d71",
   "metadata": {},
   "source": [
    "The optimal potential is reminiscent of the optimal one dimensional solution presented above, and seems to involve \"distorted\" hat functions, but admits no closed form expression to our knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "14a9a6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical results from the two methods are similar\n",
    "#ϕ = res1['x']\n",
    "ϕ = λ*(div2(res2['x'],dx)+ξ)\n",
    "\n",
    "plt.title(\"Optimal potential between two Diracs\")\n",
    "plt.contour(*Xs,ϕ)\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b763af06",
   "metadata": {},
   "source": [
    "### 3.2 Oscillating signals and lift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6be79df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the measures\n",
    "def gaussian_2(x,ρ): \n",
    "    return np.exp(-np.sum(x**2,axis=0)/(2*ρ**2)) / np.sqrt(2*np.pi*ρ)**2\n",
    "\n",
    "def damped_oscillating_2(x,v,ρ): \n",
    "    g = gaussian_2(x,ρ)\n",
    "    ϕ = x[0]*v[0]+x[1]*v[1]\n",
    "    return lift(g*np.cos(ϕ),g*np.sin(ϕ))\n",
    "\n",
    "def damped_oscillating_test_2(x,v,ρ,x0,channel=None,mass_ratio=1):\n",
    "    μ = damped_oscillating_2(x,v,ρ)\n",
    "    ν = mass_ratio*damped_oscillating_2(x-fd.as_field(x0,x.shape[1:]),v,ρ)\n",
    "    ξ = μ-ν    \n",
    "    λ = 2*ρ/np.max(μ) # Somehow a length over an amplitude\n",
    "    if channel is not None: ξ = ξ[channel]\n",
    "    if λ is None: λ = 2*ρ/np.max(μ) # Somehow a length over an amplitude\n",
    "    return λ,ξ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c84ca8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xs,Xv,dx = stag_grids((100,100),[[-1,-1],[1,1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74542a09",
   "metadata": {},
   "source": [
    "We consider two-channel oscillating signals in two dimensions. The lifting procedure adds a third channel, defined as the norm $\\mu_2 = \\sqrt{\\mu_0^2+\\mu_1^2}$ of the first two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0679ef83",
   "metadata": {},
   "outputs": [],
   "source": [
    "μ = damped_oscillating_2(Xs,[6,8],0.3)\n",
    "\n",
    "figs,axs = plt.subplots(1,3,figsize=[15,4])\n",
    "for i,ax in enumerate(axs):\n",
    "    ax.contourf(*Xs,μ[i])\n",
    "    ax.set_title(f\"μ[{i}]\")\n",
    "    plt.axis('equal')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b872a73d",
   "metadata": {},
   "source": [
    "The two numerical solvers, and the GPU implementation, solve this problem without issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "171e5ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "λ,ξ = damped_oscillating_test_2(Xs,[6,8],0.3,[0.2,0.4],channel=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "24364ffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primal-dual solver completed 551 steps in 0.24980592727661133 seconds\n",
      "CPU times: user 233 ms, sys: 21.7 ms, total: 255 ms\n",
      "Wall time: 254 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "res1,_ = solve1_ot(λ,ξ,dx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "eb9c0c52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primal-dual solver completed 71 steps in 0.5353000164031982 seconds\n",
      "CPU times: user 1.23 s, sys: 526 ms, total: 1.75 s\n",
      "Wall time: 1.43 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "res2,_ = solve2_ot(λ,ξ,dx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5c8a9e91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1 µs, sys: 1 µs, total: 2 µs\n",
      "Wall time: 1.91 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if xp is not np: res3,_ = solve3_ot(λ,ξ,dx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c614d27b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Implicit, niter=71: 0.0721057125177082 <= W1 dist <= 0.07217754932205549\n",
      "Explicit niter=551: 0.07210719794400629 <= W1 dist <= 0.07217754592386884\n"
     ]
    }
   ],
   "source": [
    "dv = np.prod(dx)\n",
    "print(f\"Implicit, niter={res2['niter']}: {dv*res2['dual_values'][-1]} <= W1 dist <= {dv*res2['primal_values'][-1]}\")\n",
    "print(f\"Explicit niter={res1['niter']}: {-dv*res1['primal_values'][-1]} <= W1 dist <= {-dv*res1['dual_values'][-1]}\")\n",
    "if xp is not np: print(f\"Explicit (gpu), niter={res3['niter']}: {-dv*res3['primal_values'][-1]} <= W1 dist <= {-dv*res3['dual_values'][-1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17bb702",
   "metadata": {},
   "source": [
    "The first (as well as the second) channel of the measures is very oscillatory, which leads to the issue of mass transported on itself. \n",
    "The last channel (the norm of the first two, i.e. the signal envelope) is a smooth bump, which leads to poor sensitivity w.r.t. translations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7e3ada25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primal-dual solver completed 241 steps in 0.039093017578125 seconds\n"
     ]
    }
   ],
   "source": [
    "λ,ξ = damped_oscillating_test_2(Xs,[6,8],0.3,[-0.6,-0.6],channel=0)\n",
    "res1,_ = solve1_ot(λ,ξ,dx)\n",
    "\n",
    "plt.figure(figsize=[14,4])\n",
    "plt.subplot(1,3,1); plt.title(\"Difference of measures ξ\")\n",
    "plt.contour(*Xs,ξ); plt.axis('equal');\n",
    "plt.subplot(1,3,2); plt.title(\"Optimal potential\")\n",
    "plt.contour(*Xs,res1['x']); plt.axis('equal');\n",
    "plt.subplot(1,3,3); plt.title(\"Optimal flow\")\n",
    "Plotting.quiver(*Xv,*res1['y'],subsampling=(3,3)); plt.axis('equal');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "fe4a042c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primal-dual solver completed 711 steps in 0.11566781997680664 seconds\n"
     ]
    }
   ],
   "source": [
    "λ,ξ = damped_oscillating_test_2(Xs,[6,8],0.3,[-0.6,-0.6],channel=2)\n",
    "res1,_ = solve1_ot(λ,ξ,dx)\n",
    "\n",
    "plt.figure(figsize=[14,4])\n",
    "plt.subplot(1,3,1); plt.title(\"Difference of measures ξ\")\n",
    "plt.contour(*Xs,ξ); plt.axis('equal');\n",
    "plt.subplot(1,3,2); plt.title(\"Optimal potential\")\n",
    "plt.contour(*Xs,res1['x']); plt.axis('equal');\n",
    "plt.subplot(1,3,3); plt.title(\"Optimal flow\")\n",
    "Plotting.quiver(*Xv,*res1['y'],subsampling=(3,3)); plt.axis('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9830b87",
   "metadata": {},
   "source": [
    "Similarly to the one dimensional case : \n",
    "- Using only an oscillating channel (0 or 1) yields several local minima and maxima.\n",
    "- Using only channel 2, which is positive, yields a single well, but behavior near the minimum is very smooth. \n",
    "- Using all channels together yields a single well, with a minimizer that is well localized.\n",
    "\n",
    "Note that in the latter case, we have more resolution in the direction orthogonal to the phase. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b583d222",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 23.1 s, sys: 570 ms, total: 23.6 s\n",
      "Wall time: 23.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "# Note : this cell is a bit long to execute, especially on the CPU\n",
    "if xp is np: solve_ot,nshift = solve1_ot,7\n",
    "else: solve_ot,nshift = solve3_ot,9 # Use gpu if available\n",
    "shift = np.linspace(-0.6,0.6,nshift)\n",
    "shifts = np.array(np.meshgrid(shift,shift,indexing='ij'))\n",
    "results = [[[-solve_ot(*damped_oscillating_test_2(Xs,[6,8],0.3,shifts[:,i,j],channel),dx,verbosity=0)[0]['dual_values'][-1]\n",
    "             for j in range(nshift)] for i in range(nshift)] \n",
    "           for channel in (0,1,2,None)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "76146946",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axs = plt.subplots(2,2,figsize=[10,10])\n",
    "axs[0,0].contourf(*shifts,results[0]); axs[0,0].set_title(\"Channel 0\")\n",
    "axs[1,0].contourf(*shifts,results[1]); axs[1,0].set_title(\"Channel 1\")\n",
    "axs[0,1].contourf(*shifts,results[2]); axs[0,1].set_title(\"Channel 2\")\n",
    "axs[1,1].contourf(*shifts,results[3]); axs[1,1].set_title(\"All channels\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec25da39",
   "metadata": {},
   "source": [
    "### 3.3 A loss function between seismograms\n",
    "\n",
    "One of the objective of this notebook is to investigate loss functions between multichannel seismograms.\n",
    "We subsample this large test case in order to have reasonnable runtimes, and observe that : \n",
    "- The implicit scheme converges in approximately 200 steps, independently of the subsampling rate. However, its efficiency is limited by the choice of linear solver\n",
    "- The explicit scheme (cpu or gpu) needs excessively many steps to reach convergence, at least 100000/k where k is the subsampling rate. This makes it prohibitive for our usage. \n",
    "\n",
    "<!---\n",
    "μ1 = np.fromfile(\"data/mu1_0064\",dtype=np.float32).reshape(3000,169)\n",
    "μ2 = np.fromfile(\"data/mu2_0064\",dtype=np.float32).reshape(3000,169)\n",
    "ν1 = np.fromfile(\"data/nu1_0064\",dtype=np.float32).reshape(3000,169)\n",
    "ν2 = np.fromfile(\"data/nu2_0064\",dtype=np.float32).reshape(3000,169)\n",
    "ξ = np.array([ν1-μ1,ν2-μ2])\n",
    "\n",
    "np.savez_compressed(\"data/xi.npz\",xi0=ξ[0],xi1=ξ[1])\n",
    "--->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "985fd445",
   "metadata": {},
   "outputs": [],
   "source": [
    "subsampling=4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9123d1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Plotting.open_local_or_web(np.load,\"Notebooks_Div/TestData/xi.npz\")\n",
    "ξ = lift(data['xi0'],data['xi1'])\n",
    "λ = 21\n",
    "\n",
    "ξ = ξ[:,::subsampling,::subsampling] # subsample (optional, but computation time is long otherwise)\n",
    "Xs,Xv,dx = stag_grids(ξ.shape[1:],[[0,0],[1,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a65baad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.contour(*Xs,ξ[2])\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "fc491177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning : duality gap not reduced to target within iteration budget\n",
      "Primal-dual solver completed 5000 steps in 10.40587306022644 seconds\n"
     ]
    }
   ],
   "source": [
    "res1,_ = solve1_ot(λ,ξ,dx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e9635226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primal-dual solver completed 121 steps in 2.8237178325653076 seconds\n",
      "CPU times: user 3.94 s, sys: 166 ms, total: 4.11 s\n",
      "Wall time: 4.11 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "res2,_ = solve2_ot(λ,ξ,dx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a824409a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if xp is not np: res3,_ = solve3_ot(λ,ξ,dx,maxiter=100000,rtol=0) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1609af32",
   "metadata": {},
   "source": [
    "- The implicit scheme converged correctly.\n",
    "- The explicit scheme (cpu) did not have a sufficient iteration budget, and produces incorrect energies. \n",
    "- The explicit scheme (gpu) is limited by the float32 accuracy. Even with a very large iteration budget, it does not reduce the primal dual gap to acceptable levels. (Note : the keyword argument `rtol=0` removes a stopping criterion based on the stabilization of the solution, so that only the primal-dual gap is considered.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "0b2cf7cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Implicit, niter=121: 8.432089857537884 <= W1 dist <= 8.43861283497407\n",
      "Explicit niter=5000: -106.24535782242252 <= W1 dist <= 8.439386190965097\n"
     ]
    }
   ],
   "source": [
    "dv=np.prod(dx)\n",
    "print(f\"Implicit, niter={res2['niter']}: {dv*res2['dual_values'][-1]} <= W1 dist <= {dv*res2['primal_values'][-1]}\")\n",
    "print(f\"Explicit niter={res1['niter']}: {-dv*res1['primal_values'][-1]} <= W1 dist <= {-dv*res1['dual_values'][-1]}\")\n",
    "if xp is not np: print(f\"Explicit (gpu), niter={res3['niter']}: {-dv*res3['primal_values'][-1]} <= W1 dist <= {-dv*res3['dual_values'][-1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e311f204",
   "metadata": {},
   "source": [
    "The primal dual gap of the explicit scheme on the gpu does not decrease to zero due to fp32 limited accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d576e420",
   "metadata": {},
   "outputs": [],
   "source": [
    "if xp is not np: \n",
    "    plt.title(\"Primal-dual gap, explicit scheme (gpu)\") \n",
    "    plt.loglog(res3['primal_values']-res3['dual_values']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae864f7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}