{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "969f84c0",
   "metadata": {},
   "source": [
    "# Adaptive PDE discretizations on Cartesian grids\n",
    "## Volume : Divergence form PDEs\n",
    "## Part : Primal-Dual optimization\n",
    "## Chapter : The mincut problem\n",
    "\n",
    "$$\n",
    "\\def\\cF{\\mathcal{F}}\n",
    "\\def\\cO{\\mathcal{O}}\n",
    "\\def\\cL{\\mathcal{L}}\n",
    "\\def\\bE{\\mathbb{E}}\n",
    "\\def\\bR{\\mathbb{R}}\n",
    "\\def\\bT{\\mathbb{T}}\n",
    "\\def\\vp{\\varphi}\n",
    "\\def\\diff{\\mathrm{d}}\n",
    "\\def\\sm{\\setminus}\n",
    "\\DeclareMathOperator\\TV{TV}\n",
    "\\DeclareMathOperator\\Area{Area}\n",
    "\\DeclareMathOperator\\prox{prox}\n",
    "\\DeclareMathOperator\\proj{proj}\n",
    "\\DeclareMathOperator*\\argmin{argmin}\n",
    "\\DeclareMathOperator\\diver{div}\n",
    "\\DeclareMathOperator\\sign{sign}\n",
    "\\def\\Id{\\mathrm{Id}}\n",
    "\\def\\<{\\langle}\n",
    "\\def\\>{\\rangle}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819f0fdc",
   "metadata": {},
   "source": [
    "The objective of this notebook is to numerically solve the *mincut* geometric optimization problem associated with an anisotropic metric, using Chambolle-Pock primal-dual optimization method.\n",
    "\n",
    "### The mincut problem\n",
    "Let $\\Omega \\subset \\bR^d$ be a domain. In this notebook, $\\Omega$ is box shaped, generally two-dimensional, and equipped with reflected boundary conditions. Denote by $\\cF_x(v)$ a metric on a domain $\\Omega$, and let $g : \\Omega \\to \\bR$ be a bias cost function. \n",
    "\n",
    "The mincut problem asks for the minimizer $\\vp : \\Omega \\to \\bR$ of the following convex functional\n",
    "$$\n",
    "    E(\\vp) := \\int_\\Omega \\Big( \\cF_x(\\nabla \\vp(x)) + g(x)\\vp(x) \\Big) \\diff x,\n",
    "$$\n",
    "subject to the constraint $-1 \\leq \\vp \\leq 1$ on $\\Omega$ (or sometimes equivalently $0\\leq \\vp \\leq 1$)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d69898e",
   "metadata": {},
   "source": [
    "**Geometric variant.**\n",
    "Consider the minimization of the following functional, related to the surface area\n",
    "$$\n",
    "    \\mathbf E(\\Phi) := \\int_{\\partial \\Phi} \\cF_x(n_{\\Phi}(x)) \\diff \\sigma(x) + \\int_\\Phi g(x) \\diff x\n",
    "$$\n",
    "among all (rectifiable) subsets $\\Phi \\subset \\bT$, where $n_{\\Phi}$ denotes the outward normal.\n",
    "\n",
    "Using the co-area formula, one can show under adequate assumptions that $\\vp : \\bT \\to \\bR$ is a minimizer of $\\TV_\\cF$ iff for almost every $-1<t<1$ the sublevel set $\\Phi := \\{x \\in \\bT| \\vp(x) \\leq t\\}$ is a minimizer of $\\Area_\\cF$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68cdab68",
   "metadata": {},
   "source": [
    "**Comparison with existing work.**\n",
    "The continuous mincut problem has been extensively studied in the special case where $\\cF_x(v) = \\lambda |v|$ is a constant multiple of the Euclidean norm. (A multichannel variant known as the relaxed Potts model is also well studied.) Up to rescaling $g$ we may assume that $\\lambda=1$, and the only degree of freedom in the classical approach is thus the value $g(x)$ at each $x\\in \\Omega$. \n",
    "\n",
    "In this notebook, we investigate the relevance of non-constant and anisotropic metrics for the mincut problem. \n",
    "Depending on the choice of structure, we have up to $d(d+1)/2+d+d$ additional degrees of freedom per pixel to describe our objective functional ($7$ if $d=2$, and $12$ if $d=3$). \n",
    "\n",
    "We acknowledge that too many parameters can make a model difficult to use. \n",
    "However, they have here a natural geometric intepretation (Riemannian metric, etc), and a number of tools describing the local geometry of images can be leveraged to tune them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40643848",
   "metadata": {},
   "source": [
    "**Lagrangian, and dual problem**\n",
    "\n",
    "The mincut problem can be reformulated as :\n",
    "$$\n",
    "    \\inf_{-1 \\leq \\vp \\leq 1} \\sup_{\\cF^*_x(\\eta(x)) \\leq 1} \\cL(\\vp,\\eta) := \n",
    "    \\int_\\Omega \\Big(\\<\\nabla\\vp(x),\\eta(x)\\> + g(x) \\vp(x)\\Big) \\diff x.\n",
    "$$\n",
    "\n",
    "Interverting the inf-sup we obtain the dual problem of minimizing \n",
    "$$\n",
    "    -E^*(\\eta) := \\int_\\Omega |\\diver \\eta(x) - g(x)| \\diff x,\n",
    "$$\n",
    "subject to the constraint $\\cF^*_x(\\eta(x)) \\leq 1$ at each $x \\in \\Omega$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115dad68",
   "metadata": {},
   "source": [
    "**Chambolle-Pock primal-dual optimization algorithm**\n",
    "\n",
    "The numerical solution of \n",
    "$$\n",
    "    \\inf_{x\\in X} \\sup_{y \\in Y} \\<K x,y\\> + F(x)-G^*(y),\n",
    "$$\n",
    "can be addressed with the algorithm\n",
    "\\begin{align*}\n",
    "    x^{n+1}&:=\\prox_{\\sigma F}(x^n+\\sigma K^\\top y^n),\\\\\n",
    "    \\overline x^{n+1}&:=2 x^{n+1} - x^n,\\\\\n",
    "    y^{n+1}&:=\\prox_{\\tau G^*}(y^n - \\tau K \\overline x^{n+1}),\n",
    "\\end{align*}\n",
    "where the proximal time steps $\\sigma,\\tau > 0$ are subject to the compatibility condition $\\sigma \\tau \\|K\\|^2 \\leq 1$. This basic algorithm admits a number of variants. \n",
    "\n",
    "Under suitable assumptions, the pair $(x^n,y^n)$ converges to a saddle point of the Lagrangian, and the  primal-dual gap goes to zero. Recall that this gap is defined as \n",
    "$$\n",
    "    E(x) + E^*(y)\n",
    "$$\n",
    "where \n",
    "\\begin{align*}\n",
    "    E(x) &:= F(x)+G(K x),\\\\\n",
    "    E^*(y) &:= -(F^*(-K^\\top y)+G^*(y)).\n",
    "\\end{align*}\n",
    "\n",
    "*Reference:* Chambolle, A. & Pock, T. On the ergodic convergence rates of a first-order primal--dual algorithm. Mathematical Programming 159, 253–287 (2016).\n",
    "  \n",
    "*Overrelaxation:* This algorithm also has an optional overrelaxation parameter `ρ_overrelax`$\\in [1,2]$, which cuts the number of iterations in half in the best case, but also often causes some instabilities, especially if strongly anisotropic metrics are used. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fb5e57",
   "metadata": {},
   "source": [
    "**Specialization to the mincut problem**\n",
    "\n",
    "Denoting $\\vp := x$ and $\\eta := y$, and denoting by $\\chi_{P(x)}$ the characteristic function of some property $P(x)$, we have \n",
    "\\begin{align*}\n",
    "    F(\\vp) &:= \\int_\\Omega \\big(g(x) \\vp(x) + \\chi_{-1\\leq \\vp(x) \\leq 1}\\big) \\diff x,\\\\\n",
    "    G^*(\\eta) &:= \\int_\\Omega  \\chi_{\\cF_x^*(\\eta(x)) \\leq 1} \\diff x.\n",
    "\\end{align*}\n",
    "The proximal operators associated to functions these are essentially projections. The primal and dual energies $E$ and $E^*$ are given above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a76ba34",
   "metadata": {},
   "source": [
    "[**Summary**](Summary.ipynb) of volume Divergence form PDEs, this series of notebooks.\n",
    "\n",
    "[**Main summary**](../Summary.ipynb) of the Adaptive Grid Discretizations \n",
    "\tbook of notebooks, including the other volumes.\n",
    "\n",
    "# Table of contents\n",
    "  * [1. Implementation of the proximal operators](#1.-Implementation-of-the-proximal-operators)\n",
    "    * [1.1 Projection onto the unit ball of the metric](#1.1-Projection-onto-the-unit-ball-of-the-metric)\n",
    "    * [1.2 Second proximal operator](#1.2-Second-proximal-operator)\n",
    "  * [2. Staggered grid](#2.-Staggered-grid)\n",
    "    * [2.1 Grid layout and dimensions](#2.1-Grid-layout-and-dimensions)\n",
    "    * [2.2 Gradient operators](#2.2-Gradient-operators)\n",
    "    * [2.3 Divergence](#2.3-Divergence)\n",
    "    * [2.4 Primal and dual energies](#2.4-Primal-and-dual-energies)\n",
    "  * [3. One dimension](#3.-One-dimension)\n",
    "    * [3.1 Variable metric, vanishing ground cost](#3.1-Variable-metric,-vanishing-ground-cost)\n",
    "    * [3.2 Constant metric, variable ground cost](#3.2-Constant-metric,-variable-ground-cost)\n",
    "  * [4. Two dimensions](#4.-Two-dimensions)\n",
    "    * [4.1 Variable metric, vanishing ground cost](#4.1-Variable-metric,-vanishing-ground-cost)\n",
    "    * [4.2 Constant metric, variable ground cost](#4.2-Constant-metric,-variable-ground-cost)\n",
    "  * [5. Choice of metric](#5.-Choice-of-metric)\n",
    "    * [5.1 Filtering](#5.1-Filtering)\n",
    "    * [5.2 Isotropic metric](#5.2-Isotropic-metric)\n",
    "    * [5.3 Riemannian metric](#5.3-Riemannian-metric)\n",
    "    * [5.4 Randers metric](#5.4-Randers-metric)\n",
    "    * [5.5 Randers metric via divergence](#5.5-Randers-metric-via-divergence)\n",
    "    * [5.6 Asymmetric quadratic metrics](#5.6-Asymmetric-quadratic-metrics)\n",
    "\n",
    "\n",
    "\n",
    "**Acknowledgement.** Some of the experiments presented in these notebooks are part of \n",
    "ongoing research with Ludovic Métivier and Da Chen.\n",
    "\n",
    "Copyright Jean-Marie Mirebeau, Centre Borelli, ENS Paris-Saclay, CNRS, University Paris-Saclay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0879c75d",
   "metadata": {},
   "source": [
    "## 0. Importing the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b615ac6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.path.insert(0,\"..\") # Allow import of agd from parent directory (useless conda package is installed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aaf1f509",
   "metadata": {
    "tags": [
     "ExportCode"
    ]
   },
   "outputs": [],
   "source": [
    "from agd import AutomaticDifferentiation as ad\n",
    "import numpy as np; xp=np\n",
    "from agd.Eikonal.HFM_CUDA import MinCut\n",
    "from agd.ODE import proximal\n",
    "from scipy.ndimage import gaussian_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19bb1a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm = ad.Optimization.norm\n",
    "from agd import Metrics\n",
    "from agd import LinearParallel as lp\n",
    "from agd import Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4bff773b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt \n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a40d2f",
   "metadata": {},
   "source": [
    "### 0.1 Additional configuration\n",
    "\n",
    "The notebook presents a step by step CPU implementation of the numerical solution of the mincut problem.\n",
    "We also propose an optimized GPU implementation, which is considerably faster for large test cases (CUDA gpu required).\n",
    "\n",
    "The GPU implementation can also address three dimensional problem instances (not demonstrated in this notebook).\n",
    "\n",
    "Uncomment the following line to use the gpu implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35eebb4f",
   "metadata": {
    "tags": [
     "GPU_config"
    ]
   },
   "outputs": [],
   "source": [
    "#import cupy as xp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd08ac6a",
   "metadata": {},
   "source": [
    "The CPU and GPU implementations are meant to be consistent up to machine precision. The following method checks this property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "593c2ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_cpu_gpu(data_cpu,maxiter=4,data_gpu=None):\n",
    "    if xp is np: return # No GPU available\n",
    "    res_cpu = mincut_cpu(*data_cpu,maxiter=maxiter,E_rtol=0) # Run on the cpu #ρ_overrelax=None\n",
    "\n",
    "    # Run on the gpu\n",
    "    (g,metric,dx,gradname) = data_cpu if data_gpu is None else data_gpu \n",
    "    res_gpu = MinCut.mincut(g,metric,dx,maxiter=maxiter,E_rtol=0,grad=gradname)\n",
    "\n",
    "    # Compare the results\n",
    "    ops,tmp = res_cpu['ops'],res_cpu['tmp']\n",
    "    E_primal,E_dual = ops['E_primal'],ops['E_dual']\n",
    "    def norminf(a): return np.max(np.abs(a))\n",
    "    print(\"ϕ max error : \",norminf(res_gpu[\"ϕ\"].get()-res_cpu['x']))\n",
    "    print(\"η max error : \",norminf(res_gpu[\"η\"].get()-res_cpu['y']))\n",
    "    print(f\"primal vals. GPU={res_gpu['primal_values'][-1]}, CPU={E_primal(tmp['x_'])}\")\n",
    "    print(f\"dual vals. GPU={res_gpu['dual_values'][-1]}, CPU={E_dual(tmp['yold'])}\")\n",
    "\n",
    "    tols={'rtol':1e-5,'atol':1e-5}; \n",
    "    assert np.allclose(res_gpu['dual_values'][-1],E_dual(tmp['yold']),**tols)\n",
    "    assert np.allclose(res_gpu['primal_values'][-1],E_primal(tmp[\"x_\"]),**tols)\n",
    "    assert np.allclose(res_gpu[\"ϕ\"].get(),res_cpu['x'],**tols)\n",
    "    assert np.allclose(res_gpu[\"η\"].get(),res_cpu['y'],**tols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94cd8b34",
   "metadata": {},
   "source": [
    "## 1. Implementation of the proximal operators"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09499e32",
   "metadata": {},
   "source": [
    "### 1.1 Projection onto the unit ball of the metric\n",
    "\n",
    "The optimization problem of interest involves the function\n",
    "$$\n",
    "    G^*(\\eta) := \\int_\\Omega g^*_x(\\eta(x)) \\diff x.\n",
    "$$\n",
    "where $g^*_x(\\eta) := \\chi_{\\cF_x^*(\\eta) \\leq 1}$ is the characteristic function of the unit ball $B^*$ of the dual metric $\\cF^*$. Thanks to the separable structure of $G^*$, the proximal operator (and the dual function) can be computed pointwise.\n",
    "\n",
    "The proximal operator $\\prox_{\\tau G^*}$ is obtained pointwise as the orthogonal projection onto the dual unit ball $B^*$, for any $\\tau>0$.\n",
    "For the geometries of interest (isotropic, Riemannian, asymmetric quadratic), this projection can be efficiently computed, and is implemented in the agd library. (The solution involves some root finding using a one-dimensional Newton method.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d949ce86",
   "metadata": {},
   "outputs": [],
   "source": [
    "aX = np.linspace(-2,2,20)\n",
    "X = np.array(np.meshgrid(aX,aX,indexing='ij'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1284a0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def illustrate_proj(x,metric):\n",
    "    proj = metric.make_proj_dual()\n",
    "    \n",
    "    plt.axis('equal'); plt.title(f\"Projection, {metric.model_HFM()}\")\n",
    "    plt.quiver(*x,*(proj(x)-x),angles='xy',scale_units='xy',scale=1)\n",
    "    \n",
    "    plt.contour(*x,metric.dual().norm(x),levels=[1],colors=['red'])\n",
    "    plt.scatter(*(0,0),color='black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4365d94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=[10,10])\n",
    "metrics = [Metrics.Isotropic(1.3,vdim=2),Metrics.Riemann([[2.,1.],[1.,1.]]),\n",
    "           Metrics.Rander([[1.,-0.5],[-0.5,0.4]],[0.2,0.1]),Metrics.AsymQuad([[1.,0],[0,1]],[1.3,1.7])]\n",
    "for i,metric in enumerate(metrics):\n",
    "    plt.subplot(2,2,i+1)\n",
    "    illustrate_proj(X,metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7af0457",
   "metadata": {},
   "source": [
    "While we're at it, we also implement $G^*$ and $G$. As already observed\n",
    "$$\n",
    "    G(\\sigma) = \\int_\\Omega \\cF_x(\\sigma(x)) \\diff x,\n",
    "$$\n",
    "is the integral of the norm of the vector field $\\sigma$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "add1b536",
   "metadata": {
    "tags": [
     "ExportCode"
    ]
   },
   "outputs": [],
   "source": [
    "def make_Gs(metric):\n",
    "    \"\"\"Implementation of the constraint G(η) the characteristic function of the constraint\n",
    "    F_x^*(η(x)) <= 1 for all x, and its dual and prox\"\"\"\n",
    "    proj = metric.make_proj_dual()\n",
    "    def Gs(η): return 0. # Gs:=G^* will only be evaluated inside its domain\n",
    "    def G(σ):  return metric.norm(σ).sum()\n",
    "    def prox_Gs(η,τ=1.): return proj(η)\n",
    "    return Gs,G,prox_Gs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c1d889",
   "metadata": {},
   "source": [
    "### 1.2 Second proximal operator\n",
    "\n",
    "The function \n",
    "$$\n",
    "    f(\\vp) := \\int_\\Omega \\Big(g(x) \\vp(x) + \\chi_{-1 \\leq \\vp(x) \\leq 1}\\Big) \\diff x\n",
    "$$\n",
    "is separable like $g$. \n",
    "Again, thanks to the separable structure of $F$, we can compute the proximal operator (and dual function) pointwise.\n",
    "\n",
    "Note that the dual to $\\chi_{[-1,1]}$ is the absolute value function, whereas the corresponding proximal operator is the projection onto $[-1,1]$. The linear term introduces a shift in these operators, see the implementations below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5b2043",
   "metadata": {},
   "source": [
    "While we're at it, we also implement $F$ and $F^*$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "73b30dd1",
   "metadata": {
    "tags": [
     "ExportCode"
    ]
   },
   "outputs": [],
   "source": [
    "def make_F(g):\n",
    "    def F(ϕ):  return np.sum(g*ϕ) # Only evaluated inside its domain\n",
    "    def Fs(ψ): return np.sum(np.abs(ψ-g))\n",
    "    def prox_F(ϕ,τ): return np.maximum(-1.,np.minimum(1.,ϕ-τ*g))\n",
    "    return F,Fs,prox_F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9abe8a",
   "metadata": {},
   "source": [
    "## 2. Staggered grid\n",
    "\n",
    "Computations occur on a staggered grid. Vectors are stored at the cell centers, and function values at the cell corners.\n",
    "\n",
    "The basic identity for estimating the operator norm of the gradient operator is \n",
    "$$\n",
    "    \\big(\\frac{\\vp(x+h)-\\vp(x)} {h}\\big)^2 \\leq 2 h^{-2} \\big(\\vp(x)^2 + \\vp(x+h)^2\\big).\n",
    "$$\n",
    "From this point, one obtains for various discretizations with grid scale $h=(h_1,\\cdots,h_d)$, that\n",
    "$$\n",
    "    \\|\\nabla_h\\|^2 = \\|\\diver_h\\|^2 \\leq 4 \\sum_{1 \\leq i \\leq d} h_i^{-2}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "856291a3",
   "metadata": {
    "tags": [
     "ExportCode"
    ]
   },
   "outputs": [],
   "source": [
    "def norm2_grad(dx,gradname):\n",
    "    \"\"\"Squared norm of the discrete gradient operator\"\"\"\n",
    "    assert np.ndim(dx)==1 # One grid scale per axis\n",
    "    res = 4*(dx**-2).sum()\n",
    "    return (res/2) if gradname=='grad2' else res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ed3481",
   "metadata": {},
   "source": [
    "### 2.1 Grid layout and dimensions\n",
    "\n",
    "The domain is discretized using two grids : \n",
    "- a cell centered grid $X_m$ used for quantities related with the metric, vector fields, etc\n",
    "- a vertex centered one $X_\\vp$, used for the level set function.\n",
    "\n",
    "<!---\n",
    "(Note: For the discretizations `gradb` and `gradt` considered below, this estimate is actually slightly violated due to the introduction of weights on a boundary layer. We ignore this detail, which raises no numerical issue.)\n",
    "\n",
    "The grid $X_\\phi$ has an additional layer of points. For consistency, we introduce weights when computing integrals on $X_\\phi$, so that the total mass of the domain is independent of the chosen grid. \n",
    "\n",
    "Our mincut solver uses reflected boundary conditions.\n",
    "\n",
    "For consistent numerical integration, the edges should be counted with half weight, and the corners with 1/4 weight, in two dimensions. The following function returns the weights, as sparse matrix based implementations of the gradient and negative divergence. (In pure Python, this is usually more efficient than finite differences.)\n",
    "\n",
    "\n",
    "def stag_weights(shape,xp=np):\n",
    "    \"\"\"\n",
    "    Integration weights for the standard grid, consistent with reflected boundary conditions.\n",
    "    In two dimensions : 1 in the interior, 1/2 one edges, 1/4 on corners.\n",
    "    \"\"\"\n",
    "    weights = xp.ones(shape)\n",
    "    for i in range(len(shape)): \n",
    "        weights[(slice(None),)*i + ( 0,)]*=0.5\n",
    "        weights[(slice(None),)*i + (-1,)]*=0.5\n",
    "    return weights\n",
    "    \n",
    "    \n",
    "weights = stag_weights(Xϕ[0].shape)\n",
    "weights\n",
    "\n",
    "\n",
    "\n",
    "def grad_ndiv_operators(grad,dx,weights):\n",
    "    \"\"\"\n",
    "    Gradient and negative divergence operators, sparse matrix based implementation.\n",
    "    (Turns a programmatic implementation of a linear function into a sparse matrix implementation,\n",
    "    which respects the shapes of the inputs and outputs. Also transposes.)\n",
    "    Input : \n",
    "    - grad : gradient implementation, usually finite differences based.\n",
    "    - dx : grid scale\n",
    "    - weights : integration weights for self adjointness, in the potential domain.\n",
    "    Output : (grad, ndiv) functions\n",
    "    \"\"\"\n",
    "    shape = weights.shape\n",
    "    size = weights.size\n",
    "    shape_grad = grad(weights,dx).shape\n",
    "    ϕ_ad = ad.Sparse.identity(shape)\n",
    "    grad_ = ad.array(grad(ϕ_ad,dx)).tangent_operator(size)\n",
    "    ndiv_ = ad.array(grad(ϕ_ad/weights,dx)).adjoint_operator(size) # Cautious about scalar product\n",
    "    def grad(ϕ): return (grad_*ϕ.reshape(-1)).reshape(shape_grad)\n",
    "    def ndiv(η): return (ndiv_*η.reshape(-1)).reshape(shape)\n",
    "    grad.T=ndiv; ndiv.T=grad\n",
    "    return grad, ndiv\n",
    "\n",
    "We introduce some weights in the summation of $F$, related to the treatment of the boundary conditions, see the next section.\n",
    "\n",
    "def make_F(g,weights):\n",
    "    def F(ϕ):  return (g*ϕ*weights).sum() # Only evaluated inside its domain\n",
    "    def Fs(ψ): return (np.abs(ψ-g)*weights).sum()\n",
    "    def prox_F(ϕ,τ): return np.maximum(-1.,np.minimum(1.,ϕ-τ*g))\n",
    "    return F,Fs,prox_F\n",
    "--->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2966cdac",
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = (7,8)\n",
    "Xϕ,Xm,dx = MinCut.stag_grids(shape,[[-1,-1],[1,1]]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8019b01",
   "metadata": {},
   "source": [
    "The variable `dx` contains the grid scales along each axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b3c3f7f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.33333333, 0.28571429])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e7bbd71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Discretization points\"); plt.axis('equal')\n",
    "plt.scatter(*Xm,label=\"vectors\")\n",
    "plt.scatter(*Xϕ,label=\"scalars\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df52817",
   "metadata": {},
   "source": [
    "### 2.2 Gradient operators\n",
    "\n",
    "We use standard, axis aligned finite differences to discretize the gradient. For that purpose, we introduce an utility function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cb9d753b",
   "metadata": {
    "tags": [
     "ExportCode"
    ]
   },
   "outputs": [],
   "source": [
    "def drop(arr,n,axis=0): \n",
    "    \"\"\"\n",
    "    Drop the first or last elements of an array, along the chosen axis.\n",
    "    - n : number of elements to drop. If n<0 then drop from the end.\n",
    "    - axis : index of the axis along which to drop.\n",
    "    \"\"\"\n",
    "    sl = slice(n,None) if n>=0 else slice(0,n)\n",
    "    if axis<0: axis += arr.ndim\n",
    "    return arr[(slice(None),)*axis+(sl,)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78c3768",
   "metadata": {},
   "source": [
    "In one dimension, there is a single obvious way to discretize the gradient operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e780bece",
   "metadata": {
    "tags": [
     "ExportCode"
    ]
   },
   "outputs": [],
   "source": [
    "def grad1(ϕ,dx): return (drop(ϕ,1)-drop(ϕ,-1))[None]/dx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa75fcdd",
   "metadata": {},
   "source": [
    "There are several ways to discretize a two dimensional gradient operator, in a cell centered grid, with grid scale $h$. \n",
    "\n",
    "We can use upwind finite differences from the bottom corner:\n",
    "$$\n",
    "    \\nabla_h^b u(x,y) := ( \\frac{u(x+h,y)-u(x,y)} h, \\frac {u(x,y+h)-u(x,y)} h)\n",
    "$$\n",
    "Alternatively, downwind finite differences from the top corner:\n",
    "$$\n",
    "    \\nabla_h^t u(x,y) := ( \\frac{u(x+h,y+h)-u(x,y+h)} h, \\frac {u(x+h,y+h)-u(x+h,y)} h)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2d06f11a",
   "metadata": {
    "tags": [
     "ExportCode"
    ]
   },
   "outputs": [],
   "source": [
    "def gradb(ϕ,dx): return ad.array([drop(drop(ϕ, 1,i)-drop(ϕ,-1,i),-1,1-i)/dxi for i,dxi in enumerate(dx)])\n",
    "def gradt(ϕ,dx): return ad.array([drop(drop(ϕ, 1,i)-drop(ϕ,-1,i), 1,1-i)/dxi for i,dxi in enumerate(dx)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febc4459",
   "metadata": {},
   "source": [
    "Both $\\nabla_h^b$ and $\\nabla_h^t$ use non-centered finite differences. In order improve accuracy, one option is to average them, defining:\n",
    "$$\n",
    "    \\nabla^c_h u := \\frac 1 2 (\\nabla^b_h u + \\nabla^t_h u)\n",
    "$$\n",
    "One easily obtains that for smooth $u$\n",
    "$$\n",
    "    \\nabla_h^b u(x) = \\nabla u(x)+\\cO(h), \\quad \n",
    "    \\nabla_h^t u(x) = \\nabla u(x)+\\cO(h), \\quad \n",
    "    \\nabla_h^c u(x) = \\nabla u(x)+\\cO(h^2).\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f14344d3",
   "metadata": {
    "tags": [
     "ExportCode"
    ]
   },
   "outputs": [],
   "source": [
    "def gradc(ϕ,dx): return 0.5*(gradb(ϕ,dx)+gradt(ϕ,dx))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aced1c9",
   "metadata": {},
   "source": [
    "Unfortunately, $\\nabla^c_h$ vanishes on chessboard patterns, which leads to chessboard artifacts in numerical simulations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "13e24452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0 1 0 1 0 1]\n",
      " [1 0 1 0 1 0 1 0]\n",
      " [0 1 0 1 0 1 0 1]\n",
      " [1 0 1 0 1 0 1 0]\n",
      " [0 1 0 1 0 1 0 1]\n",
      " [1 0 1 0 1 0 1 0]\n",
      " [0 1 0 1 0 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "chessboard = (np.arange(shape[0])[:,None]+np.arange(shape[1])[None])%2\n",
    "print(chessboard)\n",
    "assert np.allclose( gradc(chessboard+0.,dx), 0) # The chessboard pattern lies in the kernel of gradc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1d0ec9",
   "metadata": {},
   "source": [
    "Another option is to keep both $\\nabla^b_h$ and $\\nabla^t_h$. If $f$ and $u$ are a smooth functions, then a symmetry argument shows that \n",
    "$$\n",
    "    \\tfrac 1 2 (f(\\nabla^b_h u) + f(\\nabla^t_h u)) = f(\\nabla u) + \\cO(h^2).\n",
    "$$\n",
    "Thus second order accuracy is recovered, and the chessboard pattern is not in the opertor kernel anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f622d5ab",
   "metadata": {
    "tags": [
     "ExportCode"
    ]
   },
   "outputs": [],
   "source": [
    "def grad2(ϕ,dx): return 0.5*np.stack([gradb(ϕ,dx), gradt(ϕ,dx)],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2a83e6",
   "metadata": {},
   "source": [
    "### 2.3 Divergence\n",
    "\n",
    "The negative divergence is obtained as the transposed operator to the gradient.\n",
    "We implement both using sparse matrices, which can be faster than finite differences in Python. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0fdc6476",
   "metadata": {
    "tags": [
     "ExportCode"
    ]
   },
   "outputs": [],
   "source": [
    "def grad_ndiv_operators(grad,dx,shape):\n",
    "    \"\"\"\n",
    "    Gradient and negative divergence operators, sparse matrix based implementation.\n",
    "    (Turns a programmatic implementation of a linear function into a sparse matrix implementation,\n",
    "    which respects the shapes of the inputs and outputs. Also transposes.)\n",
    "    Input : \n",
    "    - grad : gradient implementation, usually finite differences based.\n",
    "    - dx : grid scale.\n",
    "    - shape : shape of the potential domain.\n",
    "    Output : (grad, ndiv) functions\n",
    "    \"\"\"\n",
    "    ϕ_ad = ad.Sparse.identity(shape)\n",
    "    grad_ad = ad.array(grad(ϕ_ad,dx))\n",
    "    shape_grad = grad_ad.shape\n",
    "    grad_ = grad_ad.tangent_operator(ϕ_ad.size)\n",
    "    ndiv_ = grad_ad.adjoint_operator(ϕ_ad.size)\n",
    "    def grad(ϕ): return (grad_*ϕ.reshape(-1)).reshape(shape_grad)\n",
    "    def ndiv(η): return (ndiv_*η.reshape(-1)).reshape(shape)\n",
    "    grad.T=ndiv; ndiv.T=grad\n",
    "    return grad, ndiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7127c7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "shape = (7,8)\n",
    "for grad_ in (gradb,gradc,grad2):\n",
    "    grad,ndiv = grad_ndiv_operators(grad_,dx,shape)\n",
    "    ϕ = np.random.rand(*shape)\n",
    "    η = np.random.rand(*grad(ϕ).shape)\n",
    "    \n",
    "    # Check that sparse matrix implementation is faithful\n",
    "    assert np.allclose(grad(ϕ),grad_(ϕ,dx))\n",
    "    # Check adjointness of gradient and negative divergence, with weight matrix\n",
    "    assert np.allclose( (grad(ϕ)*η).sum(), (ϕ*ndiv(η)).sum() )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afab14e1",
   "metadata": {},
   "source": [
    "### 2.4 Primal and dual energies\n",
    "\n",
    "The primal-dual gap is commonly used as a stopping criterion for the Chambolle-Pock optimization algorithm.\n",
    "Note that the knowledge of this quantity is not strictly necessary, since other criteria can be considered as well, such as the convergence of the iterates, or simply the number of iterations.\n",
    "\n",
    "<!---\n",
    "Let us implement the primal energy of the mincut problem, and the dual energy which is related to the maxflow problem.\n",
    "\n",
    "def make_Energies(g,metric,grad,dx):\n",
    "    \"\"\"\n",
    "    Returns the primal and dual energies, and the duality gap.\n",
    "    \"\"\"\n",
    "    weights = stag_weights(g.shape)\n",
    "    grad,ndiv = grad_ndiv_operators(grad,dx,weights)\n",
    "\n",
    "    def E_primal(ϕ): \n",
    "        \"\"\"\n",
    "        The primal energy of the mincut problem.\n",
    "        Assumes -1 <= ϕ(x) <= 1 for all x. (Otherwise infinite.)\n",
    "        \"\"\"\n",
    "        return metric.norm(grad(ϕ)).sum() + (ϕ*g*weights).sum()\n",
    "    \n",
    "    def E_dual(η):\n",
    "        \"\"\"\n",
    "        The dual energy, related with the maxflow problem.\n",
    "        Assumes F^*_x(η(x)) <= 1 for all x. (Otherwise infinite.)\n",
    "        \"\"\"\n",
    "        return np.abs((ndiv(η)+g)*weights).sum()\n",
    "    \n",
    "    def gap(ϕ,η):\n",
    "        \"\"\"The duality gap.\"\"\"\n",
    "        return E_primal(ϕ)+E_dual(η) \n",
    "    \n",
    "    return {'E_primal':E_primal,'E_dual':E_dual,'gap':gap,'grad':grad,'ndiv':ndiv,'weights':weights}\n",
    "\n",
    "Note the admissibility constraints, $-1 \\leq \\vp(x) \\leq 1$ and $\\cF_x^*(\\eta(x)) \\leq 1$ for all $x\\in \\Omega$, are automatically satisfied by the iterates of the Chambolle-Pock primal-dual algorithm.\n",
    "--->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b71d6d4",
   "metadata": {},
   "source": [
    "**General expression.** In fact, the primal and dual energies can be expressed generally in the primal-dual saddle-point optimization setting \n",
    "$$\n",
    "    \\inf_x \\sup_y \\<K x,y\\> + F(x) - G^*(y),\n",
    "$$\n",
    "as \n",
    "$$\n",
    "    E(x) := F(x) + G(K x),\n",
    "$$\n",
    "and \n",
    "$$\n",
    "    E^*(y) := -(F^*(-K^\\top y) + G^*(y)).\n",
    "$$\n",
    "\n",
    "Thus it is enough to provide the implementations of $F,F^*,G^*,G$ and the linear operators $K$ and $K^\\top$. In fact, we already implemented all of these methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf69bdf",
   "metadata": {},
   "source": [
    "## 3. One dimension\n",
    "\n",
    "We illustrate and validate the mincut segmentation in one dimension.\n",
    "Numerical applications are usually in higher dimension, of course, but the visualization and debugging is easier in one dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82b11c7",
   "metadata": {},
   "source": [
    "### 3.1 Variable metric, vanishing ground cost\n",
    "\n",
    "In this first experiment, the ground cost is vanishing except in the neighborhood of the center and of the interval boundary, which serve as source and sink regions. \n",
    "\n",
    "The cost function defining the metric varies however, and as a result the segmentation places the region interfaces exactly where their cost is minimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5da9c35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xϕ,Xm,dx = MinCut.stag_grids((101,),[[-1],[1]]) \n",
    "Xm=Xm[0]; Xϕ=Xϕ[0] # Only one coordinate in dimension one\n",
    "g = (np.abs(Xϕ)<=0.1) - (np.abs(Xϕ)>=0.9).astype(float)\n",
    "metric = Metrics.Isotropic( 0.1*(1.+0.5*np.cos(2*np.pi*Xm)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3d26031a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(Xϕ,g,label=\"ground cost\")\n",
    "plt.plot(Xm,metric.cost,label=\"metric\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b9bed4",
   "metadata": {},
   "source": [
    "Let us generate further input data needed for the Chambolle-Pock algorithm.\n",
    "\n",
    "<!---\n",
    "# Alternatively, τ_F = 0.2/np.max(np.abs(g)) \n",
    "--->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c9461333",
   "metadata": {},
   "outputs": [],
   "source": [
    "ϕ0 = np.zeros(g.shape) # Initial guess\n",
    "grad,_ = grad_ndiv_operators(grad1,dx,g.shape) # Coupling operator\n",
    "\n",
    "# Proximal operators\n",
    "impl_F,impl_Gs = make_F(g),make_Gs(metric)\n",
    "τ_F = 0.2 # Reasonnable defaults : 0.1 ... 1  \n",
    "τ_Gs = 1./(τ_F*norm2_grad(dx,'grad1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d9db65aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primal-dual solver completed 141 steps in 0.009001493453979492 seconds\n"
     ]
    }
   ],
   "source": [
    "res = proximal.chambolle_pock(impl_F,impl_Gs,τ_F,τ_Gs,grad,ϕ0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4777f152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-11.974256301604694 <= energy <= -11.980267284282716\n"
     ]
    }
   ],
   "source": [
    "print(f\"{res['primal_values'][-1]} <= energy <= {res['dual_values'][-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6184730a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ϕ = res['x']; η = res['y']\n",
    "\n",
    "plt.title(\"Primal and dual solutions\")\n",
    "plt.plot(Xϕ,ϕ,label='ϕ')\n",
    "plt.plot(Xm,*η,label='η')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "58c934f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"The constraint 'dual norm of flow <=1' is saturated at the discontinuity\")\n",
    "plt.plot(η[0],label='flow')\n",
    "plt.plot(metric.dual().norm(η),label='dual norm')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0bfcd56",
   "metadata": {},
   "source": [
    "The improvement of the primal-dual gap is not monotone along the iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "68401b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Primal dual gap along the iterations\")\n",
    "niter = 10*np.arange(res['primal_values'].size)\n",
    "plt.plot(niter,res['primal_values'],label='primal')\n",
    "plt.plot(niter,res['dual_values'],label='dual')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9403b89a",
   "metadata": {},
   "source": [
    "For convenience, we gather the preprocessing and the optimization in a method.\n",
    "\n",
    "<!---\n",
    "def mincut(g,metric,gradname,dx,τ_F=0.2,**kwargs):\n",
    "    grad = {'grad1':grad1,'gradb':gradb,'gradt':gradt,'gradc':gradc,'grad2':grad2}[gradname]\n",
    "    ops = make_Energies(g,metric,grad,dx)\n",
    "    gap,grad = [ops[key] for key in ('gap','grad')]\n",
    "    ϕ0 = np.zeros(g.shape)\n",
    "    τ_Gs = 1./(τ_F*norm2_grad(dx,gradname))    \n",
    "    res = proximal.chambolle_pock(make_prox_F(g),make_prox_Gs(metric),τ_F,τ_Gs,grad,ϕ0,**kwargs)\n",
    "    return res,ops\n",
    "--->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2f4167ab",
   "metadata": {
    "tags": [
     "ExportCode"
    ]
   },
   "outputs": [],
   "source": [
    "def mincut_cpu(g,metric,dx,gradname='gradb',τ_F=0.2,ρ_overrelax=1.8,**kwargs):\n",
    "    if len(dx)==1: gradname='grad1'\n",
    "    grad,_ = grad_ndiv_operators({'grad1':grad1,'gradb':gradb,'gradt':gradt,'gradc':gradc,'grad2':grad2}[gradname],dx,g.shape)\n",
    "    ϕ0 = np.zeros(g.shape)\n",
    "    τ_Gs = 1./(τ_F*norm2_grad(dx,gradname))    \n",
    "    res = proximal.chambolle_pock(make_F(g),make_Gs(metric),τ_F,τ_Gs,grad,ϕ0,ρ_overrelax=ρ_overrelax,**kwargs)\n",
    "    return {**res,'ϕ':res['x'],'η':res['y']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ebc5c5c7",
   "metadata": {
    "tags": [
     "ExportCode"
    ]
   },
   "outputs": [],
   "source": [
    "def mincut_gpu(*args,**kwargs): return ad.cupy_generic.cupy_get(MinCut.mincut(*args,**kwargs),iterables=(dict,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d03cc7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if xp is not np: mincut = mincut_gpu # Use gpu if available\n",
    "else: mincut = mincut_cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0da19634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primal-dual solver completed 4 steps in 0.000997781753540039 seconds\n",
      "GPU primal-dual solver completed 4 steps in 0.0010004043579101562 seconds\n",
      "ϕ max error :  1.1913299591270743e-07\n",
      "η max error :  4.394143833519326e-09\n",
      "primal vals. GPU=13.997852325439453, CPU=13.997858796206259\n",
      "dual vals. GPU=-22.0, CPU=-21.999999999999996\n"
     ]
    }
   ],
   "source": [
    "compare_cpu_gpu((g,metric,dx,'grad1'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10330a23",
   "metadata": {},
   "source": [
    "### 3.2 Constant metric, variable ground cost\n",
    "\n",
    "This is the most common usage setting in image segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d0fae5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xϕ,Xm,dx = MinCut.stag_grids((201,),[[-1],[1]]) \n",
    "Xm=Xm[0]; Xϕ=Xϕ[0] # Only one coordinate in dimension one\n",
    "g = np.cos(np.pi*Xϕ)\n",
    "metric = Metrics.Isotropic(0.1,vdim=1) # Constant cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ef7c58c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU primal-dual solver completed 118 steps in 0.02001667022705078 seconds\n"
     ]
    }
   ],
   "source": [
    "res = mincut(g,metric,dx,'grad1')\n",
    "assert res['niter']<1000 # Did not exhaust iteration budget"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33b0d7f",
   "metadata": {},
   "source": [
    "In this example, we have one central region where the ground cost $g$ is positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ce58c3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(Xϕ,g,label=\"g\")\n",
    "plt.plot(Xϕ,res['ϕ'],label=\"ϕopt\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b12f8f",
   "metadata": {},
   "source": [
    "We can add some perturbations to the ground cost function, and observe that the segmentation remains stable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8cf92fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(40)\n",
    "g_pert = g+(np.random.rand(*g.shape)-0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "38596418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU primal-dual solver completed 111 steps in 0.019997835159301758 seconds\n"
     ]
    }
   ],
   "source": [
    "metric = Metrics.Isotropic(0.1,vdim=1) # Large cost of interfaces\n",
    "res = mincut(g_pert,metric,dx,'grad1')\n",
    "\n",
    "plt.plot(Xϕ,g_pert,label=\"g\")\n",
    "plt.plot(Xϕ,res['ϕ'],label=\"region\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0fb4637f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primal-dual solver completed 4 steps in 0.0010199546813964844 seconds\n",
      "GPU primal-dual solver completed 4 steps in 0.000997781753540039 seconds\n",
      "ϕ max error :  2.2410672784189956e-07\n",
      "η max error :  6.359798382371551e-09\n",
      "primal vals. GPU=154.42367553710938, CPU=154.4236866157235\n",
      "dual vals. GPU=-136.3352508544922, CPU=-136.33525379527055\n"
     ]
    }
   ],
   "source": [
    "compare_cpu_gpu((g_pert,metric,dx,'grad1'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8313d309",
   "metadata": {},
   "source": [
    "If the cost of the region boundaries, which is defined by the metric, is too small, then additional interfaces can appear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c3cbebd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU primal-dual solver completed 11 steps in 0.0020008087158203125 seconds\n"
     ]
    }
   ],
   "source": [
    "metric = Metrics.Isotropic(0.002,vdim=1) # Very small cost of interfaces\n",
    "res = mincut(g_pert,metric,dx,'grad1')\n",
    "plt.plot(Xϕ,g_pert,label=\"g\")\n",
    "plt.plot(Xϕ,res['ϕ'],label=\"region\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23edcf76",
   "metadata": {},
   "source": [
    "## 4. Two dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40951cdb",
   "metadata": {},
   "source": [
    "### 4.1 Variable metric, vanishing ground cost\n",
    "\n",
    "We solve a minimal surface problem, whose solution is a circle.\n",
    "\n",
    "<!---\n",
    "res=mincut_gpu(g,metric,dx,maxiter=2600);\n",
    "res=mincut_gpu(g,metric,dx,maxiter=2600);\n",
    "res=mincut_gpu(g,metric,dx,maxiter=2600);\n",
    "\n",
    "plt.contourf(*Xϕ,res['ϕ'].get())\n",
    "plt.axis('equal');\n",
    "--->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2722bab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xϕ,Xm,dx = MinCut.stag_grids((101,101),[[-1,-1],[1,1]])\n",
    "r = np.linalg.norm(Xϕ,axis=0)\n",
    "g = 10.*(r<=0.2) - (np.max(np.abs(Xϕ),axis=0)>=0.9)\n",
    "\n",
    "#def cost0(r): return 0.05*(1.+0.5*np.cos(2*np.pi*r))\n",
    "def cost0(r): return 0.05*(1+np.abs(r-0.5)) # Non differentiable. Also works\n",
    "r = np.linalg.norm(Xm,axis=0)\n",
    "metric = Metrics.Isotropic(cost0(r)/np.maximum(r,0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a327a219",
   "metadata": {},
   "source": [
    "The ground cost $g$ involves large positive values in the center, and negative values on the boundary, thus defining a source and a sink."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d994def7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.contourf(*Xϕ,g,levels = [-1.5,-0.5,0.5,9.5,10.5])\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93d8e60",
   "metadata": {},
   "source": [
    "The metric is isotropic, with a cost function $\\mathrm{cost}_0(r)$ varying radially and which is minimal at $r=0.5$.  \n",
    "\n",
    "In the case where $\\phi$ is the indicator function of a centered disk of radius $r\\geq 0.2$, the cost of its interface is $2 \\times 2 \\pi\\  \\mathrm{cost}_0(r)$, where the first factor two is because we use the convention $-1\\leq \\vp \\leq 1$ (rather than $0 \\leq \\vp \\leq 1$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ed629efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.contourf(*Xm,metric.cost*r)\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878c3e6a",
   "metadata": {},
   "source": [
    "The exact energy of the optimal boundary is known in the continuous setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3a4a336c",
   "metadata": {},
   "outputs": [],
   "source": [
    "E_exact = 2*2*np.pi*cost0(0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a05c706",
   "metadata": {},
   "source": [
    "The problem is correctly solved by the numerical method, with all gradient discretizations, with comparable errors.\n",
    "We find as expected that the optimal region is a disk of radius $0.5$. \n",
    "\n",
    "The most visually pleasing solution is obtained with the gradc centered scheme (thinnest boundary, best rotation invariance). However, we will see later that it suffers from chessboard artifact unstabilities in some contexts.\n",
    "\n",
    "The gradc,gradt, and grad2 discretizations suffer from small artifacts, but nothing too serious. Second order accurate schemes,  gradc and grad2, do not provide a substantial advantage, which is not really surprising since the solution is not smooth anyway. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "906b6955",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[8,8])\n",
    "for i,gradname in enumerate(('gradb','gradt','gradc','grad2')):\n",
    "    res = mincut_cpu(g,metric,dx,gradname,verbosity=1)\n",
    "    \n",
    "    # Compare the computed and the exact energy\n",
    "    ϕ = res['ϕ']\n",
    "    E_primal = res['ops']['E_primal']\n",
    "    E_num = np.prod(dx)*(E_primal(ϕ) - np.sum(g*ϕ))\n",
    "    \n",
    "    plt.subplot(2,2,1+i)\n",
    "    plt.title(f\"{gradname}, {E_num-E_exact=:.4f}\")\n",
    "    plt.contourf(*Xϕ,ϕ)\n",
    "    plt.axis('equal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "66bb39b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primal-dual solver completed 4 steps in 0.0030014514923095703 seconds\n",
      "GPU primal-dual solver completed 4 steps in 0.001001119613647461 seconds\n",
      "ϕ max error :  2.405166630303768e-07\n",
      "η max error :  8.746385577601545e-09\n",
      "primal vals. GPU=614.6980590820312, CPU=614.6979475325311\n",
      "dual vals. GPU=-5505.419921875, CPU=-5505.42\n"
     ]
    }
   ],
   "source": [
    "compare_cpu_gpu((g,metric,dx,'grad2'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69d9d5d",
   "metadata": {},
   "source": [
    "### 4.2 Constant metric, variable ground cost\n",
    "\n",
    "We solve the mincut problem with a constant isotropic metric, and a ground cost which is a triangular geometric region. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "00a7600b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xϕ,Xm,dx = MinCut.stag_grids((101,71),[[0,0],[1,0.7]])\n",
    "g_sharp = ((Xϕ[0]>=0.15) & (Xϕ[1]>=0.15) & (Xϕ[0]+2*Xϕ[1]<=1.1)) | ((Xϕ[0]-0.7)**2+(Xϕ[1]-0.45)**2<=0.15**2)\n",
    "g_sharp = 2*g_sharp-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f4735f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.contourf(*Xϕ,g_sharp);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6989aabd",
   "metadata": {},
   "source": [
    "The metric cost parameter acts as a regularization parameter. \n",
    "- When it is small, the shape is almost untouched. \n",
    "- For medium values, the corners of the shape are smoothed. \n",
    "- For large values, the shape shrinks and finally diappears."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "93915428",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[12,5])\n",
    "for i,cost in enumerate([0.01,0.02,0.03,0.05,0.06,0.07,0.075,0.08]):\n",
    "    metric = Metrics.Isotropic(cost,vdim=2)\n",
    "    res = mincut(g_sharp,metric,dx,'gradc',verbosity=1)\n",
    "    plt.subplot(2,4,1+i)\n",
    "    plt.title(f\"Metric {cost=}\") \n",
    "    plt.axis('equal')\n",
    "    plt.contourf(*Xϕ,res['ϕ'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a413b8",
   "metadata": {},
   "source": [
    "Let us add some salt and pepper noise to this image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7d210cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "ρ = 0.7 # Proportion of pixels that are modified\n",
    "r = np.random.rand(*g_sharp.shape)\n",
    "g_noisy = np.where(r<ρ, np.sign(r-ρ/2), g_sharp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6b545527",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.contourf(*Xϕ,g_noisy);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eace474",
   "metadata": {},
   "source": [
    "**Chessboard artifacts.** \n",
    "We see large regions with obvious chessboard artifacts with the centered gradc scheme, as we expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5d8f20e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[12,5])\n",
    "for i,cost in enumerate([0.005,0.01,0.02,0.03]):\n",
    "    metric = Metrics.Isotropic(cost,vdim=2)\n",
    "    res = mincut(g_noisy,metric,dx,'gradc',verbosity=1)\n",
    "    plt.subplot(2,4,1+i)\n",
    "    plt.title(f\"Metric {cost=}\") \n",
    "    plt.axis('equal')\n",
    "    plt.contourf(*Xϕ,res['ϕ'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719ed5fb",
   "metadata": {},
   "source": [
    "The chessboard artifact problem is solved by using another gradient scheme.\n",
    "\n",
    "Again, the metric cost acts as a regularizer. \n",
    "- For small values, the noise remains.\n",
    "- For medium values, the shape is more or less recovered. \n",
    "- For large values, the penalization of the perimeter is too strong, and all data is lost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "540b9845",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[12,5])\n",
    "for i,cost in enumerate([0.005,0.01,0.02,0.03]):\n",
    "    metric = Metrics.Isotropic(cost,vdim=2)\n",
    "    res = mincut(g_noisy,metric,dx,'gradb',verbosity=1)\n",
    "    plt.subplot(2,4,1+i)\n",
    "    plt.title(f\"Metric {cost=}\") \n",
    "    plt.axis('equal')\n",
    "    plt.contourf(*Xϕ,res['ϕ'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "323f60bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[12,5])\n",
    "for i,cost in enumerate([0.005,0.01,0.02,0.03]):\n",
    "    metric = Metrics.Isotropic(cost,vdim=2)\n",
    "    res = mincut(g_noisy,metric,dx,'grad2',verbosity=1)\n",
    "    plt.subplot(2,4,1+i)\n",
    "    plt.title(f\"Metric {cost=}\") \n",
    "    plt.axis('equal')\n",
    "    plt.contourf(*Xϕ,res['ϕ'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "84ac084f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primal-dual solver completed 4 steps in 0.0009911060333251953 seconds\n",
      "GPU primal-dual solver completed 4 steps in 0.0009987354278564453 seconds\n",
      "ϕ max error :  3.399828929495641e-07\n",
      "η max error :  5.229067838452561e-09\n",
      "primal vals. GPU=1194.169189453125, CPU=1194.16909410418\n",
      "dual vals. GPU=-4424.55078125, CPU=-4424.550609025824\n"
     ]
    }
   ],
   "source": [
    "compare_cpu_gpu((g_noisy,Metrics.Isotropic(0.01,vdim=2),dx,'grad2'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37f2551",
   "metadata": {},
   "source": [
    "## 5. Choice of metric\n",
    "\n",
    "We continue with the previous example, and try to introduce some geometry to help the shape recovery."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c1245f",
   "metadata": {},
   "source": [
    "### 5.1 Filtering\n",
    "\n",
    "Il order to approximately locate the boundaries of the region, and their orientation, we rely on a classical construction based on convolutions and known as the structure tensor. \n",
    "$$\n",
    "    S := K_\\rho * (\\nabla u_\\sigma \\nabla u_\\sigma^\\top), \\quad u_\\sigma := K_\\sigma * u,\n",
    "$$\n",
    "where $\\rho$ denotes the feature scale, $\\sigma$ denotes the noise scale, and $K_\\sigma$ is a Gaussian convolution kernel of variance $\\sigma^2$.\n",
    "\n",
    "More sophisticated edge detectors, possibly based on learning, could be considered as well.\n",
    "\n",
    "\n",
    "*Note*: salt and peper noise actually has no scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "465a3dd4",
   "metadata": {
    "tags": [
     "ExportCode"
    ]
   },
   "outputs": [],
   "source": [
    "def structure_tensor(g,noise_scale=3.,feature_scale=4.): # scales in pixels\n",
    "    # Interpolate on the staggered grid, used for geometric data\n",
    "    g = (g[:-1,:-1]+g[1:,:-1]+g[:-1,1:]+g[1:,1:])/4.\n",
    "    \n",
    "    # Smoothed gradient\n",
    "    gx = gaussian_filter(g,noise_scale,[1,0])\n",
    "    gy = gaussian_filter(g,noise_scale,[0,1])\n",
    "    \n",
    "    # Structure tensor\n",
    "    gxx = gaussian_filter(gx**2,feature_scale)\n",
    "    gxy = gaussian_filter(gx*gy,feature_scale)\n",
    "    gyy = gaussian_filter(gy**2,feature_scale)\n",
    "    \n",
    "    # Similarly smoothed gradient (for Randers metrics)\n",
    "    gx = gaussian_filter(gx,feature_scale)\n",
    "    gy = gaussian_filter(gy,feature_scale)\n",
    "\n",
    "    return (gx,gy),(gxx,gxy,gyy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7b8812b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "(gx,gy),(gxx,gxy,gyy) = structure_tensor(g_noisy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352f89c8",
   "metadata": {},
   "source": [
    "### 5.2 Isotropic metric\n",
    "\n",
    "The trace of the structure tensor is obtained as \n",
    "$$\n",
    "    \\mathrm{Tr}(S) = K_\\rho * (\\|\\nabla u_\\sigma\\|^2).\n",
    "$$\n",
    "This defines a reasonable edge detector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "dfb4db41",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Trace of the structure tensor\")\n",
    "plt.contourf(*Xm,gxx+gyy)\n",
    "plt.axis('equal')\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c87230bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_detect = gxx+gyy\n",
    "edge_detect = np.maximum(np.max(edge_detect)/5.,edge_detect) # Avoid excessive contrast.\n",
    "edge_cost = np.min(edge_detect)/edge_detect # rescale in [0,1], inverse\n",
    "metric = Metrics.Isotropic(edge_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "cd1b382d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.contourf(*Xm,edge_cost) \n",
    "plt.title(\"Edge cost function\")\n",
    "plt.axis('equal')\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a976c5",
   "metadata": {},
   "source": [
    "Another way to visualize a metric is through Tissot's indicatrix, which is the collection of its unit balls. \n",
    "In this specific case, the unit balls are isotropic, but their radius vary accross the domain. Large unit balls mean that the cost of vectors is small. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "694ecc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Promotes large vectors only close to boundaries\")\n",
    "Plotting.Tissot(metric,Xm,subsampling=6,scale=-1.5)\n",
    "plt.contour(*Xϕ,g_sharp);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7368159d",
   "metadata": {},
   "source": [
    "The shapes are correctly recovered for a wider range of parameters than with the constant metric case.\n",
    "\n",
    "However, the boundaries remain quite irregular, and the tip of the triangle is severely cut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9306726a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[12,5])\n",
    "for i,cost in enumerate([0.005,0.01,0.02,0.03,0.04,0.05,0.06,0.07]):\n",
    "    res = mincut(g_noisy,metric.with_cost(cost),dx,'grad2',verbosity=1)\n",
    "    plt.subplot(2,4,1+i)\n",
    "    plt.title(f\"Metric {cost=}\") \n",
    "    plt.axis('equal')\n",
    "    plt.contourf(*Xϕ,res['ϕ'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd623dc6",
   "metadata": {},
   "source": [
    "### 5.3 Riemannian metric \n",
    "\n",
    "The structure tensor $S(x)$ defines a positive (semi-definite) matrix. If the image has some discontinuity at a point $x$, with normal $S(x)$, then by construction $\\<n(x),S(x) n(x)\\> \\gg 1$.\n",
    "\n",
    "We use here the inverse structure tensor to define the metric cost, which satisfies $\\<n(x),S(x)^{-1} n(x)\\> \\ll 1$, with some thresholding to ensure positive definiteness. More complex constructions could be considered, e.g. related to Weickert's structure enhancing and coherence enhancing tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f9fc2201",
   "metadata": {},
   "outputs": [],
   "source": [
    "gmax = np.max(gxx+gyy)\n",
    "S = np.array([[gxx,gxy],[gxy,gyy]]) / gmax # Normalized structure tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8d19026d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapping(λs): # How to map the eigenvalues of S\n",
    "    λ1,λ2 = λs # λ1 <= λ2\n",
    "    t = 0.05 # Threshold parameter\n",
    "    μ1 = t/np.maximum(t,λ1)\n",
    "    μ2 = t/np.maximum(t,λ2)\n",
    "    return μ1,μ2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "04bc702d",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = Metrics.Riemann.from_mapped_eigenvalues(S,mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d9f9f374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Strongest anisotropy : 4.32326156906436\n"
     ]
    }
   ],
   "source": [
    "print(f\"Strongest anisotropy : {np.max(metric.anisotropy())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "96734897",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Strongly promotes collinearity with the normal\")\n",
    "Plotting.Tissot(metric,Xm,subsampling=10,scale=-1.5)\n",
    "plt.contour(*Xϕ,g_sharp);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f261c489",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[12,3])\n",
    "plt.subplot(1,3,1); plt.axis('equal'); plt.title(\"Norm of horizontal vector\")\n",
    "plt.contourf(*Xm,metric.norm([1,0]))\n",
    "plt.colorbar()\n",
    "plt.subplot(1,3,2); plt.axis('equal'); plt.title(\"Norm of vertical vector\")\n",
    "plt.contourf(*Xm,metric.norm([0,1]))\n",
    "plt.subplot(1,3,3); plt.axis('equal'); plt.title(\"Norm of tilted vector\")\n",
    "plt.contourf(*Xm,metric.norm([1,2]));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b21080",
   "metadata": {},
   "source": [
    "We obtain reasonably sharp and rather smooth boundaries, which is an improvement. There are again a few checkerboard artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "78a276f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[12,5])\n",
    "for i,cost in enumerate([0.005,0.01,0.02,0.03,0.04,0.05,0.06,0.07]):\n",
    "    res = mincut(g_noisy,metric.with_cost(cost),dx,'gradb',verbosity=1)\n",
    "    plt.subplot(2,4,1+i)\n",
    "    plt.title(f\"Metric {cost=}\") \n",
    "    plt.axis('equal')\n",
    "    plt.contourf(*Xϕ,res['ϕ'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1fab47f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primal-dual solver completed 4 steps in 0.01401066780090332 seconds\n",
      "GPU primal-dual solver completed 4 steps in 0.0010132789611816406 seconds\n",
      "ϕ max error :  5.923938747320889e-07\n",
      "η max error :  1.3542890593687407e-08\n",
      "primal vals. GPU=7821.98974609375, CPU=7821.9905076792365\n",
      "dual vals. GPU=-5384.3505859375, CPU=-5384.350831265945\n"
     ]
    }
   ],
   "source": [
    "compare_cpu_gpu( (g_noisy,metric.with_cost(0.04),dx,'gradb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d18bfe7",
   "metadata": {},
   "source": [
    "### 5.4 Randers metric\n",
    "\n",
    "A Randers metric is defined as the sum of a Riemannian metric and of a drift term.\n",
    "$$\n",
    "    F_x(v) = \\|v\\|_{M(x)} + \\<\\omega(x),v\\>,\n",
    "$$\n",
    "subject to the compatibility condition \n",
    "$$\n",
    "    \\|\\omega\\|_{M^{-1}(x)} \\leq 1,\n",
    "$$\n",
    "which can equivalently be written as \n",
    "$$\n",
    "    \\begin{pmatrix}\n",
    "    M &\\omega\\\\\n",
    "    \\omega^\\top &1\n",
    "    \\end{pmatrix}\n",
    "    \\succeq 0.\n",
    "$$\n",
    "Randers metrics are asymmetric when the drift term is non-zero, in the sense that $F_x(v)\\neq F_x(-v)$ for some vectors $v$. \n",
    "\n",
    "The structure tensor $S$ naturally comes with a linear term $\\eta := K_\\rho * \\nabla u_\\sigma$, which satisfies the Randers compatibility condition.\n",
    "This can be used to enhance the segmentation, since the gradient $\\nabla \\vp$ is naturally oriented outwards of the segmented region. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "5f159038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that Randers compatibility condition is satisfied\n",
    "arr = np.array([[gxx,gxy,gx],\n",
    " [gxy,gyy,gy],\n",
    " [gx,gy,np.ones_like(gx)]])\n",
    "assert np.all(lp.det(arr)>=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "475e9c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "gmax = np.max(gxx+gyy)\n",
    "S = np.array([[gxx,gxy],[gxy,gyy]]) / gmax # Normalized structure tensor\n",
    "η = np.array([gx,gy])/np.sqrt(gmax) # Normalized gradient\n",
    "assert np.all(lp.det(S-lp.outer_self(η))>0)\n",
    "\n",
    "relax = 0.05*np.eye(2)[:,:,None,None] # Ensure positive definiteness\n",
    "metric = Metrics.Rander(S+relax,-η).dual()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae8f231",
   "metadata": {},
   "source": [
    "The vector $[1,2]$ is the outward normal to the long edge of the triangle. Below, we can see that as desired:\n",
    "- (left) the norm of this vector is small along the edge.\n",
    "- (right) the norm of the opposite vector is large along the edge.\n",
    "\n",
    "Thus the chosen metric promotes the correct direction of normal vectors to the shape, in addition to their correct orientation and location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c2df0d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Metric strongly promotes Positive collinearity with the normal\")\n",
    "Plotting.Tissot(metric,Xm,subsampling=10,scale=-1.8)\n",
    "plt.contour(*Xϕ,g_sharp);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "dbe3be42",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[12,5])\n",
    "for i,cost in enumerate([0.002,0.003,0.004,0.005,0.008,0.014,0.02,0.03]):\n",
    "    res = mincut_cpu(g_noisy,metric.with_cost(cost),dx,'gradb',verbosity=1)\n",
    "    plt.subplot(2,4,1+i)\n",
    "    plt.title(f\"Metric {cost=}\") \n",
    "    plt.axis('equal')\n",
    "    plt.contourf(*Xϕ,res['ϕ'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95db14e0",
   "metadata": {},
   "source": [
    "### 5.5 Randers metric via divergence\n",
    "\n",
    "Randers metrics can alternatively be incorporated in the mincut problem, via to the identity \n",
    "$$\n",
    "    \\int_\\Omega \\<\\omega,\\nabla \\vp\\> = -\\int_\\Omega \\diver(\\omega) \\vp,\n",
    "$$\n",
    "which holds with suitable boundary conditions. \n",
    "\n",
    "Thanks to this identity, we can incorporate the linear asymmetric term of the metric in the ground cost $g$, and rely on the Riemannian case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ffc05997",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_riemann = Metrics.Riemann(metric.m)\n",
    "ndiv_omega = res['ops']['KT'](metric.w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2c9b7a1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primal-dual solver completed 101 steps in 0.34699225425720215 seconds\n",
      "Primal-dual solver completed 81 steps in 0.2780003547668457 seconds\n"
     ]
    }
   ],
   "source": [
    "cost = 0.008\n",
    "res_riemann = mincut_cpu(g_noisy+cost*ndiv_omega,metric_riemann.with_cost(cost),dx,'gradb')\n",
    "res_randers = mincut_cpu(g_noisy,metric.with_cost(cost),dx,'gradb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d3ca3d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primal-dual solver completed 4 steps in 0.014003992080688477 seconds\n",
      "GPU primal-dual solver completed 4 steps in 0.001999378204345703 seconds\n",
      "ϕ max error :  6.976948836445729e-07\n",
      "η max error :  1.6478183650914247e-08\n",
      "primal vals. GPU=6100.3681640625, CPU=6100.368028953011\n",
      "dual vals. GPU=-5966.87646484375, CPU=-5966.876577774686\n"
     ]
    }
   ],
   "source": [
    "# The GPU implementation implicitly uses the divergence trick\n",
    "compare_cpu_gpu(data_cpu=(g_noisy+cost*ndiv_omega,metric_riemann.with_cost(cost),dx,'gradb'), \n",
    "                data_gpu=(g_noisy,metric.with_cost(cost),dx,'gradb') )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3396159",
   "metadata": {},
   "source": [
    "The numerical results are identical, up the accuracy of the numerical solution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "2b2f6525",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.mean(np.abs(res_riemann['ϕ']-res_randers['ϕ'])) < 1e-3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "853696fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[10,3.5])\n",
    "plt.subplot(1,2,1); plt.title(\"Riemann with divergence term\")\n",
    "plt.contourf(*Xϕ,res_riemann['x'])\n",
    "plt.subplot(1,2,2); plt.title(\"Randers\")\n",
    "plt.contourf(*Xϕ,res_riemann['x']);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab3e7cf",
   "metadata": {},
   "source": [
    "The number of iterations is similar (although not identical). Thus there is no obvious reason to choose one method over the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "30a26785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res_riemann['niter']=101, res_randers['niter']=81\n"
     ]
    }
   ],
   "source": [
    "print(f\"{res_riemann['niter']=}, {res_randers['niter']=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81bafb45",
   "metadata": {},
   "source": [
    "If one uses the Riemannian metric without the divergence term, then the result is obviously modified. In this specific case, all information is lost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "875deb54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU primal-dual solver completed 169 steps in 0.04000067710876465 seconds\n"
     ]
    }
   ],
   "source": [
    "res = mincut(g_noisy,metric_riemann.with_cost(cost),dx,'gradb')\n",
    "assert np.mean(res['ϕ']) > 0.999 # Segmentation fails, the level set function ϕ=1 almost identically\n",
    "#plt.title(\"Riemann without divergence term (fails)\")\n",
    "#plt.contourf(*Xϕ,res['ϕ']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e4653f7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.4254044135829993"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(metric_riemann.anisotropy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a31cacf",
   "metadata": {},
   "source": [
    "The divergence of the Randers field tends to be positive inside the regions of interest, and negative outside, similar to the ground cost function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "088069ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Divergence of the randers field')\n",
    "plt.contour(*Xϕ,g_sharp)\n",
    "plt.contourf(*Xϕ,ndiv_omega,levels=np.linspace(-100,100,20))\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d54d002",
   "metadata": {},
   "source": [
    "### 5.6 Asymmetric quadratic metrics\n",
    "\n",
    "Asymmetric Quadratic (AQ) metrics take the form \n",
    "$$\n",
    "    F_x(v) := \\sqrt{ \\|v\\|_{M(x)}^2 + \\<w(x),v\\>_+^2},\n",
    "$$\n",
    "where $\\alpha_+ := \\max \\{0,\\alpha\\}$, where $M$ must be positive definite pointwise, and $w$ is an arbitrary vector field.\n",
    "\n",
    "They are asymmetric, similar to Randers metrics, but with a different algebraic structure. The divergence trick presented in the last section does not apply, but thankfully the projection onto their unit ball is computable.\n",
    "\n",
    "Asymmetric quadratic metrics are more general than Riemannian metrics, and thanks to the divergence trick a Rander's like linear term can be added as well. I order to design the vector field $w$, we consider the smoothed gradient $\\eta$ of the image.\n",
    "\n",
    "**Asymmetric isotropic metrics**\n",
    "They are a special case of asymmetric quadratic metrics, with the form\n",
    "$$\n",
    "    F_x(v) := \\sqrt{ a^2\\|v\\|^2 + \\mathrm{sign}(a)\\<w(x),v\\>_+^2},\n",
    "$$\n",
    "subject to the compatibility condition $|a|>0$ and $a^2+\\mathrm{sign}(a)\\|w\\|^2 > 0$. \n",
    "The parameters may be a little bit more transparent, and numerical computations are slightly faster and less memory intensive. \n",
    "\n",
    "\n",
    "<!---\n",
    "However, we choose to illustrate them with an enhancement of the most basic penalization, defined by the isotropic metric with constant cost.\n",
    "--->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "21ef55c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"The vector field η points inwards the objects\")\n",
    "Plotting.quiver(*Xm,*η,subsampling=[4,4])\n",
    "plt.contour(*Xϕ,g_sharp);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7449bb5",
   "metadata": {},
   "source": [
    "**Isotropic metric with forbidden half space.**\n",
    "We choose here the metric\n",
    "$$\n",
    "    F_x(v) := \\sqrt{\\|v\\|^2 + \\epsilon^{-2} \\<\\eta(x),v\\>_+^2},\n",
    "$$\n",
    "which is obtained with $M := \\Id$ and $w := \\epsilon^{-1}\\eta$.\n",
    "With this choice one has $F_x(v) \\approx \\|v\\|$ if $\\<\\eta(x),v\\> \\leq 0$, and $F_x(v) \\gg \\|v\\|$ otherwise, in other words we recover the usual isotropic cost except that a half space of directions is forbidden.\n",
    "\n",
    "This approach is useful if $-\\eta$ is an imprecise approximation of the normal direction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "2b08dd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agd.Metrics.asym_iso import AsymIso\n",
    "from agd.Metrics import AsymQuad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "8efc24e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AsymQuad and AsymIso implementations are equivalent\n",
    "ϵ = 1e-1\n",
    "#metric = AsymQuad(np.eye(2),η/ϵ)\n",
    "metric = AsymIso(1.,η/ϵ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "aa61d0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Usual Euclidean norm, except where opposite to w\")\n",
    "Plotting.Tissot(metric,Xm,subsampling=10)\n",
    "plt.contour(*Xϕ,g_sharp);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b605e9c",
   "metadata": {},
   "source": [
    "The numerical results are reminiscent of the constant metric case, with quite irregular boundaries that are not well aligned with the direction on the real object boundaries. Better results were obtained with a variable cost isotropic metric, and even better with fully anisotropic metrics, but at the cost of a more complex metric design."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "f584c635",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[12,5])\n",
    "for i,cost in enumerate([0.005,0.01,0.02,0.03]):\n",
    "    res = mincut(g_noisy,metric.with_cost(cost),dx,'gradb',verbosity=1)\n",
    "    plt.subplot(2,4,1+i)\n",
    "    plt.title(f\"Metric {cost=}\") \n",
    "    plt.axis('equal')\n",
    "    plt.contourf(*Xϕ,res['ϕ'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "bbd809f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primal-dual solver completed 4 steps in 0.014029741287231445 seconds\n",
      "GPU primal-dual solver completed 4 steps in 0.0009987354278564453 seconds\n",
      "ϕ max error :  8.984395027944458e-07\n",
      "η max error :  1.4803999975106924e-08\n",
      "primal vals. GPU=3800.38671875, CPU=3800.3870264577376\n",
      "dual vals. GPU=-5099.0078125, CPU=-5099.008824382741\n"
     ]
    }
   ],
   "source": [
    "compare_cpu_gpu((g_noisy,metric.with_cost(0.02),dx,'gradb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89419f0",
   "metadata": {},
   "source": [
    "On the contrary, we can choose to promote vectors which are aligned with the approximate normal $-\\eta$, rather than penalize those aligned with $\\eta$. There are various ways to design the parameters for that purpose. Here we use norm duality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "7c3d905f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AsymQuad and AsymIso implementations are equivalent\n",
    "metric = Metrics.AsymQuad(np.eye(2),-η/ϵ).dual()\n",
    "#metric = AsymIso(1.,-η/ϵ).dual()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "82988b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Usual Euclidean norm, except where opposite to w\")\n",
    "Plotting.Tissot(metric,Xm,subsampling=10)\n",
    "plt.contour(*Xϕ,g_sharp);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "a74c0ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[12,5])\n",
    "for i,cost in enumerate([0.005,0.01,0.02,0.03,0.04,0.06,0.08,0.1]):\n",
    "    res = mincut(g_noisy,metric.with_cost(cost),dx,'gradb',verbosity=1)\n",
    "    plt.subplot(2,4,1+i)\n",
    "    plt.title(f\"Metric {cost=}\") \n",
    "    plt.axis('equal')\n",
    "    plt.contourf(*Xϕ,res['ϕ'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "44224263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primal-dual solver completed 4 steps in 0.013034820556640625 seconds\n",
      "GPU primal-dual solver completed 4 steps in 0.0009992122650146484 seconds\n",
      "ϕ max error :  8.984395027944458e-07\n",
      "η max error :  1.4803999975106924e-08\n",
      "primal vals. GPU=3800.38671875, CPU=3800.3870264577376\n",
      "dual vals. GPU=-5099.0078125, CPU=-5099.008824382741\n"
     ]
    }
   ],
   "source": [
    "compare_cpu_gpu((g_noisy,metric.with_cost(0.02),dx,'gradb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "60cd8c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code cell validates the AsymIso and AsymQuad implementations against one another\n",
    "np.random.seed(42)\n",
    "v = np.random.normal(size=(2,100))\n",
    "a = 2.; w = [1.5,0.5];\n",
    "iso0 = AsymIso(a,w); iso1 = iso0.dual()\n",
    "quad0 = AsymQuad(a**2*np.eye(2),w); quad1 = quad0.dual()\n",
    "for x,y in zip(iso0,iso1.dual()): assert np.allclose(x,y)\n",
    "\n",
    "for iso,quad in ((iso0,quad0),(iso1,quad1)):\n",
    "    for x,y in zip(quad,AsymQuad.from_cast(iso)): assert np.allclose(x,y)\n",
    "    assert np.allclose(iso.norm(v),quad.norm(v))\n",
    "    assert np.allclose(iso.gradient(v),quad.gradient(v))\n",
    "    assert iso.is_definite()\n",
    "    assert iso.vdim==quad.vdim\n",
    "    assert iso.shape==quad.shape\n",
    "    assert iso.cost_bound()<=quad0.cost_bound()\n",
    "    assert np.allclose(iso.anisotropy(),quad.anisotropy())\n",
    "    for x,y in zip(quad.rotate_by(np.pi/3),AsymQuad.from_cast(iso.rotate_by(np.pi/3))): assert np.allclose(x,y)\n",
    "    for x,y in zip(quad.with_cost(0.5),AsymQuad.from_cast(iso.with_cost(0.5))): assert np.allclose(x,y)\n",
    "    proj_iso,proj_quad = iso.make_proj_dual(),quad.make_proj_dual()\n",
    "    assert np.allclose(proj_iso(v),proj_quad(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3942fabf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}