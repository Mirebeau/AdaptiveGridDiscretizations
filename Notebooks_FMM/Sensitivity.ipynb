{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# The HFM library - A fast marching solver with adaptive stencils \n",
    "\n",
    "## Part : Algorithmic enhancements to the fast marching method\n",
    "## Chapter : Sensitivity analysis\n",
    "\n",
    "The present notebook is devoted to forward and reverse differentiation of the fast marching algorithm. We limit ourselves to isotropic fast marching, but more complex models are supported equally well, see the subsequent notebooks as well as the publication :\n",
    "\n",
    "* Jean-Marie Mirebeau and Johann Dreo, “Automatic differentiation of non-holonomic fast marching for computing most threatening trajectories under sensors surveillance,” Geometrical Science of Information conference, 2017. [link](https://hal.archives-ouvertes.fr/hal-01503607)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Limitations.** For the purposes of sensitivity analysis, we do avoid some enhancements of the fast marching method :\n",
    "* Second order enhancement of the numerical scheme. (Obtained by setting hfmInput['order']=2)\n",
    "* Time dependent speed functions.\n",
    "\n",
    "Indeed, if they were used, in the current implementation, then minor inaccuracies could arise in the the computed sensitivities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**Summary**](Summary.ipynb) of volume Fast Marching Methods, this series of notebooks.\n",
    "\n",
    "[**Main summary**](../Summary.ipynb) of the Adaptive Grid Discretizations \n",
    "\tbook of notebooks, including the other volumes.\n",
    "\n",
    "# Table of contents\n",
    "  * [1. Setting up the problem](#1.-Setting-up-the-problem)\n",
    "  * [2. Forward differentiation](#2.-Forward-differentiation)\n",
    "    * [2.1 Raw arguments](#2.1-Raw-arguments)\n",
    "    * [2.2 Using automatic differentiation](#2.2-Using-automatic-differentiation)\n",
    "  * [3. Reverse differentiation](#3.-Reverse-differentiation)\n",
    "    * [3.1 Raw arguments](#3.1-Raw-arguments)\n",
    "    * [3.2 Using automatic differentiation](#3.2-Using-automatic-differentiation)\n",
    "    * [3.3 Caching data](#3.3-Caching-data)\n",
    "  * [4. Gradient of the value function](#4.-Gradient-of-the-value-function)\n",
    "    * [4.1 Relation with the geodesic flow](#4.1-Relation-with-the-geodesic-flow)\n",
    "    * [4.2 Centered and upwind scheme](#4.2-Centered-and-upwind-scheme)\n",
    "    * [4.3 Forward differentiation](#4.3-Forward-differentiation)\n",
    "    * [4.4 Reverse differentiation](#4.4-Reverse-differentiation)\n",
    "  * [5. An optimization problem : finding the cost function which maximizes distance](#5.-An-optimization-problem-:-finding-the-cost-function-which-maximizes-distance)\n",
    "  * [6. Sensitivity at multiple points, possibly weighted](#6.-Sensitivity-at-multiple-points,-possibly-weighted)\n",
    "    * [6.1 Raw arguments](#6.1-Raw-arguments)\n",
    "    * [6.2 Using automatic differentiation](#6.2-Using-automatic-differentiation)\n",
    "\n",
    "\n",
    "\n",
    "This Python&reg; notebook is intended as documentation and testing for the [HamiltonFastMarching (HFM) library](https://github.com/mirebeau/HamiltonFastMarching), which also has interfaces to the Matlab&reg; and Mathematica&reg; languages. \n",
    "More information on the HFM library in the manuscript:\n",
    "* Jean-Marie Mirebeau, Jorg Portegies, \"Hamiltonian Fast Marching: A numerical solver for anisotropic and non-holonomic eikonal PDEs\", 2019 [(link)](https://hal.archives-ouvertes.fr/hal-01778322)\n",
    "\n",
    "Copyright Jean-Marie Mirebeau, University Paris-Sud, CNRS, University Paris-Saclay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Importing the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.path.insert(0,\"..\") # Allow import of agd from parent directory (useless if conda package installed)\n",
    "#from Miscellaneous import TocTools; print(TocTools.displayTOC('Sensitivity','FMM'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agd import Eikonal\n",
    "from agd import AutomaticDifferentiation as ad\n",
    "from agd import FiniteDifferences as fd\n",
    "from agd import Metrics\n",
    "from agd.Interpolation import UniformGridInterpolation\n",
    "from agd.Plotting import savefig; #savefig.dirName = 'Figures/Sensitivity'\n",
    "norm_infinity = ad.Optimization.norm_infinity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReloadPackages():\n",
    "    from Miscellaneous.rreload import rreload\n",
    "    global Eikonal,ad,fd,Metrics\n",
    "    Eikonal,ad,fd,Metrics = rreload([Eikonal,ad,fd,Metrics],rootdir=\"../..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setting up the problem\n",
    "\n",
    "We choose as a start to set up a same problem as considered in the [first notebook](http://nbviewer.jupyter.org/urls/rawgithub.com/Mirebeau/HFM_Python_Notebooks/master/A1_Isotropic.ipynb): a path planning problem involving an isotropic cost, on a two dimensional domain with obstacles.\n",
    "More precisely, we compute the unique viscosity solution $u: \\overline \\Omega \\to ]-\\infty,\\infty]$ to an eikonal equation\n",
    "\\begin{align*}\n",
    "\\forall x \\in \\Omega, \\|\\nabla u(x)\\| &= c(x), &\n",
    "\\forall x \\in \\partial \\Omega, u(x) &= \\sigma(x).\n",
    "\\end{align*}\n",
    "This PDE solution is known to solve the following optimal control problem\n",
    "\\begin{equation*}\n",
    "    u(x) = \\min_{\\substack{\\gamma(0) \\in \\partial \\Omega\\\\ \\gamma(1)=x}} \\sigma(\\gamma(0))+ \\int_0^1 c(\\gamma(t)), \\|\\gamma'(t)\\| \\,\\mathrm dt\n",
    "\\end{equation*}\n",
    "and the minimal paths $\\gamma:[0,1] \\to \\overline \\Omega$ can be efficiently backtracked."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Choice of the numerical scheme.** At this point, we need to mention that there exists two consistent discretizations of the eikonal equation, namely:\n",
    "$$\n",
    "    \\|\\nabla u(x)\\|^2 \n",
    "    \\approx h^{-2} \\sum_{1\\leq i \\leq d} \\max \\{0,u(x)-u(x-h e_i), u(x)-u(x+h e_i)\\}^2, \n",
    "$$\n",
    "and \n",
    "$$\n",
    "    \\|\\nabla u(x)\\|^2 \n",
    "    \\approx h^{-2} \\sum_{1\\leq i \\leq d} \\sum_{s \\in \\{-1,1\\}} \\max \\{0,u(x)-u(x- h s e_i)\\}^2,\n",
    "$$\n",
    "where $h$ denotes the gridscale, and $(e_1,\\cdots,e_d)$ is the canonical basis of $\\mathbb R^d$.\n",
    "\n",
    "* The first implementation (left), referred to as 'Isotropic2', is usually preferred since it is more accurate at points were the solution $u$ looses differentiability, e.g. near the cut locus (the points reached by two minimal geodesics). \n",
    "\n",
    "* The second implementation, referred to as 'IsotropicDiff2', has the advantage of being continuously differentiable w.r.t. the values of $u$, and is thus better behaved when it comes to automatic differentiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "hfmIn = Eikonal.dictIn()\n",
    "hfmIn['model']='Isotropic2' # Alternatively 'IsotropicDiff2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before turning to sensitivity analysis, let us recall how the software is run and its output displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the domain\n",
    "hfmIn.SetRect(sides=[[-1,1],[0,1]],gridScale=1./100.)\n",
    "\n",
    "# Set up the boundary conditions\n",
    "hfmIn['seeds']=[[-0.5,0.3],[0.5,0.8]] # Seed position\n",
    "hfmIn['seedValues']=[0.,0.5] # Boundary condition imposed at the seed. Defaults to $[0.,0.]$.\n",
    "\n",
    "# Define the speed function\n",
    "X,Y = hfmIn.Grid() # Create a coordinate system\n",
    "hfmIn['cost'] = np.exp(-0.5*(X**2+Y**2)) # Define the cost function\n",
    "\n",
    "# Insert the obstacles\n",
    "disk = (X-0.3)**2 + (Y-0.3)**2 <= 0.2**2\n",
    "barrier = np.logical_and(X==X[70,0], Y>=0.4)\n",
    "walls = np.logical_or(disk,barrier) \n",
    "hfmIn['walls']= walls\n",
    "\n",
    "# Request the desired outputs\n",
    "hfmIn['exportValues']=1. # Ask for the PDE solution\n",
    "hfmIn['tips'] = [[0.,0.6],[-0.9,0.5],[0.8,0.8]] # Ask for the geodesics from these three points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Field verbosity defaults to 1\n",
      "Field order defaults to 1\n",
      "Field seedRadius defaults to 0\n",
      "Fast marching solver completed in 0.005 s.\n",
      "Field geodesicSolver defaults to Discrete\n",
      "Field geodesicStep defaults to 0.25\n",
      "Field geodesicWeightThreshold defaults to 0.001\n",
      "Field geodesicVolumeBound defaults to 8.45\n",
      "Ended Geodesic Discrete Solver\n"
     ]
    }
   ],
   "source": [
    "hfmOut = hfmIn.Run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[6,3]); plt.title('Distance and minimal geodesics'); plt.axis('equal'); \n",
    "for geo in hfmOut['geodesics']: plt.plot(*geo) \n",
    "plt.contourf(X,Y,hfmOut['values'],cmap='Greys');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2. Forward differentiation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we differentiate the front arrival times $u : \\Omega \\to ]-\\infty,\\infty[$ w.r.t variations in the cost function $c : \\Omega \\to ]0,\\infty[$, and in the boundary conditions $\\sigma : \\Omega\\to ]-\\infty,\\infty[$.\n",
    "More precisely, denote by $u[c,\\sigma] : \\Omega \\to ]-\\infty,\\infty[$, the solution to the eikonal equation\n",
    "\\begin{align*}\n",
    "    \\forall x \\in \\Omega, \\| \\nabla u[c,\\sigma](x) \\| &= c(x) &\n",
    "    \\forall x \\in \\partial \\Omega, u[c,\\sigma](x) &= \\sigma(x).\n",
    "\\end{align*}\n",
    "\n",
    "Consider perturbation fields $\\xi : \\Omega \\to \\mathbb R$ and $\\zeta : \\partial \\Omega \\to \\mathbb R$. Forward differentiation allows to compute the first term  $\\nu : \\Omega \\to \\mathbb R$ in the Taylor expansion of the distance function, if it exists. In other words\n",
    "\\begin{equation*}\n",
    "\\mu(x) := \\frac d {d \\varepsilon} u[c+ \\varepsilon \\xi, \\sigma+ \\varepsilon \\zeta] (x)\n",
    "\\end{equation*}\n",
    "\n",
    "These features are implemented in the HFM library, and can be accessed in two ways:\n",
    "- raw arguments, where the perturbation fields $\\xi$, $\\zeta$ and $\\mu$ discussed above are directly manipulated (under different names).\n",
    "- interface with the AutomaticDifferentiation module of the agd library.\n",
    "\n",
    "The first approach is more explicit and possibly pedagogical, yet the second approach is expected to be more convenient in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Raw arguments\n",
    "\n",
    "We show how the perturbation to the cost function, and to the seed values, can be provided to the HFM library by raw explicit arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = hfmIn['cost']\n",
    "seedValues = hfmIn['seedValues']\n",
    "\n",
    "# Define the cost perturbation(s), above named xi. We actually define three perturbations, \n",
    "# xi_0 (on the right side of the domain only), xi_1 (on the left side only), and xi_2 (no perturbation)\n",
    "hfmIn['costVariation']= np.stack([(X>0.)*cost, (X<=0.)*cost, 0.*X],2) \n",
    "\n",
    "# Define the boundary condition perturbation(s), above named zeta. \n",
    "# Again, similarly define three perturbation, zeta_0 (no perturbation), zeta_1 (no perturbation), and zeta_2.\n",
    "hfmIn['seedValueVariation']= [[0,0],[0,0],seedValues]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Field verbosity defaults to 1\n",
      "Field order defaults to 1\n",
      "Field seedRadius defaults to 0\n",
      "Fast marching solver completed in 0.006 s.\n",
      "Field geodesicSolver defaults to Discrete\n",
      "Field geodesicStep defaults to 0.25\n",
      "Field geodesicWeightThreshold defaults to 0.001\n",
      "Field geodesicVolumeBound defaults to 8.45\n",
      "Ended Geodesic Discrete Solver\n"
     ]
    }
   ],
   "source": [
    "hfmOut = hfmIn.Run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell shows the effect $\\mu_0$ of the first perturbation $(\\xi_0,\\zeta_0)$. Since $\\xi_0$ is positive in the right side of the domain $\\{x>0\\}$, the perturbation increases the cost function there, hence also the value function. Therefore $\\mu_0>0$ where $\\{x>0\\}$, as can be observed numerically. On the other hand $\\zeta_0=0$, which means that boundary conditions, in other words the seed values, are untouched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[6,3]); plt.title(r'Value variation $\\mu$, cost perturbed on the right.'); plt.axis('equal'); \n",
    "# The field 'valueVariation' is denoted mu in the above mathematical expression.\n",
    "plt.contourf(X,Y,hfmOut['values'].gradient(0)) \n",
    "plt.axis('equal');plt.colorbar();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell shows the effect $\\mu_1$ of the second perturbation $(\\xi_1,\\zeta_1)$. Since $\\xi_1$ is positive on the left side of the domain $\\{x\\leq 0\\}$, the perturbation increases the cost function there, hence also the value function $u$. Therefore $\\mu_1>0$ where $\\{x \\leq 0\\}$, as can be observed numerically.\n",
    "\n",
    "However, one can note that $\\mu_1>0$ on part of the right side of the domain $\\{x>0\\}$ as well. That is because the corresponding minimal paths come from the left part of the domain, hence they see their cost increased by the perturbation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=[6,3]); plt.title(r'Value variation $\\mu$, cost perturbed on the left.'); plt.axis('equal'); \n",
    "plt.contourf(X,Y,hfmOut['values'].gradient(1))\n",
    "plt.axis('equal');plt.colorbar();\n",
    "savefig(fig,'ValueVariation_CostPerturbationLeft.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The third perturbation affects the boundary conditions only: $\\xi_2=0$ and $\\zeta_2 \\neq 0$. More precisely, the perturbation increases the boundary condition at the right seed $x_1$ only. Therefore, as can be observed numerically, hence the value function $u$ increases in the Voronoi region of $x_1$ only. In other words $\\mu_2>0$ at at all the points for which the backtracked geodesic leads to $x_1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=[6,3]); plt.title(r'Value variation $\\mu$, right seed value perturbed.'); plt.axis('equal'); \n",
    "plt.contourf(X,Y,hfmOut['values'].gradient(2))\n",
    "plt.axis('equal');plt.colorbar();\n",
    "savefig(fig,'ValueVariation_SeedValuePerturbationRight.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We end this section with a consistency test, based on a mathematical property that we describe below.\n",
    "\n",
    "It is worth noting that the arrival time function $u$ is $1$-homogeneous w.r.t. its parameters $c$ and $\\sigma$\n",
    "\\begin{equation*}\n",
    "u[\\lambda c,\\lambda \\sigma] = \\lambda u[c,\\sigma].\n",
    "\\end{equation*}\n",
    "This implies a differential identity, referred to as Euler identity for homogeneous functions:\n",
    "\\begin{equation*} \n",
    "    \\frac d {d\\lambda} u[\\lambda c,\\lambda \\sigma] = u[c,\\sigma].\n",
    "\\end{equation*}\n",
    "In the test case above, we have chosen the perturbations such that $\\xi_0+\\xi_1+\\xi_2 = c$ and $\\zeta_0+\\zeta_1+\\zeta_2 = \\sigma$. Thus denoting by $\\mu_0,\\mu_1,\\mu_2$ the corresponding value variations, Euler's identity implies that $\\mu_0+\\mu_1+\\mu_2 = u$, as can be observed numerically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = hfmOut['values'].value \n",
    "values[hfmIn['walls']]=0.; # Eliminate values inside walls, which equal Infinity\n",
    "\n",
    "# Check Euler's identity\n",
    "assert np.max(np.abs(hfmOut['values'].gradient().sum(axis=0)-values)) < 1e-14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Using automatic differentiation\n",
    "\n",
    "For convenience, we provide a (limited) interface between HFM library and the AutomaticDifferentiation (ad) module of the AdaptiveGridDiscretizations (agd) package.\n",
    "In that setting, some of the keys of the hfmInput dictionary can be provided as *first order dense AD* variables.\n",
    "Under the hood, a pre-processing and a post-processing step reformat the AD data as in the previous subsection.\n",
    "<!---In a pre-processing , the AD information will be separated and before the call to the HFM library, and then --->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following keys cannot be enhanced with AD information\n",
    "hfmIn_ad = Eikonal.dictIn({key:hfmIn[key] for key in \n",
    "            ['model','arrayOrdering','gridScale','dims','origin','seeds','walls','exportValues']})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us construct a first order symbolic perturbation with three independent components. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = ad.Dense.identity(shape=(3,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can define a cost and seed values which incorporate first order perturbations. Here we simply reproduce the previous ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "hfmIn_ad['cost'] = cost*(1+ delta[0]*(X>0.) + delta[1]*(X<=0.))\n",
    "hfmIn_ad['seedValues'] = seedValues * (1+ delta[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A smart run will pre-process and post-process the HFM data to correct formatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Field verbosity defaults to 1\n",
      "Field order defaults to 1\n",
      "Field seedRadius defaults to 0\n",
      "Fast marching solver completed in 0.005 s.\n"
     ]
    }
   ],
   "source": [
    "hfmOut_ad = hfmIn_ad.Run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The field of values output by the smart run incorporates first order AD information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "values_ad = hfmOut_ad['values']\n",
    "grad = values_ad.gradient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=[15,2.5])\n",
    "plt.subplot(1,3,1)\n",
    "plt.contourf(X,Y,grad[0])\n",
    "plt.subplot(1,3,2)\n",
    "plt.contourf(X,Y,grad[1])\n",
    "plt.subplot(1,3,3)\n",
    "plt.contourf(X,Y,grad[2]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Reverse differentiation\n",
    "\n",
    "Consider a cost function $c : \\Omega \\to ]0,\\infty[$ and some boundary values $\\sigma : \\Omega \\to ]-\\infty,\\infty]$. Reverse differentiation, for a given point $x\\in \\Omega$ provides two fields $\\rho = \\rho[x,c,\\sigma] : \\Omega \\to \\mathbb R$ and $\\pi = \\pi[x,c,\\sigma] : \\partial \\Omega \\to \\mathbb R$ such that \n",
    "\\begin{equation*}\n",
    "u[c+\\varepsilon \\xi,\\sigma+\\varepsilon \\zeta](x) = u[x,\\sigma](x)+ \\varepsilon \\Bigg(\\int_\\Omega \\rho \\xi + \\int_{\\partial \\Omega} \\pi \\zeta\\Bigg) + o(\\varepsilon).\n",
    "\\end{equation*}\n",
    "This equality holds, assuming differentiability, for any perturbation $\\xi$ of the cost function $c$, and any perturbation $\\zeta$ of the boundary condition $\\sigma$.\n",
    "The fields $\\rho$ and $\\pi$ express how much the front arrival time value $u[c,\\sigma]$ is sensitive to variations in these parameters.\n",
    "\n",
    "\n",
    "Similarly to the forward case, this functionality can be accessed in two ways:\n",
    "- by directly manipulating the perturbations fields $\\xi$, $\\zeta$, $\\rho$, and $\\pi$, suitably renamed.\n",
    "- using an interface with the AutomaticDifferentiation module of the AdaptiveGridDiscretizations library.\n",
    "\n",
    "Again, the first usage is more explicit and possibly pedagogical, but the second one is expected to be much more convenient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Raw arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "hfmIn['inspectSensitivity']=[ [-0.8,0.8], [0.575,0.1] ] # Ask for rho and pi related to these two points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Field verbosity defaults to 1\n",
      "Field order defaults to 1\n",
      "Field seedRadius defaults to 0\n",
      "Fast marching solver completed in 0.005 s.\n",
      "Field geodesicSolver defaults to Discrete\n",
      "Field geodesicStep defaults to 0.25\n",
      "Field geodesicWeightThreshold defaults to 0.001\n",
      "Field geodesicVolumeBound defaults to 8.45\n",
      "Ended Geodesic Discrete Solver\n"
     ]
    }
   ],
   "source": [
    "hfmOut = hfmIn.Run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can illustrated in the next cell, the sensitivity $\\rho = \\rho[x,c,\\sigma]$, of the value function $u(x)$ at a given point $x$ w.r.t. variations in the cost $c$, is (mostly) supported in the neighborhood of the minimal geodesic from $x$ to the nearest seed point. This property is actually at the foundation of one of our backtracking methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[6,3]); plt.title(r'Value sensitivity $\\rho$.'); plt.axis('equal'); \n",
    "plt.contourf(X,Y,hfmOut['costSensitivity_0']); # rho\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second point $x_2 = (0.575,0.1)$ for which we request the sensitivity, is located precisely on the cut locus. In other words, it is at equal distance of the two seeds (taking into account the boundary conditions). The corresponding sensitivity $\\rho$ is thus supported on the neighborhood of two geodesics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=[6,3]); plt.title(r'Value sensitivity $\\rho$. Two minimal geodesics.'); plt.axis('equal'); \n",
    "plt.contourf(X,Y,hfmOut['costSensitivity_1']);\n",
    "plt.axis('equal');plt.colorbar();\n",
    "savefig(fig,'ValueSensitivity_TwoPaths.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sensitivity of the value $u[x,c,\\sigma]$ w.r.t. the boundary condition $\\sigma$ is also returned, above denoted $\\pi : \\partial \\Omega \\to \\mathbb R$. The format is \n",
    "\\begin{equation*}\n",
    "[ [s_0^0,s_0^1,\\pi_0], [s_1^0,s_1^1,\\pi_1], ...].\n",
    "\\end{equation*}\n",
    "Here $s_0=(s_0^0,s_0^1)$ and $s_1=(s_1^0,s_1^1)$ are the seeds for which the corresponding sensitivity $\\pi_0$ and $\\pi_1$ is positive. \n",
    "If the sensitivity $u(x)$ was requested unless for a generic point $x$, then the list is of length one, and of the form $[ [s^0,s^1,1] ]$. In addition $s=(s^0,s^1)$ is the seed linked to $x$ by the minimal path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.495,  0.305,  1.   ]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hfmOut['seedSensitivity_0']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, if sensitivity is requested for a point on the cut-locus, for which there exists several minimal geodesics leading to distinct seeds, then $\\pi$ is supported on several seeds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.505     ,  0.805     ,  0.37180042],\n",
       "       [-0.495     ,  0.305     ,  0.62819958]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hfmOut['seedSensitivity_1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A slight annoyance of the above output is that the original seeds are not returned. Instead, the returned positions correspond to the nearest point on the discretization grid. Some minor postprocessing, for instance by converting these points to multi-indices, is thus required to establish the matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed indices :  [[ 50  29]\n",
      " [150  80]]\n",
      "Sensitivity returned as positions :  [[150  80]\n",
      " [ 50  30]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Seed indices : \",hfmIn.IndexFromPoint(hfmIn['seeds'])[0])\n",
    "print(\"Sensitivity returned as positions : \",hfmIn.IndexFromPoint(hfmOut['seedSensitivity_1'][:,0:2])[0])\n",
    "# Note that the order of the seeds may not be preserved. Also, roundoff direction may change for points exactly in the middle of a cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We conclude this section with a consistency check. More precisely, we ensure that our two automatic differentiation methods yield consistent results.\n",
    "Both can be used to compute the scalar $\\mu(x)$ appearing in the following Taylor expansion\n",
    "\\begin{equation*}\n",
    "    u[c+\\varepsilon \\xi, \\sigma+\\varepsilon \\zeta] (x) = u[c,\\sigma](x)+\\varepsilon \\mu(x) + o(\\varepsilon),\n",
    "\\end{equation*}\n",
    "where the point $x$ and the perturbations $\\xi$ and $\\zeta$ are given.\n",
    "The two differentiation methods use distinct inputs, as follows.\n",
    "* Forward differentiation. *Input*: $\\xi$ (costVariation), $\\zeta$ (seedValueVariation). *Output*: $\\mu$ (valueVariation).\n",
    "* Reverse differentiation. *Input*: $x$ (inspectSensitivity). *Output*: $\\rho$ (costSensitivity) and $\\pi$ (seedSensitivity).\n",
    "\n",
    "The inputs $x,\\zeta,\\xi$ and outputs $\\mu, \\rho, \\pi$, are mathematically tied by the the following identity, which we verify numerically:\n",
    "\\begin{equation*}\n",
    "\\mu(x) = \\int_\\Omega \\rho \\xi + \\int_{\\partial \\Omega} \\pi \\zeta.\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "valueVariation = hfmOut['values'].gradient()\n",
    "index,_ = hfmIn.IndexFromPoint(hfmIn['inspectSensitivity'][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first instance of forward differentiation, boundary conditions were not perturbed ($\\zeta=0$). Hence we expect, and numerically check, that $\\mu(x) = \\int_\\Omega \\rho \\xi$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mu_x = valueVariation[0,index[0],index[1]] # Evaluates mu(x)\n",
    "int_dom = (hfmIn['costVariation'][:,:,0]*hfmOut['costSensitivity_1']).sum() #  Evaluates int_Omega rho*xi \n",
    "\n",
    "assert abs(mu_x-int_dom) < 1e-14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last instance of forward differentiation, the cost function was not perturbed ($\\xi=0$). Hence we expect, and numerically check, that  $\\mu(x) = \\int_{\\partial\\Omega} \\pi \\zeta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_x = valueVariation[2,index[0],index[1]] # Evaluates mu(x)\n",
    "int_bd = np.dot(hfmOut['seedSensitivity_1'][(1,0),2] , hfmIn['seedValueVariation'][2,:]) # int_Boundary pi*zeta\n",
    "\n",
    "assert abs(mu_x-int_bd) < 1e-14 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Using automatic differentiation\n",
    "\n",
    "An interface is provided with the *reverse first order* automatic differentiation module of the agd library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These variables cannot be enhanced with automatic differentiation\n",
    "hfmIn_rev = Eikonal.dictIn({key:hfmIn[key] for key in \n",
    "            ['model','arrayOrdering','gridScale','dims','origin','seeds','walls']})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to register the variables w.r.t which sensitivity will be requested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev,(cost_rev,seedValues_rev) = ad.Reverse.empty(inputs=(cost,seedValues),input_iterables=(tuple,Eikonal.dictIn,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note on the `input_iterables` field.** By default, the ad.Reverse automatic differentiation module only looks for AD information inside tuples. This behavior is here modified, through the `input_iterables` field, since calls to the HFM library involve a dictionary of inputs. See the notebook [Reverse](../Notebooks_Algo/Reverse.ipynb) for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "hfmIn_rev['cost'] = cost_rev\n",
    "hfmIn_rev['seedValues'] = seedValues_rev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to be taken into account, the solution values will be returned separately from the rest of the HFM outputs. \n",
    "This is achieved with the `extractValues` key.\n",
    "\n",
    "<!---\n",
    "*Note on the `output_iterables` field.* Another approach would be to instruct the reverse AD method to look into output dictionaries. However, doing so would mean tagging all the output variables with AD information (including CPU times, etc)--->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "hfmIn_rev['extractValues']=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Field verbosity defaults to 1\n",
      "Field order defaults to 1\n",
      "Field seedRadius defaults to 0\n",
      "Fast marching solver completed in 0.005 s.\n"
     ]
    }
   ],
   "source": [
    "hfmOut,values_rev = rev.apply(Eikonal.dictIn.Run,hfmIn_rev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us define the objective function, which has to be a scalar function, in terms of the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = np.array((X,Y))\n",
    "values_rev_interp = UniformGridInterpolation(grid,values_rev)\n",
    "\n",
    "points = np.array([[-0.8,0.8],[0.575,0.1]]).T\n",
    "val = values_rev_interp(points)\n",
    "\n",
    "objective = 2*val[0]**2+val[1]**2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Field verbosity defaults to 1\n",
      "Field order defaults to 1\n",
      "Field seedRadius defaults to 0\n",
      "Fast marching solver completed in 0.005 s.\n"
     ]
    }
   ],
   "source": [
    "grad = rev.gradient(objective)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_sensitivity,seed_sensitivity = rev.to_inputshapes(grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Gradient of the objective function')\n",
    "plt.contourf(*grid,cost_sensitivity);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.29170665, 0.42199756])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed_sensitivity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Caching data\n",
    "\n",
    "Reverse automatic differentiation is a two pass procedure:\n",
    "- The function *value* is evaluated in an initial *forward* pass.\n",
    "- The function *jacobian*, transposed, is applied to a suitable co-vector in a final *backward* pass.\n",
    "\n",
    "If suitable data is cached in the first pass, then the jacobian evaluation in the second pass can avoid the full recomputation of the function value.\n",
    "Compare the results above and below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "hfmIn_rev = Eikonal.dictIn({key:hfmIn[key] for key in \n",
    "            ['model','arrayOrdering','gridScale','dims','origin','seeds','walls','seedValues']})\n",
    "hfmIn_rev['extractValues']=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a change, in contrast with the previous paragraph, we do not register the seed values for automatic differentiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev,cost_rev = ad.Reverse.empty(inputs=cost,input_iterables=(Eikonal.dictIn,))\n",
    "hfmIn_rev['cost'] = cost_rev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requesting cacheable data\n",
      "Field verbosity defaults to 1\n",
      "Field order defaults to 1\n",
      "Field seedRadius defaults to 0\n",
      "Fast marching solver completed in 0.004 s.\n",
      "Filling cache data\n"
     ]
    }
   ],
   "source": [
    "cache = Eikonal.Cache()\n",
    "hfmOut,values_rev = rev.apply(Eikonal.dictIn.Run,hfmIn_rev,cache=cache)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some output data is cached so as to bypass most the fast marching solver in future computations on this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['values', 'activeNeighs'])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cache.contents.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next define the objective function, and compute its sensitivity w.r.t. the cost function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "values_rev_interp = UniformGridInterpolation(grid,values_rev)\n",
    "val = values_rev_interp(points)\n",
    "objective = 2*val[0]**2+val[1]**2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Providing cached data\n",
      "Field verbosity defaults to 1\n",
      "Field order defaults to 1\n",
      "Field seedRadius defaults to 0\n",
      "Bypassing fast marching solver based on cached data.\n"
     ]
    }
   ],
   "source": [
    "cost_sensitivity_cache, = rev.to_inputshapes(rev.gradient(objective))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned in the last line, and in contrast with the previous subsection, the fast marching solver was bypassed entirely in the reverse pass thanks to the cached data. Yet the result is identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert norm_infinity(cost_sensitivity-cost_sensitivity_cache,axis=None)==0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Gradient of the value function\n",
    "\n",
    "We discuss the computation of the gradient of the value function, using a centered or upwind scheme.\n",
    "Then we compute its first order perturbation using forward or reverse automatic differentiation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Relation with the geodesic flow\n",
    "\n",
    "Let $u : \\Omega \\to R$ be the value function, also referred to as the distance map, which is approximated by the fast marching algorithm. One would like to estimate the gradient $\\nabla u$, which satisfies the eikonal equation\n",
    "$$\n",
    "    \\| \\nabla u(x) \\| = c(x)\n",
    "$$\n",
    "at any $x \\in \\Omega$, where $c(x)$ is the cost function. A closely related quantity is the geodesic flow, which is defined by \n",
    "$$\n",
    "    V(x) := \\frac 1 {c(x)^2} \\nabla u(x),\n",
    "$$\n",
    "and for which the HFM library provides an upwind approximation. \n",
    "\n",
    "**Note on anisotropic metric.** When metric is Riemannian or Finslerian, the gradient and the geodesic flow are not anymore proportionnal, but are related through norm duality, see the notebook [Sensitivity in Semi-Lagrangian schemes](SensitivitySL.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "hfmIn = Eikonal.dictIn({\n",
    "    'model':'Isotropic2',\n",
    "    'arrayOrdering':'RowMajor',\n",
    "    'seeds':[[-0.5,0.3],[0.5,0.8]],\n",
    "    'seedValues':[0.,0.5],\n",
    "    'tips':[[0.,0.6],[-0.9,0.5],[0.8,0.8]],\n",
    "    'walls':np.logical_or(disk,barrier),\n",
    "    \n",
    "    'exportValues':True,\n",
    "    'exportGeodesicFlow':True,\n",
    "})\n",
    "\n",
    "hfmIn.SetRect(sides=[[-1,1],[0,1]],gridScale=1./100.)\n",
    "X,Y = hfmIn.Grid()\n",
    "hfmIn['cost']=np.exp(-0.5*(X**2+Y**2))\n",
    "h = hfmIn['gridScale']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Field verbosity defaults to 1\n",
      "Field order defaults to 1\n",
      "Field seedRadius defaults to 0\n",
      "Fast marching solver completed in 0.005 s.\n",
      "Field geodesicSolver defaults to Discrete\n",
      "Field geodesicStep defaults to 0.25\n",
      "Field geodesicWeightThreshold defaults to 0.001\n",
      "Field geodesicVolumeBound defaults to 8.45\n",
      "Ended Geodesic Discrete Solver\n"
     ]
    }
   ],
   "source": [
    "hfmOut = hfmIn.Run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient is easily recovered from the geodesic flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_hfm = hfmOut['flow']*hfmIn['cost']**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Gradient as computed by the HFM library\")\n",
    "s=5; plt.quiver(*grid[:,::s,::s],*grad_hfm[:,::s,::s]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Centered and upwind scheme\n",
    "\n",
    "We may also directly recompute the gradient using finite differences. Centered finite differences are usually more precise, whereas upwind finite differences are usually more stable.\n",
    "\n",
    "*Note on Python warnings.** Those come from the computation of the gradient inside obtacles, where the the value function is $+\\infty$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jmmir\\Documents\\GitHub\\AdaptiveGridDiscretizations\\Notebooks_FMM\\..\\agd\\FiniteDifferences.py:211: RuntimeWarning: invalid value encountered in add\n",
      "  return sum(TakeAtOffset(u,mult*ad.asarray(offset),**kwargs)*weight\n"
     ]
    }
   ],
   "source": [
    "grad_centered = fd.DiffGradient(hfmOut['values'],gridScale=h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_upwind(u,h):\n",
    "    \"\"\"(Quasi-)Upwind gradient of an array u at gridscale h\"\"\"\n",
    "    offsets = np.eye(u.ndim).astype(int)\n",
    "    dup = fd.DiffUpwind(u, offsets, h, padding=np.inf)\n",
    "    dum = fd.DiffUpwind(u,-offsets, h, padding=np.inf)\n",
    "    return np.where(dup<dum,dup,-dum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_upwind = gradient_upwind(hfmOut['values'],h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The upwind and hfm gradient are actually identical up to machine precision, except inside obstacles and along lines where the scheme \"degenerates\" in the sense that there is only one active neighbor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pixel ratio where hfm and 'upwind' gradient are equal : 0.97345\n"
     ]
    }
   ],
   "source": [
    "grad_same = np.linalg.norm(grad_upwind-grad_hfm,axis=0)<1e-12\n",
    "ratio = np.logical_or(grad_same,hfmIn['walls']).sum()/grad_same.size\n",
    "assert ratio>0.95\n",
    "print(f\"Pixel ratio where hfm and 'upwind' gradient are equal : {ratio}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Distinct hfm and upwind gradient\")\n",
    "plt.contourf(*grid,grad_same);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, not much distinguishes the centered and the upwind gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,3))\n",
    "plt.subplot(1,2,1)\n",
    "plt.title(\"Upwind gradient\")\n",
    "s=5; plt.quiver(*grid[:,::s,::s],*grad_upwind[:,::s,::s]);\n",
    "plt.subplot(1,2,2)\n",
    "plt.title(\"Centered gradient\")\n",
    "s=5; plt.quiver(*grid[:,::s,::s],*grad_centered[:,::s,::s]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Forward differentiation\n",
    "\n",
    "Finite differences are compatible with automatic differentiation. For illustration, we compute the first order perturbation of the value function gradient w.r.t. the perturbations of the cost function and seed values previously considered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Field verbosity defaults to 1\n",
      "Field order defaults to 1\n",
      "Field seedRadius defaults to 0\n",
      "Fast marching solver completed in 0.004 s.\n"
     ]
    }
   ],
   "source": [
    "hfmOut = hfmIn_ad.Run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jmmir\\Documents\\GitHub\\AdaptiveGridDiscretizations\\Notebooks_FMM\\..\\agd\\AutomaticDifferentiation\\Dense.py:54: RuntimeWarning: invalid value encountered in add\n",
      "  return self.new(self.value+other.value, _add_coef(self.coef,other.coef))\n"
     ]
    }
   ],
   "source": [
    "grad_upwind_ad = gradient_upwind(hfmOut['values'],h)\n",
    "grad_centered_ad = fd.DiffGradient(hfmOut['values'],gridScale=h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Automatic differentiation has no impact on the zero-th order term, where it is defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.all(np.logical_or(grad_upwind_ad.value==grad_upwind,np.isnan(grad_upwind)))\n",
    "assert np.all(np.logical_or(grad_centered_ad.value==grad_centered,np.isnan(grad_centered)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The improved stability of the upwind gradient, w.r.t.\\ the centered gradient, is slightly more visible when differentiation is involved. One sees that the centered gradient features more long spurious arrow, originating from the vicity of obtsacles or of the boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "perturbation_index = 0\n",
    "plt.figure(figsize=[10,3])\n",
    "plt.subplot(1,2,1)\n",
    "plt.title(\"Upwind gradient perturbation\")\n",
    "plt.quiver(*grid[:,::s,::s],*grad_upwind_ad.gradient(perturbation_index)[:,::s,::s]);\n",
    "plt.subplot(1,2,2)\n",
    "plt.title(\"Centered gradient perturbation (omiting boundary layer)\")\n",
    "plt.quiver(*grid[:,::s,::s],*grad_centered_ad.gradient(perturbation_index)[:,1:-1:s,1:-1:s]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Reverse differentiation\n",
    "\n",
    "We illustrate reverse differentiation of the value function gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev,cost_rev = ad.Reverse.empty(inputs=cost,input_iterables=(Eikonal.dictIn,))\n",
    "hfmIn_rev['cost'] = cost_rev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requesting cacheable data\n",
      "Field verbosity defaults to 1\n",
      "Field order defaults to 1\n",
      "Field seedRadius defaults to 0\n",
      "Fast marching solver completed in 0.005 s.\n",
      "Filling cache data\n"
     ]
    }
   ],
   "source": [
    "hfmOut,values_rev = rev.apply(Eikonal.dictIn.Run,hfmIn_rev,cache=Eikonal.Cache())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jmmir\\Documents\\GitHub\\AdaptiveGridDiscretizations\\Notebooks_FMM\\..\\agd\\AutomaticDifferentiation\\Sparse.py:73: RuntimeWarning: invalid value encountered in add\n",
      "  value = self.value+other.value\n"
     ]
    }
   ],
   "source": [
    "grad_upwind_rev = gradient_upwind(values_rev,h)\n",
    "grad_centered_rev = fd.DiffGradient(values_rev,gridScale=h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(grad):\n",
    "    interp = UniformGridInterpolation(grid,grad)\n",
    "    p0,p1 = points.T \n",
    "    # Return horizontal component at p0, vertical component at p1\n",
    "    return interp(p0)[0] +interp(p1)[1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Providing cached data\n",
      "Field verbosity defaults to 1\n",
      "Field order defaults to 1\n",
      "Field seedRadius defaults to 0\n",
      "Bypassing fast marching solver based on cached data.\n",
      "---- Finished upwind, turning to centered ----\n",
      "Providing cached data\n",
      "Field verbosity defaults to 1\n",
      "Field order defaults to 1\n",
      "Field seedRadius defaults to 0\n",
      "Bypassing fast marching solver based on cached data.\n"
     ]
    }
   ],
   "source": [
    "grad_upwind_cost, = rev.to_inputshapes(rev.gradient(objective(grad_upwind_rev)))\n",
    "print(\"---- Finished upwind, turning to centered ----\")\n",
    "grad_centered_cost, = rev.to_inputshapes(rev.gradient(objective(grad_centered_rev)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "levels = [0.0001,0.001,0.01]\n",
    "plt.figure(figsize=(10,3))\n",
    "plt.subplot(1,2,1)\n",
    "plt.title(\"Sensitivity of upwind gradient\")\n",
    "plt.contourf(*grid,grad_upwind_cost,levels=levels)\n",
    "plt.subplot(1,2,2)\n",
    "plt.title(\"Sensitivity of centered gradient\")\n",
    "plt.contourf(*grid,grad_centered_cost,levels=levels);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation by comparison with forward gradient. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_diff = (grad_upwind_cost * hfmIn_ad['cost']).sum().gradient() - objective(grad_upwind_ad).gradient()\n",
    "assert norm_infinity(grad_diff[:2])<1e-13\n",
    "# Third component ignored because related with seedValueVariation, not considered in reverse more here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. An optimization problem : finding the cost function which maximizes distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We illustrate automatic differentiation solving an optimization problem posed on cost functions $c$. The objective is to maximize the distance from a point $x_*$ to the boundary, minus a integral penalty on the cost. Formally, the problem reads as follows\n",
    "\\begin{align*}\n",
    "    \\max_{c : \\Omega \\to [\\alpha,\\beta]} \\, u[c](x_*) - \\gamma \\int_\\Omega c.\n",
    "\\end{align*}\n",
    "The bounds $\\alpha,\\beta > 0$ imposed on the cost function, and the penalization factor $\\gamma >0$ on the cost function integral, are given parameters, as well as the target point $x_*$.\n",
    "\n",
    "Experiments of similar nature are presented in :\n",
    "* F. Benmansour, G. Carlier, G. Peyré, and F. Santambrogio, “Derivatives with respect to metrics and applications: subgradient marching algorithm,” Numerische Mathematik, vol. 116, no. 3, pp. 357–381, May 2010.\n",
    "* J.-M. Mirebeau and J. Dreo, “Automatic differentiation of non-holonomic fast marching for computing most threatening trajectories under sensors surveillance,” presented at the Geometrical Science of Information, 2017."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "optIn = Eikonal.dictIn()\n",
    "optIn['model']='IsotropicDiff2' # Alternatively, 'Isotropic2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fast marching parameters, copied from earlier problem, with different seed\n",
    "for key in ['arrayOrdering','dims','gridScale','origin','walls']:\n",
    "    optIn[key]=hfmIn[key]\n",
    "\n",
    "# Problem parameters\n",
    "alpha = 0.1; beta=1.; gamma=1.5;\n",
    "optIn['seed']=[-0.7,0.7] # Seed position\n",
    "\n",
    "# For better stability, we slightly spread the target x_* on the four neighbor points\n",
    "targetIndices = [(180,40),(179,40),(181,40),(180,39),(180,41)];\n",
    "targetWeights = [0.5,0.125,0.125,0.125,0.125]\n",
    "#Unspread target, commented below, yields less stable computations.\n",
    "#targetIndices = [(40,180)]; targetWeights = [1]; \n",
    "\n",
    "# Silent the execution, since HFM will be called many times\n",
    "optIn['verbosity']=0\n",
    "\n",
    "# Utilities\n",
    "targetPoints = optIn.PointFromIndex(targetIndices)\n",
    "allOnes = np.ones(optIn.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We prepare two functions returning the objective function and its jacobian. \n",
    "\n",
    "This particular test application works better with the differentiable scheme 'IsotropicDiff2', although the usual discretization 'Isotropic2' is still usable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(cost,sign=1):\n",
    "    optIn['cost']=cost.reshape(optIn.shape)\n",
    "    optIn['exportValues']=1\n",
    "    optIn.pop('inspectSensitivity',None)\n",
    "    optOut = optIn.Run()\n",
    "    value = sum([weight*optOut['values'][index] for index,weight in zip(targetIndices,targetWeights)])\n",
    "    return sign*(value - gamma*cost.sum()*optIn['gridScale']**2)\n",
    "\n",
    "def func_deriv(cost,sign=1):\n",
    "    optIn['cost']=cost.reshape(optIn.shape)\n",
    "    optIn['exportValues']=0\n",
    "    \n",
    "    # Request sensitivity for a weighted sum of values\n",
    "    optIn['inspectSensitivity'] = targetPoints \n",
    "    optIn['inspectSensitivityWeights'] = targetWeights\n",
    "    optIn['inspectSensitivityLengths'] = [len(targetWeights)]\n",
    "    \n",
    "    optOut = optIn.Run()\n",
    "    return sign*(optOut['costSensitivity_0'].reshape(-1) - gamma*allOnes*optIn['gridScale']**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 2min 15s\n",
      "Wall time: 19.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Warning : takes up to a minute.\n",
    "res = scipy.optimize.\\\n",
    "minimize(func,\n",
    "         allOnes*(alpha+beta)/2., # Initial guess\n",
    "         bounds=np.array((alpha*allOnes,beta*allOnes)).transpose(), # alpha <= c <= beta\n",
    "         jac=func_deriv,\n",
    "         args=(-1.), # Minimize instead of maximize\n",
    "         method='L-BFGS-B',options={'gtol':1e-4,'maxiter':300})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimal strategy uses a large cost around the seed, the target, around the boundary of obstacles, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=[6,3]); plt.title('Optimal cost function.'); plt.axis('equal'); \n",
    "plt.contourf(X,Y,res.x.reshape(hfmIn.shape));\n",
    "savefig(fig,'OptimalCost.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the optimal strategy, there is a continuum of optimal curves, spread all over the domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=[6,3]); plt.title('Sensitivity at the optimal cost.'); plt.axis('equal'); \n",
    "plt.contourf(X,Y,func_deriv(res.x).reshape(optIn.shape));\n",
    "savefig(fig,'OptimalSensitivity.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "optIn['tips']=optIn['inspectSensitivity']\n",
    "optIn['exportValues']=1\n",
    "optOut = optIn.Run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=[6,3]); plt.title('Distance and minimal geodesics, at the optimal cost'); plt.axis('equal'); \n",
    "for geo in optOut['geodesics']: plt.plot(*geo) \n",
    "plt.contourf(X,Y,optOut['values'],cmap='Greys');\n",
    "savefig(fig,'OptimalPathCost.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Sensitivity at multiple points, possibly weighted\n",
    "\n",
    "The HFM library lets you compute the sensitivity at multiple points, possibly weighted. \n",
    "For instance, assume $u : \\Omega \\to R$ is a distance map computed using the fast marching algorithm. \n",
    "Let also $x_0,x_1,x_2 \\in \\Omega$ and $\\alpha_0,\\alpha_1,\\alpha_2 \\in R$. We show how to compute the sensitivity of the the vector\n",
    "$$\n",
    "    (\\alpha_0 u(x_0)+\\alpha_1 u(x_1),\\ \\alpha_2 u(x_2))\n",
    "$$\n",
    "w.r.t. variations of the cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Raw arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the problem\n",
    "hfmIn = Eikonal.dictIn({\n",
    "    'model':'Isotropic2',\n",
    "    'cost':1.,\n",
    "    'seed':[0.,0.],\n",
    "    'exportValues':1.,\n",
    "})\n",
    "hfmIn.SetRect(sides=[[-1.,1.],[-1.,1.]],dimx=100)\n",
    "X = hfmIn.Grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to get the sensitivity\n",
    "x0,x1,x2 = [0.1,-0.4],[0.5,0.8],[-0.5,-0.7] \n",
    "alpha0,alpha1,alpha2 = 2.,3.,4.5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format this data for the input\n",
    "hfmIn['inspectSensitivity'] = [x0,x1,x2] \n",
    "hfmIn['inspectSensitivityWeights'] = [alpha0,alpha1,alpha2]\n",
    "\n",
    "# How to group the points (here the first two are summed together, and the last one is alone)\n",
    "hfmIn['inspectSensitivityLengths'] = [2.,1.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Field verbosity defaults to 1\n",
      "Field order defaults to 1\n",
      "Field seedRadius defaults to 0\n",
      "Fast marching solver completed in 0.002363 s.\n"
     ]
    }
   ],
   "source": [
    "hfmOut = hfmIn.Run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(r\"Cost sensitivity for the weighted sum $\\alpha_0 u(x_0)+\\alpha_1 u(x_1)$\")\n",
    "plt.contourf(*X,hfmOut['costSensitivity_0']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(r\"Cost sensitivity for $\\alpha_2 u(x_2)$\")\n",
    "plt.contourf(*X,hfmOut['costSensitivity_1']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Using automatic differentiation \n",
    "\n",
    "The reverse AD library currently only supports scalar valued objectives. In the case of a non-scalar output, multiple calls to the gradient are required. The cost remains reasonable provided the adequate data is cached in the forward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "hfmIn_ad = Eikonal.dictIn({key:hfmIn[key] for key in ['model','seeds','arrayOrdering','dims','origin','gridScale']})\n",
    "hfmIn_ad['extractValues']=True\n",
    "X = hfmIn_ad.Grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = np.ones( hfmIn_ad.shape ) \n",
    "\n",
    "rev, cost_ad = ad.Reverse.empty(inputs = cost, input_iterables = (Eikonal.dictIn,))\n",
    "hfmIn_ad['cost'] = cost_ad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We provide an un-named cache variable, which will be saved by the Reverse AD structure, like the other input arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requesting cacheable data\n",
      "Field verbosity defaults to 1\n",
      "Field order defaults to 1\n",
      "Field seedRadius defaults to 0\n",
      "Fast marching solver completed in 0.002383 s.\n",
      "Filling cache data\n"
     ]
    }
   ],
   "source": [
    "hfmOut_ad,values_ad = rev.apply(Eikonal.dictIn.Run, hfmIn_ad, cache=Eikonal.Cache() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "values_ad_interp = UniformGridInterpolation(X,values_ad)\n",
    "objective = ad.array( (alpha0*values_ad_interp(x0) + alpha1*values_ad_interp(x1), alpha2*values_ad_interp(x2)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We defined a multi-dimensional objective function, but we can only request its gradient component by component so far. The cost remains bearable since the fast marching solver is bypassed thanks to cached data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Providing cached data\n",
      "Field verbosity defaults to 1\n",
      "Field order defaults to 1\n",
      "Field seedRadius defaults to 0\n",
      "Bypassing fast marching solver based on cached data.\n",
      "------\n",
      "Providing cached data\n",
      "Field verbosity defaults to 1\n",
      "Field order defaults to 1\n",
      "Field seedRadius defaults to 0\n",
      "Bypassing fast marching solver based on cached data.\n"
     ]
    }
   ],
   "source": [
    "grad_0, = rev.to_inputshapes(rev.gradient(objective[0]))\n",
    "print(\"------\")\n",
    "grad_1, = rev.to_inputshapes(rev.gradient(objective[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[12,3])\n",
    "plt.subplot(1,2,1)\n",
    "plt.contourf(*X,grad_0)\n",
    "plt.subplot(1,2,2)\n",
    "plt.contourf(*X,grad_1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": false,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}