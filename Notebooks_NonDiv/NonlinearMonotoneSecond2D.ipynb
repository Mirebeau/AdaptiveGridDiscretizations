{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaptive PDE discretizations on Cartesian grids\n",
    "## Volume : Non-divergence form PDEs\n",
    "## Part : Monotone numerical schemes\n",
    "## Chapter : PDEs with a second order non-linearity\n",
    "\n",
    "This notebook illustrates the use of monotone finite difference schemes to compute viscosity solutions of non-linear PDEs, in two space dimensions. \n",
    "We consider Pucci's operator\n",
    "$$\n",
    "    \\Lambda u(x) := \\alpha(x) \\lambda_{\\max}(\\nabla^2 u(x)) + \\lambda_{\\min}(\\nabla^2 u(x))\n",
    "$$\n",
    "in the PDE\n",
    "$$\n",
    "    {-} \\Lambda u(x) + \\beta(x) = 0,\n",
    "$$\n",
    "with Dirichlet boundary conditions. The PDE parameters are a positive function $\\alpha$, and an arbitrary function $\\beta$.\n",
    "We denote by $\\lambda_{\\max}(M)$ and $\\lambda_{\\min}(M)$ the largest and smallest eigenvalue of a positive definite tensor $M$. More details on this problem below.\n",
    "\n",
    "We design two monotone numerical schemes: \n",
    "* The first sheme, based on a discretization of the space of controls, is simple to implement. However it is quite costly numerically, and it induces a consistency defect.\n",
    "* The second scheme is second order consistent and possibly cheaper numerically. However, implementation details are more subtle.\n",
    "\n",
    "The two schemes involves adaptive stencils, built using techniques from lattice geometry. The techniques developed are fairly general, and can be applied to a wide range of non-linear PDEs. Numerical implementation is kept simple thanks to the use of automatic differentiation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discretization of the PDE\n",
    "\n",
    "### Reformulation as an extremal operator\n",
    "\n",
    "Assume without loss of generality that $\\alpha \\leq 1$. Then for any positive definite matrix $M$ one has \n",
    "$$\n",
    "    \\alpha \\lambda_{\\max}(M) + \\lambda_{\\min}(M) = \\min_{0 \\leq \\theta \\leq \\pi} \\mathrm{Tr}(D_\\alpha(\\theta) M),\n",
    "$$\n",
    "where we denoted, with $e(\\theta) := (\\cos \\theta, \\sin \\theta)$\n",
    "$$\n",
    "    D_\\alpha(\\theta) = \\alpha\\, e(\\theta) e(\\theta)^T + e(\\theta)^\\perp (e(\\theta)^\\perp)^T,\n",
    "$$\n",
    "the symmetric matrix whose eigenvalues are $\\alpha$ and $1$, the former associated with the eigenvector $e(\\theta)$.\n",
    "\n",
    "**Remark on the range of the variable $\\theta$.**\n",
    "For any $\\theta\\in \\mathbb R$, one has $e(\\theta+\\pi) = -e(\\theta)$, and therefore $D_\\alpha(\\theta+\\pi) = D_\\alpha(\\theta)$. By periodicity, we may therefore limit our attention to the interval $[0,\\pi]$.\n",
    "\n",
    "**Remark on the case $\\alpha\\geq 1$.**\n",
    "This second case is handled by replacing the minimum over $\\theta\\in [0,\\pi]$ with a maximum. This does not induce any additional difficulty from the theoretical or numerical standpoints. However, for the sake of simplicity, we make the assumption that $\\alpha\\leq 1$ in the following.\n",
    "\n",
    "### A monotone discretization strategy : sampling the control space\n",
    "\n",
    "Let $K$ be a positive integer, and let $\\theta_1 \\leq \\cdots \\leq \\theta_K$ be a sampling of the interval $[0,\\pi]$. Then we may consider the approximate operator\n",
    "$$\n",
    "    \\Lambda_K u(x) := \\min_{1 \\leq k \\leq K} \\mathrm{Tr} (D_\\alpha(\\theta_k) \\nabla^2 u(x)).\n",
    "$$\n",
    "Introduce decompositions of the tensors, obtained e.g. by Selling's method,\n",
    "$$\n",
    "    D_\\alpha(\\theta_k) = \\sum_{1 \\leq i \\leq n} \\mu_{ki} e_{ki} e_{ki}^T,\n",
    "$$\n",
    "where $\\mu_{ki} \\geq 0$ and $e_{ki}$ has integer coordinates. Then we obtain the monotone numerical scheme\n",
    "$$\n",
    "    \\min_{1 \\leq k \\leq K} \\sum_{1 \\leq i \\leq n} \\mu_{ki} \\frac{ u(x+h e_{ki}) - 2 u(x) +u(x-h e_{ki})} {h^2}.\n",
    "$$\n",
    "A consistency defect remains, which can be estimated in terms of the width of the sampling $\\theta_1,\\cdots,\\theta_K$ of the control space $[0,\\pi]$. \n",
    "\n",
    "An additional problem is that the numerical scheme cost increases as $K$ increases.\n",
    "This issue becomes more acute in the case of a multi-dimensional control space.\n",
    "\n",
    "### Another monotone and consistent discretization \n",
    "\n",
    "In order to introduce this discretization, we need to recall some elements from lattice geometry.\n",
    "Selling's decomposition of a tensor $D$ involves a geometrical object, referred to as a *$D$-obtuse superbase* and here  denoted\n",
    "$$\n",
    "    \\mathrm{osb}(D).\n",
    "$$\n",
    "The obtuse superbase $s=\\mathrm{osb}(D)$ dictates the support $(e_{si})_{i=1}^n$ of Selling's decomposition of $D$, hence the stencil of the numerical scheme. We can take advantage of this fact to rewrite the operator as \n",
    "$$\n",
    "    \\Lambda u(x) = \\min_{s \\in S} \\Lambda_s u(x)\n",
    "$$\n",
    "where \n",
    "$$\n",
    "    \\Lambda_s u(x) := \\min_{\\theta, \\mathrm{osb}(D_\\alpha(\\theta)) = s} \\mathrm{Tr} (D_\\alpha(\\theta) \\nabla^2 u).\n",
    "$$\n",
    "Each operator $\\Lambda_s$ admits the consistent discretization\n",
    "$$\n",
    "    \\Lambda_s u(x) \\approx \\min_{\\theta, \\mathrm{osb}(D_\\alpha(\\theta)) = s} \\sum_{1 \\leq i \\leq n} \n",
    "    \\mu_{si}(\\theta) \\frac{u(x+h e_{si}) - 2 u(x) + u(x-e_{si})} {h^2}, \n",
    "$$\n",
    "and a closed form can be obtained for the r.h.s. by examining a simple optimization problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**Summary**](Summary.ipynb) of volume Non-Divergence form PDEs, this series of notebooks.\n",
    "\n",
    "[**Main summary**](../Summary.ipynb) of the Adaptive Grid Discretizations \n",
    "\tbook of notebooks, including the other volumes.\n",
    "\n",
    "# Table of contents\n",
    "  * [1. Non-Monotone discretization (purposedly fails)](#1.-Non-Monotone-discretization-(purposedly-fails))\n",
    "  * [2. Monotone discretization by sampling of the control space](#2.-Monotone-discretization-by-sampling-of-the-control-space)\n",
    "    * [2.1 Limit case : $\\alpha=1$.](#2.1-Limit-case-:-$\\alpha=1$.)\n",
    "    * [2.2 Limit case : $\\alpha \\to 0$](#2.2-Limit-case-:-$\\alpha-\\to-0$)\n",
    "    * [2.3 Optimization opportunities](#2.3-Optimization-opportunities)\n",
    "  * [3. Monotone and consistent discretization](#3.-Monotone-and-consistent-discretization)\n",
    "    * [3.1 Angular sectors and obtuse superbases](#3.1-Angular-sectors-and-obtuse-superbases)\n",
    "    * [3.2 Optimization over an angular sector](#3.2-Optimization-over-an-angular-sector)\n",
    "    * [3.3 Optimized implementation](#3.3-Optimized-implementation)\n",
    "    * [3.4 Non-square domains](#3.4-Non-square-domains)\n",
    "  * [4 Validation](#4-Validation)\n",
    "    * [4.1 Comparaison of the two schemes](#4.1-Comparaison-of-the-two-schemes)\n",
    "    * [4.2 Comparison with automatic differentiation](#4.2-Comparison-with-automatic-differentiation)\n",
    "\n",
    "\n",
    "\n",
    "**Acknowledgement.** Some of the experiments presented in these notebooks are part of \n",
    "ongoing research with Ludovic Métivier and Da Chen.\n",
    "\n",
    "Copyright Jean-Marie Mirebeau, Centre Borelli, ENS Paris-Saclay, CNRS, University Paris-Saclay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Importing the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.path.insert(0,\"..\") # Allow imports from parent directory\n",
    "#from Miscellaneous import TocTools; print(TocTools.displayTOC('NonlinearMonotoneSecond2D','NonDiv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "ExportCode"
    ]
   },
   "outputs": [],
   "source": [
    "from agd import Selling\n",
    "from agd import LinearParallel as lp\n",
    "from agd import AutomaticDifferentiation as ad\n",
    "from agd import Domain\n",
    "from agd.Plotting import savefig; #savefig.dirName = \"Figures/NonlinearMonotoneSecond2D\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "ExportCode"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "newton_root = ad.Optimization.newton_root\n",
    "norm = ad.Optimization.norm\n",
    "    \n",
    "def BoundaryNeighborhood(interior,width=1):\n",
    "    bd=interior.copy()\n",
    "    bd[0,:]=False; bd[-1,:]=False; bd[:,0]=False; bd[:,-1]=False\n",
    "    directions = ( (0,0),(0,1),(0,-1),(1,0),(-1,0) ) \n",
    "    neigh = np.stack(tuple(np.roll(bd,e,axis=(0,1)) for e in directions),axis=0)\n",
    "    neigh = np.logical_and(neigh.any(axis=0),np.logical_not(neigh).any(axis=0))\n",
    "    for i in range(width):\n",
    "        neigh = np.stack(tuple(np.roll(neigh,e,axis=(0,1)) for e in directions),axis=0).any(axis=0)\n",
    "    return neigh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Non-Monotone discretization (purposedly fails)\n",
    "\n",
    "We present a naive, non-monotone discretization of the addressed problem. This scheme can be used to check that a numerical solution (produced by other means) is correct, or to construct synthetic examples. However, using it to solve the PDE is usually bound to fail. \n",
    "\n",
    "The naive scheme is based on a reconstruction of the Hessian matrix of the form\n",
    "$$\n",
    "    \\begin{pmatrix}\n",
    "        D^h_{00} u(x) & D^h_{01} u(x)\\\\\n",
    "        D^h_{01} u(x) & D^h_{11} u(x)\n",
    "    \\end{pmatrix},\n",
    "$$\n",
    "where $D_{00}$, $D_{01}$ and $D_{11}$ are finite-difference operators. Namely\n",
    "$$\n",
    "    D^h_{00} u(x) := \\frac{u(x_0+h,x_1)-2 u(x_0,x_1) + u(x_0-h,x_1)}{h^2},\n",
    "$$\n",
    "likewise for $D^h_{11} u(x)$, and finally\n",
    "$$\n",
    "    D^h_{01} u(x) := \\frac{u(x_0+h,x_1+h)-u(x_0-h,x_1+h)-u(x_0+h,x_1-h)+h(x_0-h,x_1-h)}{4 h^2}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "ExportCode"
    ]
   },
   "outputs": [],
   "source": [
    "def SchemeNonMonotone(u,α,β,bc,sqrt_relax=1e-6):\n",
    "    # Compute the hessian matrix of u\n",
    "    uxx = bc.Diff2(u,(1,0))\n",
    "    uyy = bc.Diff2(u,(0,1))\n",
    "    uxy = 0.25*(bc.Diff2(u,(1,1)) - bc.Diff2(u,(1,-1)))\n",
    "    \n",
    "    # Compute the eigenvalues\n",
    "    # The relaxation is here to tame the non-differentiability of the square root.\n",
    "    htr = (uxx+uyy)/2. # Half trace\n",
    "    Δ = ((uxx-uyy)/2.)**2 + uxy**2 # Discriminant of characteristic polynomial\n",
    "    sΔ = np.sqrt( np.maximum( Δ, sqrt_relax) )\n",
    "\n",
    "    λ_max = htr+sΔ\n",
    "    λ_min = htr-sΔ\n",
    "    \n",
    "    # Numerical scheme\n",
    "    residue = β - α*λ_max - λ_min\n",
    "    \n",
    "    # Boundary conditions\n",
    "    return np.where(bc.interior,residue,u-bc.grid_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our next step is to define the parameters of our specific problem. \n",
    "Regarding the boundary conditions, we set $u=0$ on the square boundary, and $u=-1$ on some interior diamond.\n",
    "For well posedness, $d$ and $\\alpha$ must be positive over the domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the domain\n",
    "aX0,dx = np.linspace(-1,1,100,retstep=True); aX1=aX0;\n",
    "X = np.array(np.meshgrid(aX0,aX1,indexing='ij'))\n",
    "\n",
    "# Set the boundary conditions \n",
    "bc_grid_values=np.full(X.shape[1:],np.nan) \n",
    "bc_grid_values[ad.Optimization.norm(X,ord=1,axis=0) < 0.4] = -1\n",
    "\n",
    "bc = Domain.MockDirichlet(bc_grid_values,dx,padding=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the PDE parameters\n",
    "α = 0.25\n",
    "β = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The naive and non-monotone discretization scheme is consistent, but lacks any other sort of theoretical guarantees. \n",
    "It is pure luck that the Newton method does converge in this simple instance, and that the result looks reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1  Residue norm: 26.02813859719685\n",
      "Iteration: 2  Residue norm: 0.9663557973408544\n",
      "Iteration: 3  Residue norm: 0.05893186412779561\n",
      "Iteration: 4  Residue norm: 0.0004164739390075667\n",
      "Iteration: 5  Residue norm: 7.709924487731001e-08\n",
      "Iteration: 6  Residue norm: 1.606159649725214e-12\n",
      "Target residue reached. Terminating.\n"
     ]
    }
   ],
   "source": [
    "params = (α,β,bc); guess = np.zeros(bc.shape); \n",
    "solution = ad.Optimization.newton_root(SchemeNonMonotone,guess,params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Solution obtained with non-monotone scheme\")\n",
    "plt.contourf(*X,solution); plt.axis('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Monotone discretization by sampling of the control space\n",
    "\n",
    "We present a numerical scheme based on sampling the control space, which is quite simple and generic. Given fields of positive definite diffusion tensors $D_k(x)$, $1 \\leq k \\leq K$, without any specific assumption on their origin, we compute the decompositions\n",
    "$$\n",
    "    D_k(x) = \\sum_{1 \\leq i \\leq n} \\mu_{ki}(x) e_{ki}(x) e_{ki}(x)^T.\n",
    "$$\n",
    "We then implement the scheme\n",
    "$$\n",
    "    \\beta(x) - \\min_{1\\leq k \\leq K} \\sum_{1 \\leq i \\leq n} \n",
    "    \\mu_{ki}(x) \\frac{u(x+h e_{ki}(x))-2 u(x) +u(x-h e_{ki}(x))}{h^2}.\n",
    "$$\n",
    "\n",
    "<!---ExoFR\n",
    "Implémenter le schéma ci-dessus.\n",
    "--->\n",
    "\n",
    "<!---ExoCode\n",
    "def SchemeSampling(u,diffs,β,bc):\n",
    "    # Tensor decomposition \n",
    "    μ,e = Selling.Decomposition(diffs)\n",
    "    μ = bc.as_field(μ) # Broadcast\n",
    "\n",
    "    # Numerical scheme \n",
    "    residue = # TODO\n",
    "    \n",
    "    # Boundary conditions\n",
    "    return np.where(bc.interior,residue,u-bc.grid_values)\n",
    "--->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": [
     "ExportCode"
    ]
   },
   "outputs": [],
   "source": [
    "def SchemeSampling(u,diffs,β,bc):\n",
    "    # Tensor decomposition \n",
    "    μ,e = Selling.Decomposition(diffs)\n",
    "    \n",
    "    # Numerical scheme \n",
    "    μ = bc.as_field(μ)\n",
    "    residue = β - (μ*bc.Diff2(u,e)).sum(0).min(0)\n",
    "    \n",
    "    # Boundary conditions\n",
    "    return np.where(bc.interior,residue,u-bc.grid_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tensors involved in our PDE take the following form.\n",
    "\n",
    "<!---\n",
    "from agd import FiniteDifferences as fd\n",
    "    α,θ = fd.common_field((α,θ),depths=(0,0))\n",
    "--->\n",
    "\n",
    "<!---ExoFR\n",
    "Implémenter une fonction calculant le tenseur associé \n",
    "à une anisotropie $\\alpha$ et une orientation $\\theta$, \n",
    "apparaissant dans l'opérateur de Pucci.\n",
    "--->\n",
    "\n",
    "<!---ExoCode\n",
    "def Diff(α,θ):\n",
    "    return # TODO\n",
    "--->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": [
     "ExportCode"
    ]
   },
   "outputs": [],
   "source": [
    "def Diff(α,θ):\n",
    "    e0 = np.array(( np.cos(θ),np.sin(θ)))\n",
    "    e1 = np.array((-np.sin(θ),np.cos(θ)))\n",
    "    if isinstance(α,np.ndarray): \n",
    "        e0,e1 = (as_field(e,α.shape) for e in (e0,e1))\n",
    "    return α*lp.outer_self(e0) + lp.outer_self(e1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also choose a discretization of the control space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "nθ = 20\n",
    "θs = np.linspace(0,np.pi,nθ,endpoint=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next solve the PDE and display the solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1  Residue norm: 69.1143589693038\n",
      "Iteration: 2  Residue norm: 1.665907484554825\n",
      "Iteration: 3  Residue norm: 0.2523086292703738\n",
      "Iteration: 4  Residue norm: 0.05531949819951609\n",
      "Iteration: 5  Residue norm: 0.003448993349207008\n",
      "Iteration: 6  Residue norm: 1.4304113449270517e-12\n",
      "Target residue reached. Terminating.\n"
     ]
    }
   ],
   "source": [
    "params = (Diff(α,θs), β,bc)\n",
    "solution = newton_root(SchemeSampling,guess,params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.axis('equal'); plt.title('Solution to lambda_max/2 + lambda_min = 1')\n",
    "plt.contourf(*X,solution);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the solution is not explicit, we use the non-monotone numerical scheme to test the result.\n",
    "We eliminate a layer around the neighborhood of the boundary conditions, where the solution is not smooth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max cross-residue of the sampling based numerical solution: 0.05897075265438145\n"
     ]
    }
   ],
   "source": [
    "residue_non_monotone = SchemeNonMonotone(solution,α,β,bc)\n",
    "residue_non_monotone[BoundaryNeighborhood(bc.interior,width=5)] = 0.\n",
    "print(\"Max cross-residue of the sampling based numerical solution:\",norm(residue_non_monotone,ord=np.inf))\n",
    "plt.axis('equal'); plt.title(\"Cross-residue of the sampling based numerical solution:\")\n",
    "plt.contourf(*X,residue_non_monotone);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Limit case : $\\alpha=1$.\n",
    "\n",
    "If one chooses $\\alpha=1$, then the PDE becomes linear, namely $-\\Delta u + \\beta = 0$.\n",
    "In this very specific case, the sampling based and non-monotone scheme coincide, with the usual discretization of the laplacian operator.\n",
    "As a result the Newton method converges in one iteration, and the cross-residue vanishes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1  Residue norm: 8.640199666842818e-12\n",
      "Target residue reached. Terminating.\n"
     ]
    }
   ],
   "source": [
    "params = (Diff(1.,θs), β,bc)\n",
    "solution = newton_root(SchemeSampling,guess,params)\n",
    "\n",
    "plt.axis('equal'); plt.title('Solution to λ_max + λ_min = 1')\n",
    "plt.contourf(*X,solution);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross residue in special case α=1 : 8.640199666842818e-12\n"
     ]
    }
   ],
   "source": [
    "print(\"Cross residue in special case α=1 :\", norm(SchemeNonMonotone(solution,1.,β,bc),ord=np.inf) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Limit case : $\\alpha \\to 0$\n",
    "\n",
    "In contrast, if one chooses a very small value of $\\alpha$, then the PDE becomes more and more non-linear, which raises numerical difficulties discussed below.\n",
    "If in addition $\\beta = 0$, then we recover PDE characterization of the convex envelope:\n",
    "$$\n",
    "    -\\lambda_{\\min}(\\nabla^2 u) = 0\n",
    "$$\n",
    "\n",
    "**Numerical challenges.** \n",
    "As $\\alpha\\to 0$, the condition number of the tensors $D_\\alpha(\\theta)$ increase. A finer sampling of the interval $[0,\\pi]$ is required, which increases the numerical cost of the method.\n",
    "In addition, the width of the discretization stencil increases, and therefore the effective discretization scale is reduced.\n",
    "\n",
    "**Note on computing the convex envelope.**\n",
    "The computation of convex envelopes is one of the most central problems in algorithmic geometry. For instance, Voronoi diagrams are deduced from a convex envelope computation in higher dimension.\n",
    "Extremely efficient software packages are available for this problem, and PDE methods are *not* the recommended way to go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "α_small = 0.01\n",
    "nθ_small = 50\n",
    "θs_small = np.linspace(0,np.pi,nθ_small,endpoint=False)\n",
    "β_cvx_env = 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above parameters turn the solution into the convex envelope of the boundary conditions. Recall that we imposed:\n",
    "* $u=0$ on the (exterior) square boundary.\n",
    "* $u=-1$ on the (interior) diamond boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1  Residue norm: 551.0630896254996\n",
      "Iteration: 2  Residue norm: 9.991945726638845\n",
      "Iteration: 3  Residue norm: 12.634877848761128\n",
      "Iteration: 4  Residue norm: 3.5106622455859435\n",
      "Iteration: 5  Residue norm: 0.7327788585183101\n",
      "Iteration: 6  Residue norm: 0.10918596570735112\n",
      "Iteration: 8  Residue norm: 0.0029157371512035565\n",
      "Iteration: 10  Residue norm: 1.8987381810120097e-05\n",
      "Iteration: 12  Residue norm: 3.823106472032108e-08\n",
      "Iteration: 14  Residue norm: 9.96641579637215e-09\n",
      "Target residue reached. Terminating.\n"
     ]
    }
   ],
   "source": [
    "params = (Diff(α_small,θs_small), β_cvx_env,bc) \n",
    "solution = newton_root(SchemeSampling,guess,params)\n",
    "plt.axis('equal'); plt.title('Convex envelope of the boundary conditions.')\n",
    "plt.contourf(*X,solution);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The solution is piecewise affine, and its gradient is piecewise constant.\n",
    "Note in particular that the solution is not twice differentiable, and the equation $-\\lambda_{\\min}(\\nabla^2 u) = 0$ here only has meaning in the sense of viscosity solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad = np.array(np.gradient(solution,bc.gridscale))\n",
    "plt.axis('equal'); plt.title(\"The solution to the convex envelope problem is piecewise affine\")\n",
    "plt.contourf(*X,norm(grad,ord=2,axis=0));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next negate the boundary conditions, imposing:\n",
    "* $u=0$ on the (exterior) square boundary.\n",
    "* $u=1$ on the (interior) diamond boundary.\n",
    "\n",
    "This raises an apparent incompatibility: the convex envelope should be $u=0$ on the whole square, yet we impose a distinct value in the diamond. What to expect ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc_negated = Domain.MockDirichlet(-bc_grid_values,dx,padding=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1  Residue norm: 806.0820465420278\n",
      "Iteration: 2  Residue norm: 487.71324707786044\n",
      "Iteration: 3  Residue norm: 200.20838090074258\n",
      "Iteration: 4  Residue norm: 164.3187561466851\n",
      "Iteration: 5  Residue norm: 111.67932913072411\n",
      "Iteration: 6  Residue norm: 75.49541847970924\n",
      "Iteration: 8  Residue norm: 29.94570387126228\n",
      "Iteration: 10  Residue norm: 1.4099832412739488e-13\n",
      "Target residue reached. Terminating.\n"
     ]
    }
   ],
   "source": [
    "params = (Diff(α_small,θs_small), β_cvx_env,bc_negated) \n",
    "solution = newton_root(SchemeSampling,guess,params)\n",
    "plt.axis('equal'); plt.title('Convex envelope of the boundary conditions.')\n",
    "plt.contourf(*X,solution);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The solution to the second order PDE $-\\lambda_{\\min}(\\nabla^2 u)$ is still unique and well defined, in the sense of *viscosity solutions*, with these boundary conditions. It is piecewise constant, with value $u=0$ except on the interior diamond where we impose $u=-1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Optimization opportunities\n",
    "\n",
    "The numerical scheme implemented in this section involves a maximization over a set of diffusion tensors. The size of this set dictates the accuracy of the method, and it may therefore become rather large. In combination with the overhead of sparse AD, this can increase the numerical cost.\n",
    "\n",
    "We can significantly limit the numerical cost using a technique based on the *envelope theorem*, which proceeds in two steps:\n",
    "* First run the scheme with ordinary floats, to find out which tensor is active at each grid point.\n",
    "* Second, run the scheme with AD variables, and and *oracle* providing the active tensors.\n",
    "\n",
    "**Important : other optimization opportunities.** \n",
    "The \"optimization\" presented in this subsection only serves to illustrate the envelope theorem mechanism. It is *not* effective in terms of computation time, because the optimized part is not dominant. There are other optimization opportunities here, the most obvious one being to avoid recomputing the tensor decompositions at each call of the iterative solver. The choice of linear solver may also be of importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": [
     "ExportCode"
    ]
   },
   "outputs": [],
   "source": [
    "def SchemeSampling_OptInner(u,diffs,bc,oracle=None):\n",
    "    # Select the active tensors, if they are known\n",
    "    if not(oracle is None):\n",
    "        diffs = np.take_along_axis(diffs, np.broadcast_to(oracle,diffs.shape[:2]+(1,)+oracle.shape),axis=2)\n",
    "    \n",
    "    print(\"Has AD information :\", ad.is_ad(u), \". Number active tensors per point :\", diffs.shape[2])\n",
    "    \n",
    "    # Tensor decomposition \n",
    "    coefs,offsets = Selling.Decomposition(diffs)\n",
    "    \n",
    "    # Return the minimal value, and the minimizing index\n",
    "    return ad.min_argmin( lp.dot_VV(coefs,bc.Diff2(u,offsets)), axis=0)\n",
    "\n",
    "def SchemeSampling_Opt(u,diffs,β,bc):\n",
    "    # Evaluate the operator using the envelope theorem\n",
    "    result,_ = ad.apply(SchemeSampling_OptInner, u,bc.as_field(diffs),bc, envelope=True)\n",
    "        \n",
    "    # Boundary conditions\n",
    "    return np.where(bc.interior, β-result, u-bc.grid_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Has AD information : False . Number active tensors per point : 20\n",
      "Has AD information : True . Number active tensors per point : 1\n",
      "Has AD information : False . Number active tensors per point : 20\n",
      "Has AD information : True . Number active tensors per point : 1\n",
      "Iteration: 1  Residue norm: 69.1143589693038\n",
      "Has AD information : False . Number active tensors per point : 20\n",
      "Has AD information : True . Number active tensors per point : 1\n",
      "Iteration: 2  Residue norm: 1.665907484554825\n",
      "Has AD information : False . Number active tensors per point : 20\n",
      "Has AD information : True . Number active tensors per point : 1\n",
      "Iteration: 3  Residue norm: 0.2523086292703738\n",
      "Has AD information : False . Number active tensors per point : 20\n",
      "Has AD information : True . Number active tensors per point : 1\n",
      "Iteration: 4  Residue norm: 0.05531949819951609\n",
      "Has AD information : False . Number active tensors per point : 20\n",
      "Has AD information : True . Number active tensors per point : 1\n",
      "Iteration: 5  Residue norm: 0.003448993349207008\n",
      "Has AD information : False . Number active tensors per point : 20\n",
      "Has AD information : True . Number active tensors per point : 1\n",
      "Iteration: 6  Residue norm: 1.4304113449270517e-12\n",
      "Target residue reached. Terminating.\n"
     ]
    }
   ],
   "source": [
    "params = (Diff(α,θs), β,bc)\n",
    "solution = newton_root(SchemeSampling_Opt,guess,params) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Monotone and consistent discretization\n",
    "\n",
    "Setting up a monotone and consistent discretization requires a bit more work, but is worthwhile in the end if performance and accuracy are a target. Let us recall that the diffusion tensors take the form,\n",
    "$$\n",
    "    D_\\alpha(\\theta) = \\alpha\\, e(\\theta) e(\\theta)^T + e(\\theta)^\\perp (e(\\theta)^\\perp)^T,\n",
    "$$\n",
    "where $0< \\alpha \\leq 1$ is a fixed parameter in the following, and $\\theta \\in [0,\\pi]$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Angular sectors and obtuse superbases\n",
    "The first, and main, difficulty is to construct a sequence of angles $0 = \\theta_0 \\leq \\cdots \\leq \\theta_N = \\pi$ and of superbases $s_0,\\cdots, s_{N-1}$ such that \n",
    "$$\n",
    "    s_k \\text{ is } D_\\theta(\\theta) \\text{-obtuse, for all } \\theta \\in [\\theta_k, \\theta_{k+1}].\n",
    "$$\n",
    "For that purpose, we remark that \n",
    "$$\n",
    "    D_\\alpha(\\theta) = D_0 + D_1 \\cos(2 \\theta) + D_2 \\sin(2 \\theta),\n",
    "$$\n",
    "where (omitting the dependency on $\\alpha$ for readability)\n",
    "$$\n",
    "    D_0 = \\frac{\\alpha+1} 2\n",
    "    \\begin{pmatrix}\n",
    "    1 & 0\\\\\n",
    "    0 & 1\n",
    "    \\end{pmatrix},\n",
    "    \\quad \n",
    "    D_1 = \\frac{\\alpha-1} 2\n",
    "    \\begin{pmatrix}\n",
    "    1 & 0\\\\\n",
    "    0 &-1\n",
    "    \\end{pmatrix},\n",
    "    \\quad \n",
    "    D_2 = \\frac{\\alpha-1} 2\n",
    "    \\begin{pmatrix}\n",
    "    0 & 1\\\\\n",
    "    1 & 0\n",
    "    \\end{pmatrix}.\n",
    "$$\n",
    "Then, for given $u,v \\in R^2$, one has \n",
    "$$\n",
    "    <u,D_\\alpha(\\theta) v> = a_0 + a_1 \\cos(2 \\theta) + a_2 \\sin(2 \\theta)\n",
    "    = r (\\cos(2\\theta-\\phi) - c).\n",
    "$$\n",
    "where $a_i = <u,D_i v>$. Then $r e(\\phi) = (a_1,a_2)$, and $c=-a_0/r$. We assume that $r$ is positive.\n",
    "Eventually, the above scalar product is \n",
    "* always negative if $c>1$.\n",
    "* always positive if $c<-1$.\n",
    "* otherwise, positive iff $|2 \\theta-\\phi| \\leq \\arccos(c)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": [
     "ExportCode"
    ]
   },
   "outputs": [],
   "source": [
    "def MakeD(α):\n",
    "    return np.moveaxis(0.5*np.array([\n",
    "        (α+1)*np.array([[1,0],[0,1]]),\n",
    "        (α-1)*np.array([[1,0],[0,-1]]),\n",
    "        (α-1)*np.array([[0,1],[1,0]])\n",
    "    ]), 0,-1)\n",
    "\n",
    "def NextAngleAndSuperbase(θ,sb,D):\n",
    "    pairs = np.stack([(1,2), (2,0), (0,1)],axis=1)\n",
    "    scals = lp.dot_VAV(np.expand_dims(sb[:,pairs[0]],axis=1), \n",
    "                       np.expand_dims(D,axis=-1), np.expand_dims(sb[:,pairs[1]],axis=1))\n",
    "    ϕ = np.arctan2(scals[2],scals[1])\n",
    "    cst = -scals[0]/np.sqrt(scals[1]**2+scals[2]**2)\n",
    "    θ_max = np.pi*np.ones(3)\n",
    "    mask = cst<1\n",
    "    θ_max[mask] = (ϕ[mask]-np.arccos(cst[mask]))/2\n",
    "    θ_max[θ_max<=0] += np.pi\n",
    "    θ_max[θ_max<=θ] = np.pi\n",
    "    k = np.argmin(θ_max)\n",
    "    i,j = (k+1)%3,(k+2)%3\n",
    "    return (θ_max[k],np.stack([sb[:,i],-sb[:,j],sb[:,j]-sb[:,i]],axis=1))\n",
    "\n",
    "def AnglesAndSuperbases(D,maxiter=200):\n",
    "    sb = Selling.CanonicalSuperbase(np.eye(2)).astype(int)\n",
    "    θs=[]\n",
    "    superbases=[]\n",
    "    θ=0\n",
    "    for i in range(maxiter):\n",
    "        θs.append(θ)\n",
    "        if(θ>=np.pi): break\n",
    "        superbases.append(sb)\n",
    "        θ,sb = NextAngleAndSuperbase(θ,sb,D)\n",
    "    return np.array(θs), np.stack(superbases,axis=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code is a bit intricate, but its purpose is simple : split the interval $[0,\\pi]$ into sub-intervals on which the support of Selling's decomposition of the tensors is fixed and known."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "α=0.1\n",
    "θs,superbases = AnglesAndSuperbases(MakeD(α))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.12912139, 0.65627678, 0.91451955, 1.44167494,\n",
       "       1.57079633, 1.69991771, 2.2270731 , 2.48531588, 3.01247127,\n",
       "       3.14159265])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "θs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-1, -1, -1,  0,  0,  1,  0,  0, -1, -1],\n",
       "        [ 1, -1,  1,  1, -1,  0,  1, -1, -1,  1],\n",
       "        [ 0,  2,  0, -1,  1, -1, -1,  1,  2,  0]],\n",
       "\n",
       "       [[-1, -1, -1,  1,  1,  0, -1, -1,  1,  1],\n",
       "        [ 0,  0,  0,  1, -1, -1, -1,  1,  0,  0],\n",
       "        [ 1,  1,  1, -2,  0,  1,  2,  0, -1, -1]]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "superbases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "θs_sampled = np.linspace(0,np.pi,200)\n",
    "decomp = Selling.GatherByOffset(θs_sampled,*Selling.Decomposition(Diff(α,θs_sampled)))\n",
    "\n",
    "fig = plt.figure(figsize=(20,10)) # Paper:(10,5)\n",
    "plt.title(f\"Decomposition of a rotated matrix of eigenvalues {α} and {1}\") \n",
    "plt.xlabel(\"θ\"); plt.ylabel(\"Coefficient\")\n",
    "for offset,(angle,coef) in decomp.items():\n",
    "    plt.plot(angle,coef)\n",
    "plt.legend(decomp.keys());\n",
    "\n",
    "for θ in θs: # Show a vertical line for each angle theta where the stencil changes\n",
    "    plt.axvline(x=θ)\n",
    "    \n",
    "savefig(fig,\"DecompositionCoefficients.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A value of $\\alpha$ closer to $1$ yields a smaller number of superbases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "α=1/4\n",
    "θs_sampled = np.linspace(0,np.pi,200)\n",
    "θs,superbases = AnglesAndSuperbases(MakeD(α))\n",
    "decomp = Selling.GatherByOffset(θs_sampled,*Selling.Decomposition(Diff(α,θs_sampled)))\n",
    "\n",
    "fig = plt.figure(figsize=(20,10)) # Paper:(10,5)\n",
    "plt.title(f\"Decomposition of a rotated matrix of eigenvalues {α} and {1}\") \n",
    "plt.xlabel(\"θ\"); plt.ylabel(\"Coefficient\")\n",
    "for offset,(angle,coef) in decomp.items():\n",
    "    plt.plot(angle,coef)\n",
    "plt.legend(decomp.keys());\n",
    "\n",
    "for θ in θs: # Show a vertical line for each angle theta where the stencil changes\n",
    "    plt.axvline(x=θ);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whereas a smaller values yields more superbases and increases the numerical scheme complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "α=1/100\n",
    "θs_sampled = np.linspace(0,np.pi,400)\n",
    "θs,superbases = AnglesAndSuperbases(MakeD(α))\n",
    "decomp = Selling.GatherByOffset(θs_sampled,*Selling.Decomposition(Diff(α,θs_sampled)))\n",
    "\n",
    "fig = plt.figure(figsize=(20,10)) # Paper:(10,5)\n",
    "plt.title(f\"Decomposition of a rotated matrix of eigenvalues {α} and {1}\") \n",
    "plt.xlabel(\"θ\"); plt.ylabel(\"Coefficient\")\n",
    "for offset,(angle,coef) in decomp.items():\n",
    "    plt.plot(angle,coef)\n",
    "plt.legend(decomp.keys());\n",
    "\n",
    "for θ in θs: # Show a vertical line for each angle theta where the stencil changes\n",
    "    plt.axvline(x=θ);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Optimization over an angular sector\n",
    "\n",
    "The second step is discretize \n",
    "$$\n",
    "    \\min_{\\theta \\in [\\theta_0,\\theta_1]} \\mathrm{Tr}(D_\\alpha(\\theta) \\nabla^2 u(x)),\n",
    "$$\n",
    "when $D_\\alpha(\\theta)$ admits the same known obtuse superbase $b = (b_0,b_1,b_2)$ for each $\\theta \\in [\\theta_0,\\theta_1]$.\n",
    "\n",
    "The discretization reads\n",
    "$$\n",
    "    \\min_{\\theta \\in [\\theta_0, \\theta_1]} - \\sum_{1 \\leq i \\leq 3} <b_{i+1}, D_\\alpha(\\theta) b_{i+2}> \\frac{u(x+h b_i^\\perp)-2 u(x) + u(x-h b_i^\\perp)}{h^2}.\n",
    "$$\n",
    "where indices of the superbase are understood modulo $3$.\n",
    "\n",
    "For that purpose, we rely on the expression $D_\\alpha(\\theta) = D_0 + D_1 \\cos(2 \\theta) + D_2 \\sin(2 \\theta)$, and on the explicit solution \n",
    "$$\n",
    "    \\min_{\\phi \\in [2\\theta_0,2\\theta_1]} d_0 + d_1 \\cos \\phi + d_2 \\sin \\phi = d_0 - \\sqrt{d_1^2+d_2^2}\n",
    "$$\n",
    "if $(\\cos \\phi,\\sin \\phi)$ is proportional to $-(d_1,d_2)$ for some $\\phi \\in [2\\theta_0,2\\theta_1]$. Otherwise the minimum is attained at $2\\theta_0$ or $2 \\theta_1$.\n",
    "\n",
    "**Theoretical issue for the Newton method** The lack of differentiability of the term $\\sqrt{d_1^2+d_2^2}$ is a theoretical issue for the Newton method. It does not raise any difficulty from a practical standpoint, although numpy does raise a warning on the matter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": [
     "ExportCode"
    ]
   },
   "outputs": [],
   "source": [
    "def MinimizeTrace(u,α,bc,sqrt_relax=1e-16):\n",
    "    # Compute the tensor decompositions\n",
    "    D=MakeD(α)\n",
    "    θ,sb = AnglesAndSuperbases(D)\n",
    "    θ = np.array([θ[:-1],θ[1:]])\n",
    "    \n",
    "    # Compute the second order differences in the direction orthogonal to the superbase\n",
    "    sb_rotated = np.array([-sb[1],sb[0]])\n",
    "    d2u = bc.Diff2(u,sb_rotated)\n",
    "    d2u[...,bc.not_interior]=0. # Placeholder values to silent NaNs\n",
    "    \n",
    "    # Compute the coefficients of the tensor decompositions\n",
    "    sb1,sb2 = np.roll(sb,1,axis=1), np.roll(sb,2,axis=1)\n",
    "    sb1,sb2 = (e.reshape( (2,3,1)+sb.shape[2:]) for e in (sb1,sb2))\n",
    "    D = D.reshape((2,2,1,3,1)+D.shape[3:])\n",
    "    # Axes of D are space,space,index of superbase element, index of D, index of superbase, and possibly shape of u\n",
    "    scals = lp.dot_VAV(sb1,D,sb2)\n",
    "\n",
    "    # Compute the coefficients of the trigonometric polynomial\n",
    "    scals,θ = (bc.as_field(e) for e in (scals,θ))\n",
    "    coefs = -lp.dot_VV(scals, np.expand_dims(d2u,axis=1))\n",
    "    \n",
    "    # Optimality condition for the trigonometric polynomial in the interior\n",
    "    value = coefs[0] - np.sqrt(np.maximum(coefs[1]**2+coefs[2]**2,sqrt_relax))\n",
    "    coefs_ = ad.remove_ad(coefs) # removed AD information\n",
    "    angle = np.arctan2(-coefs_[2],-coefs_[1])/2.\n",
    "    angle[angle<0]+=np.pi\n",
    "    \n",
    "    # Boundary conditions for the trigonometric polynomial minimization\n",
    "    mask = np.logical_not(np.logical_and(θ[0]<=angle,angle<=θ[1]))\n",
    "    t,c = θ[:,mask],coefs[:,mask]\n",
    "    value[mask],amin_t = ad.min_argmin(c[0]+c[1]*np.cos(2*t)+c[2]*np.sin(2*t),axis=0)\n",
    "        \n",
    "    # Minimize over superbases\n",
    "    value,amin_sb = ad.min_argmin(value,axis=0)\n",
    "    \n",
    "    # Record the optimal angles for future use\n",
    "    angle[mask]=np.take_along_axis(t,np.expand_dims(amin_t,axis=0),axis=0).squeeze(axis=0) # Min over bc\n",
    "    angle = np.take_along_axis(angle,np.expand_dims(amin_sb,axis=0),axis=0) # Min over superbases\n",
    "\n",
    "    return value,angle\n",
    "\n",
    "def SchemeConsistent(u,α,β,bc):\n",
    "    value,_ = MinimizeTrace(u,α,bc)\n",
    "    residue = β - value\n",
    "    return np.where(bc.interior,residue,u-bc.grid_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scheme is efficiently solved by the Newton method. The warning (possibly) raised is related with the lack of differentiability of sqrt, as mentioned above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1  Residue norm: 261.3488583789058\n",
      "Iteration: 2  Residue norm: 18.31789554657712\n",
      "Iteration: 3  Residue norm: 12.641943601447174\n",
      "Iteration: 4  Residue norm: 7.204295363134163\n",
      "Iteration: 5  Residue norm: 7.265577373442123\n",
      "Iteration: 6  Residue norm: 4.462833234235366\n",
      "Iteration: 8  Residue norm: 2.588283030244334\n",
      "Iteration: 10  Residue norm: 0.9007355797397736\n",
      "Iteration: 12  Residue norm: 0.000947857033783861\n",
      "Iteration: 14  Residue norm: 5.0041217836138685e-09\n",
      "Target residue reached. Terminating.\n",
      "CPU times: user 18.6 s, sys: 8.87 s, total: 27.5 s\n",
      "Wall time: 27.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "params = (α,β,bc)\n",
    "guess2 = 0.5*(X[0]**2 +2.*X[1]**2)\n",
    "solution = newton_root(SchemeConsistent,guess2,params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(4,4)); plt.axis('equal')\n",
    "plt.title(\"Solution from the monotone and consistent scheme.\")\n",
    "plt.contourf(*X,solution);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "savefig(fig,\"SolutionMonotoneConsistent.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For validation, we compute the residue of the naive scheme. It is small, as expected, except on the boundary of the obstacle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6,4)); plt.axis('equal')\n",
    "plt.title(\"Residue from the monotone and consistent scheme.\")\n",
    "res = SchemeNonMonotone(solution,*params)\n",
    "res[BoundaryNeighborhood(bc.interior,width=5)] = 0.\n",
    "plt.contourf(res); plt.colorbar();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "savefig(fig,\"ResidueMonotoneConsistent.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Optimized implementation\n",
    "\n",
    "Similarly to the sampling based scheme, we propose an enhanced implementation taking advantage of the envelope theorem, which we recall lets one differentiate functions of the form\n",
    "$$\n",
    "    F(x) = \\min_{\\alpha \\in A} F_\\alpha(x).\n",
    "$$\n",
    "However, there is one important distinction, relative to the nature of the optimization parameter $\\alpha \\in A$. Indeed, it is:\n",
    "* *Discrete* in the case of the sampling scheme. Namely the (index of) the optimal angle\n",
    "* *Continuous* in the case of the consistent scheme. Namely the optimal angle itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "tags": [
     "ExportCode"
    ]
   },
   "outputs": [],
   "source": [
    "def MinimizeTrace_Opt(u,α,bc,oracle=None):\n",
    "    if oracle is None:  return MinimizeTrace(u,α,bc)\n",
    "    \n",
    "    # The oracle contains the optimal angles\n",
    "    diffs=Diff(α,oracle.squeeze(axis=0))\n",
    "    coefs,sb = Selling.Decomposition(diffs)\n",
    "    value = lp.dot_VV(coefs,bc.Diff2(u,sb))\n",
    "    return value,oracle\n",
    "    \n",
    "\n",
    "def SchemeConsistent_Opt(u,α,β,bc):\n",
    "    value,_ = ad.apply(MinimizeTrace_Opt,u,α,bc,envelope=True)\n",
    "    residue = β - value\n",
    "    return np.where(bc.interior,residue,u-bc.grid_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1  Residue norm: 261.3488583788076\n",
      "Iteration: 2  Residue norm: 18.317895546575386\n",
      "Iteration: 3  Residue norm: 12.641943601135733\n",
      "Iteration: 4  Residue norm: 7.204295363131448\n",
      "Iteration: 5  Residue norm: 7.265577373442013\n",
      "Iteration: 6  Residue norm: 4.462833234235095\n",
      "Iteration: 8  Residue norm: 2.5882830302442716\n",
      "Iteration: 10  Residue norm: 0.9007355797388508\n",
      "Iteration: 12  Residue norm: 0.0009478570337299042\n",
      "Iteration: 14  Residue norm: 6.481060133012306e-10\n",
      "Target residue reached. Terminating.\n",
      "CPU times: user 3.64 s, sys: 853 ms, total: 4.5 s\n",
      "Wall time: 4.49 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "params = (α,β,bc)\n",
    "guess2 = 0.5*(X[0]**2 +2.*X[1]**2)\n",
    "solution = newton_root(SchemeConsistent_Opt,guess2,params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Non-square domains\n",
    "\n",
    "We illustrate (first order) accurate (Dirichlet) boundary conditions on a general domain. \n",
    "This is in contrast with the numerical experiments presented in the above subsections, which rely on a rather crude implementation of the boundary conditions. \n",
    "\n",
    "The chosen domain is ring shaped, with a non-smooth boundary including reentrant corners.\n",
    "The chosen boundary condition is $0$ on the inner boundary, and $1$ on the outer boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "outer = Domain.Union(Domain.Ball(),Domain.Box([[0,1],[-1,1]]) )\n",
    "inner = Domain.AffineTransform(outer,0.4*lp.rotation(np.pi/3))\n",
    "domain_ring = Domain.Complement(outer,inner)\n",
    "\n",
    "def bc_value_ring(x):\n",
    "    \"\"\"0 on inner boundary, 1 on outer boundary.\"\"\"\n",
    "    return outer.level(x)+inner.level(x) > 0\n",
    "\n",
    "bc_ring = Domain.Dirichlet(domain_ring,bc_value_ring,X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.contourf(*X,domain_ring.contains(X)); plt.axis('equal');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1  Residue norm: 87.71412160918027\n",
      "Iteration: 2  Residue norm: 3.588611918025678\n",
      "Iteration: 3  Residue norm: 0.20858071879401102\n",
      "Iteration: 4  Residue norm: 0.009504676465152948\n",
      "Iteration: 5  Residue norm: 0.00041185762405138073\n",
      "Iteration: 6  Residue norm: 7.523386892629644e-07\n",
      "Iteration: 7  Residue norm: 3.1644964426646993e-12\n",
      "Target residue reached. Terminating.\n",
      "CPU times: user 722 ms, sys: 73.1 ms, total: 795 ms\n",
      "Wall time: 793 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "params = (0.5**2,0.,bc_ring)\n",
    "guess2 = 0.5*(X[0]**2 +2.*X[1]**2)\n",
    "solution_05 = newton_root(SchemeConsistent_Opt,guess2,params)\n",
    "\n",
    "fig = plt.figure(figsize=[4,4]);  plt.axis('equal')\n",
    "plt.title(r\"$\\lambda_{min}(\\nabla^2u)+\\alpha\\lambda_{\\max}(\\nabla^2u)=0$, $\\alpha=1/4$.\")\n",
    "plt.contourf(*X,np.where(bc_ring.interior,solution_05,np.nan));\n",
    "savefig(fig,\"Consistent_Ring_05.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad = np.array(np.gradient(solution_05,bc_ring.gridscale))\n",
    "grad[:,np.logical_not(bc_ring.domain.contains_ball(X,1.5*bc.gridscale))]=0.\n",
    "\n",
    "fig = plt.figure(figsize=[4,4]);  plt.axis('equal')\n",
    "plt.title(r\"Norm of the solution gradient, $\\alpha=1/4$\")\n",
    "plt.pcolormesh(*X,norm(grad,ord=2,axis=0)); #plt.colorbar();\n",
    "savefig(fig,\"Consistent_Ring_05_GradNorm.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=[4,4]);  plt.axis('equal')\n",
    "plt.title(r\"Norm of the solution gradient, $\\alpha=1/4$\")\n",
    "plt.contourf(*X,norm(grad,ord=2,axis=0)); #plt.colorbar();\n",
    "savefig(fig,\"Consistent_Ring_05_GradNorm_Levels.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1  Residue norm: 194.99025511873\n",
      "Iteration: 2  Residue norm: 38.274702920760774\n",
      "Iteration: 3  Residue norm: 15.351992164905013\n",
      "Iteration: 4  Residue norm: 6.635734520481244\n",
      "Iteration: 5  Residue norm: 2.2868865303014734\n",
      "Iteration: 6  Residue norm: 0.7152594416762381\n",
      "Iteration: 8  Residue norm: 0.026452529915575125\n",
      "Iteration: 10  Residue norm: 0.0007699501225764406\n",
      "Iteration: 12  Residue norm: 3.2508683149266385e-05\n",
      "Iteration: 14  Residue norm: 7.098775095677085e-07\n",
      "Iteration: 15  Residue norm: 6.211840102299241e-09\n",
      "Target residue reached. Terminating.\n",
      "CPU times: user 36.1 s, sys: 6.14 s, total: 42.3 s\n",
      "Wall time: 42.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "params = (0.05**2,0.,bc_ring)\n",
    "guess2 = 0.5*(X[0]**2 +2.*X[1]**2)\n",
    "solution_005 = newton_root(SchemeConsistent_Opt,guess2,params)\n",
    "\n",
    "fig = plt.figure(figsize=[4,4]);  plt.axis('equal')\n",
    "plt.title(r\"$\\lambda_{min}(\\nabla^2u)+\\alpha\\lambda_{\\max}(\\nabla^2u)=0$, $\\alpha=1/400$.\")\n",
    "#Paper : plt.title(r\"$\\lambda_{min}(\\nabla^2u)+\\mu\\lambda_{\\max}(\\nabla^2u)=0$, $\\mu=1/400$.\")\n",
    "plt.contourf(*X,np.where(bc_ring.interior,solution_005,np.nan))\n",
    "savefig(fig,\"Consistent_Ring_005.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad = np.array(np.gradient(solution_005,bc.gridscale))\n",
    "grad[:,np.logical_not(bc_ring.domain.contains_ball(X,1.5*bc.gridscale))]=np.nan\n",
    "\n",
    "fig = plt.figure(figsize=[5,4]);  plt.axis('equal')\n",
    "plt.title(r\"Norm of the solution gradient, $\\alpha=1/400$\")\n",
    "#Paper : plt.title(r\"Norm of the solution gradient, $\\mu=1/400$\")\n",
    "plt.pcolormesh(*X,norm(grad,ord=2,axis=0)); plt.colorbar();\n",
    "savefig(fig,\"Consistent_Ring_005_GradNorm.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Journal version of figures*\n",
    "\n",
    "<!---\n",
    "fig = plt.figure(figsize=[2.5,2.5]);  plt.axis('equal')\n",
    "plt.title(r\"Pucci equation, $\\mu=1/4$.\")\n",
    "#Paper : plt.title(r\"$\\lambda_{min}(\\nabla^2u)+\\mu\\lambda_{\\max}(\\nabla^2u)=0$, $\\mu=1/4$.\") \n",
    "plt.contourf(*X,np.where(bc_ring.interior,solution_05,np.nan));\n",
    "savefig(fig,\"Consistent_Ring_05_request.png\")\n",
    "\n",
    "fig = plt.figure(figsize=[2.5,2.5]);  plt.axis('equal')\n",
    "plt.title(r\"Pucci equation, $\\mu=1/400$.\")\n",
    "#Paper : plt.title(r\"$\\lambda_{min}(\\nabla^2u)+\\mu\\lambda_{\\max}(\\nabla^2u)=0$, $\\mu=1/400$.\")\n",
    "plt.contourf(*X,np.where(bc_ring.interior,solution_005,np.nan))\n",
    "savefig(fig,\"Consistent_Ring_005_request.png\")\n",
    "\n",
    "fig = plt.figure(figsize=[3,2.5]);  plt.axis('equal')\n",
    "plt.title(r\"Gradient norm, $\\mu=1/400$\")\n",
    "#Paper : plt.title(r\"Norm of the solution gradient, $\\mu=1/400$\")\n",
    "plt.pcolormesh(*X,norm(grad,ord=2,axis=0)); plt.colorbar();\n",
    "savefig(fig,\"Consistent_Ring_005_GradNorm_request.png\")\n",
    "--->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Validation\n",
    "\n",
    "The experiments presented below aim at informally validating the numerical implementation, by \n",
    "* Comparing the two schemes with one another.\n",
    "* Comparing with the automatic differentiation of an analytic function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Comparaison of the two schemes\n",
    "\n",
    "The consistent scheme, here denoted $F$, is arguably significantly more complex to implement than numerical scheme based on a sampling of the control space, here denoted $F_n$ where $n$ is the number of samples (angles).\n",
    "\n",
    "For cross validation, one can observe that for any discrete map $u$, one has \n",
    "$$\n",
    "    F_n(u) = F(u) + O(1/n).\n",
    "$$\n",
    "Note, crucially, that the test function $u$ is fixed, and so is the grid scale. The continuous limit only takes place in the control space.\n",
    "\n",
    "<!---\n",
    "Or is it $O(1/n^2)$ ?\n",
    "--->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "u_random = np.random.uniform(-1,1,guess.shape)\n",
    "\n",
    "bc_unit = Domain.MockDirichlet(guess.shape,1,padding=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "params=(α,β,bc_unit)\n",
    "residue_consistent = SchemeConsistent(u_random,*params)\n",
    "def error(nθ):\n",
    "    θs = np.linspace(0,np.pi,nθ,endpoint=False)\n",
    "    params=(Diff(α,θs),β,bc_unit)\n",
    "    residue_sampling = SchemeSampling(u_random,*params)\n",
    "    \n",
    "    return norm(residue_consistent-residue_sampling,ord=np.inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "nθ_validation = np.array([2**n for n in range(1,10)])\n",
    "error_validation = np.array([error(n) for n in nθ_validation])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convergence of the sampling based scheme toward the continuous one is observed, with the expected convergence rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Convergence of the sampling scheme toward the consistent scheme\")\n",
    "n_inv = 1./nθ_validation\n",
    "plt.loglog(n_inv,error_validation,\n",
    "           n_inv,n_inv);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Comparison with automatic differentiation\n",
    "\n",
    "We rely on automatic differentiation to compute the derivatives of an analytic function, and evaluate the PDE operator of interest. We then compute the numerical scheme residue on a synthetic problem with a known solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "tags": [
     "ExportCode"
    ]
   },
   "outputs": [],
   "source": [
    "def Pucci_ad(u,α,x):\n",
    "    \"\"\"\n",
    "    Computes alpha*lambda_max(D^2 u) + lambda_min(D^2 u), \n",
    "    at the given set of points, by automatic differentiation.\n",
    "    \"\"\"\n",
    "    x_ad = ad.Dense2.identity(constant=x,shape_free=(2,))\n",
    "    hessian = u(x_ad).hessian()\n",
    "    \n",
    "    Δ = ((hessian[0,0]-hessian[1,1])/2.)**2 + hessian[0,1]**2\n",
    "    sΔ = np.sqrt(Δ)\n",
    "    mean = (hessian[0,0]+hessian[1,1])/2.\n",
    "    λMin,λMax = mean-sΔ,mean+sΔ\n",
    "    \n",
    "    return λMin+α*λMax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Residue_ad(u,α,dom,X):\n",
    "    bc = Domain.Dirichlet(dom,u,X)\n",
    "    rhs = Pucci_ad(u,α,X)\n",
    "    residue = SchemeConsistent(u(X),α,rhs,bc)\n",
    "    residue[bc.not_interior]=0\n",
    "    return residue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_quadratic(x):  return x[0]**2+x[1]**2\n",
    "def test_polynomial(x): return 0.5*(x[0]**2+x[1]**2)**2\n",
    "\n",
    "\n",
    "dom_convex = Domain.Union(Domain.Ball(),Domain.Box())\n",
    "dom_ball = Domain.Ball()\n",
    "dom_square = Domain.Box()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the scheme is second order consistent its residue is zero up to essentially machine precision on the quadratic function. (Note that the relaxation introduced for differentiability of a square root is mainly responsible for the small error)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0002291883637326e-08, 1.0003609052233742e-08, 1.0002291883637326e-08]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[norm(Residue_ad(test_quadratic,0.5,dom,X),ord=np.inf) for dom in (dom_convex,dom_ball,dom_square)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the non-quadratic function, we get essentially second order convergence in the $L^1$ averaged norm. A slower convergence rate is achieved in the $L^\\infty$ norm, because the finite differences are only first order accurate at the boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain (line): convex, ball, square.\n",
      "Resolution (column) : 25,50,100.\n",
      "\n",
      "Mean residue of exact polynomial solution.\n",
      "[[0.01149697 0.00395542 0.00096948]\n",
      " [0.01172253 0.00432875 0.00105138]\n",
      " [0.00251758 0.00068901 0.00017824]]\n",
      "Residue of exact polynomial solution in the L^Infinity norm.\n",
      "[[0.12456117 0.11640453 0.06062434]\n",
      " [0.12456117 0.11640453 0.06062434]\n",
      " [0.01388889 0.00333195 0.00081624]]\n"
     ]
    }
   ],
   "source": [
    "aX_25 = np.linspace(-1,1,25)\n",
    "X_25 = np.array(np.meshgrid(aX_25,aX_25,indexing='ij'))\n",
    "\n",
    "aX_50 = np.linspace(-1,1,50)\n",
    "X_50 = np.array(np.meshgrid(aX_50,aX_50,indexing='ij'))\n",
    "\n",
    "X_100=X\n",
    "\n",
    "print(\"Domain (line): convex, ball, square.\")\n",
    "print(\"Resolution (column) : 25,50,100.\\n\")\n",
    "print(\"Mean residue of exact polynomial solution.\")\n",
    "print(np.array([[\n",
    "    norm(Residue_ad(test_polynomial,0.5,dom,X),ord=1,averaged=True)\n",
    "    for X in (X_25,X_50,X_100)]\n",
    "    for dom in (dom_convex,dom_ball,dom_square)]))\n",
    "\n",
    "print(\"Residue of exact polynomial solution in the L^Infinity norm.\")\n",
    "print(np.array([[\n",
    "    norm(Residue_ad(test_polynomial,0.5,dom,X),ord=np.inf)\n",
    "    for X in (X_25,X_50,X_100)]\n",
    "    for dom in (dom_convex,dom_ball,dom_square)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Format de la Cellule Texte Brut",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}